[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "This class will help students establish a core understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R and Python programming languages. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using the language of their choice. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\n\n\nI am maintaining the course here and on its CANVAS PAGE (for enrolled students).\nSYLLABUS\nBarrie’s GitHub\n\n\n\nGGSIDE! A companion to ggplot for making side plots! COOL!\nAwesome Quarto: A potentially interesting repository of Quarto documents, talks, tools, examples, etc.\nThe MockUp Blog - TABLES! This blog post explores the R packages gt and gtextras which will help us up our table game!\nRiffomonas Project: Pat Schloss is a Professor at the University of Michigan. The Riffomonas Project is his Youtube channel, which has HUNDREDS of easy to follow and amazingly useful instructional videos on R, ggplot, version control, and literate programming.\nDr. Tamara Munzner’s Website: It isn’t fancy, but Dr. Munzner’s website has tons of resources from her textbook and the many data visualization courses she has offered. This includes recorded lectures that align directly with the chapters of the text, much like what we are using.\nCheat Sheets: So many visual guides for many R packages, including the tidyverse, ggplot, dplyr, etc.\nLearning Vis Tools: Teaching Data Visualization Tutorials An interesting paper for discussion as we forge the structure for this class."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "I am maintaining the course here and on its CANVAS PAGE (for enrolled students).\nSYLLABUS\nBarrie’s GitHub"
  },
  {
    "objectID": "index.html#learning-resources",
    "href": "index.html#learning-resources",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "GGSIDE! A companion to ggplot for making side plots! COOL!\nAwesome Quarto: A potentially interesting repository of Quarto documents, talks, tools, examples, etc.\nThe MockUp Blog - TABLES! This blog post explores the R packages gt and gtextras which will help us up our table game!\nRiffomonas Project: Pat Schloss is a Professor at the University of Michigan. The Riffomonas Project is his Youtube channel, which has HUNDREDS of easy to follow and amazingly useful instructional videos on R, ggplot, version control, and literate programming.\nDr. Tamara Munzner’s Website: It isn’t fancy, but Dr. Munzner’s website has tons of resources from her textbook and the many data visualization courses she has offered. This includes recorded lectures that align directly with the chapters of the text, much like what we are using.\nCheat Sheets: So many visual guides for many R packages, including the tidyverse, ggplot, dplyr, etc.\nLearning Vis Tools: Teaching Data Visualization Tutorials An interesting paper for discussion as we forge the structure for this class."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#vad-model",
    "href": "posts/L4-Marks-Channels/index.html#vad-model",
    "title": "LECTURE 4",
    "section": "VAD MODEL",
    "text": "VAD MODEL"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#understand-the-data",
    "href": "posts/L4-Marks-Channels/index.html#understand-the-data",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE DATA",
    "text": "UNDERSTAND THE DATA\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#understand-the-task",
    "href": "posts/L4-Marks-Channels/index.html#understand-the-task",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE TASK",
    "text": "UNDERSTAND THE TASK\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#visual-encoding",
    "href": "posts/L4-Marks-Channels/index.html#visual-encoding",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING",
    "text": "VISUAL ENCODING\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#other-frameworks",
    "href": "posts/L4-Marks-Channels/index.html#other-frameworks",
    "title": "LECTURE 4",
    "section": "OTHER FRAMEWORKS",
    "text": "OTHER FRAMEWORKS\n\nThe Tidyverse\nThe Grammar of Graphics\nTufte"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#tidyverse",
    "href": "posts/L4-Marks-Channels/index.html#tidyverse",
    "title": "LECTURE 4",
    "section": "TIDYVERSE",
    "text": "TIDYVERSE\nR packages for data science:\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The best way to explore and understand the tidyverse is with cheetsheets, like this one for tidyr!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "href": "posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "title": "LECTURE 4",
    "section": "GRAMMAR OF GRAPHICS",
    "text": "GRAMMAR OF GRAPHICS\nThe ggplot2 cheatsheet!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#tufte",
    "href": "posts/L4-Marks-Channels/index.html#tufte",
    "title": "LECTURE 4",
    "section": "TUFTE",
    "text": "TUFTE\nTufte’s Website\nA Quarto Page Layout Example"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#analysis-framework",
    "href": "posts/L4-Marks-Channels/index.html#analysis-framework",
    "title": "LECTURE 4",
    "section": "ANALYSIS FRAMEWORK",
    "text": "ANALYSIS FRAMEWORK\nFour levels, three questions\n\n\n\nDomain situation defines the target users.\nAbstraction translate from specifics of domain to vocabulary of vis\n\nWHAT is shown? data abstraction\nWHY is the user looking at it? task abstraction\n\nIdiom defines the visualization\n\nHOW is it shown?\n\nvisual encoding idiom: how to draw\ninteraction idiom: how to manipulate\n\n\nAlgorithm creates the visualization\n\nevaluated with computational efficiency"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#encoding",
    "href": "posts/L4-Marks-Channels/index.html#encoding",
    "title": "LECTURE 4",
    "section": "ENCODING",
    "text": "ENCODING\nWe are defining the structure of the visualization (the idiom).\nTo do this, we use MARKS and CHANNELS:\n\nMARKS represent ITEMS or LINKS (aka OBSERVATIONS)\nCHANNELS change the appearance of MARKS based on ATTRIBUTES (aka VARIABLES)"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#marks-for-items",
    "href": "posts/L4-Marks-Channels/index.html#marks-for-items",
    "title": "LECTURE 4",
    "section": "MARKS FOR ITEMS",
    "text": "MARKS FOR ITEMS"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#marks-for-links",
    "href": "posts/L4-Marks-Channels/index.html#marks-for-links",
    "title": "LECTURE 4",
    "section": "MARKS FOR LINKS",
    "text": "MARKS FOR LINKS\n\n Bubblesets\n Force Directed Graph"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "href": "posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "title": "LECTURE 4",
    "section": "OBSERVABLE IN QUARTO!",
    "text": "OBSERVABLE IN QUARTO!\n\n\nCode\nd3 = require(\"d3@7\")\n\n\nchart = ForceGraph(miserables, {\n  nodeId: d =&gt; d.id,\n  nodeGroup: d =&gt; d.group,\n  nodeTitle: d =&gt; `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l =&gt; Math.sqrt(l.value),\n  width,\n  height: 1000,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d =&gt; d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) =&gt; source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) =&gt; target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 1000, // outer width, in pixels\n  height = 1000, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) =&gt; N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) =&gt; ({id: N[i]}));\n  links = d3.map(links, (_, i) =&gt; ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) =&gt; N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) =&gt; W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) =&gt; L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) =&gt; color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) =&gt; T[i]);\n  if (invalidation != null) invalidation.then(() =&gt; simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d =&gt; d.source.x)\n      .attr(\"y1\", d =&gt; d.source.y)\n      .attr(\"x2\", d =&gt; d.target.x)\n      .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\""
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channels",
    "href": "posts/L4-Marks-Channels/index.html#channels",
    "title": "LECTURE 4",
    "section": "CHANNELS",
    "text": "CHANNELS\n\n\n\nCHANNELS control the appearance of MARKS.\nThey are proportional to or based on ATTRIBUTES (aka VARIABLES).\nTheir properties differ in the type and amount of information that can be conveyed to the human perceptual system."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#this-is-important",
    "href": "posts/L4-Marks-Channels/index.html#this-is-important",
    "title": "LECTURE 4",
    "section": "THIS IS IMPORTANT",
    "text": "THIS IS IMPORTANT\nChannel properties differ in the type and amount of information that can be conveyed to the human perceptual system."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "href": "posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING EXAMPLE",
    "text": "VISUAL ENCODING EXAMPLE\nLet’s analyze the idiom structures below in terms of marks and channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#analyze-the-marks-and-channels",
    "href": "posts/L4-Marks-Channels/index.html#analyze-the-marks-and-channels",
    "title": "LECTURE 4",
    "section": "ANALYZE THE MARKS AND CHANNELS",
    "text": "ANALYZE THE MARKS AND CHANNELS\nMarks are defined by the ITEMS or OBSERVATIONS they represent.\nChannels are defined by the visually detectable properties that are mapped on to ATTRIBUTES or VARIABLES.\n\n\n\n\n\n\nKonrad’s Trees\n\n\n\n\n\n\n\nRobyn’s Books\n\n\n\n\n\n\n\nYaotian’s Wheat"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#redundant-encoding",
    "href": "posts/L4-Marks-Channels/index.html#redundant-encoding",
    "title": "LECTURE 4",
    "section": "REDUNDANT ENCODING",
    "text": "REDUNDANT ENCODING\n\n\nUses multiple channels for the same attribute.\n\nSends a stronger message\nUses up channels\nBonus points if you can identify BOTH channels in this figure!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#wheat-growth-by-area",
    "href": "posts/L4-Marks-Channels/index.html#wheat-growth-by-area",
    "title": "LECTURE 4",
    "section": "WHEAT GROWTH BY AREA*",
    "text": "WHEAT GROWTH BY AREA*\nBoth of Yaotian’s plots contain Redundant Encoding. Is this approach equally valuable for both plots?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#choosing-channels",
    "href": "posts/L4-Marks-Channels/index.html#choosing-channels",
    "title": "LECTURE 4",
    "section": "CHOOSING CHANNELS",
    "text": "CHOOSING CHANNELS\n\nEXPRESSIVENESS\n\nThe visual encoding should express all of, and only, the information in the dataset attributes.\n\nEFFECTIVENESS\n\nChannels differ in accuracy of perception.\nThe importance of the attribute should match the salience of the channel (its noticability)."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#expressiveness",
    "href": "posts/L4-Marks-Channels/index.html#expressiveness",
    "title": "LECTURE 4",
    "section": "EXPRESSIVENESS",
    "text": "EXPRESSIVENESS\nThe advantages and disadvantages of jitter plots. How might this relate to the idea of expressiveness?\n\nHeidi’s Cytoswine DataEXPRESSIVENESS: The visual encoding should express all of, and only, the information in the dataset attributes."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channel-effectiveness-rankings",
    "href": "posts/L4-Marks-Channels/index.html#channel-effectiveness-rankings",
    "title": "LECTURE 4",
    "section": "CHANNEL EFFECTIVENESS RANKINGS",
    "text": "CHANNEL EFFECTIVENESS RANKINGS\n\nNote that spatial position ranks high for both types of channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#grouping",
    "href": "posts/L4-Marks-Channels/index.html#grouping",
    "title": "LECTURE 4",
    "section": "GROUPING",
    "text": "GROUPING\n\n\n\nContainment\nConnection\nProximity\n\nSame spatial region.\n\nSimilarity\n\nSame values as other channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#summary-so-far",
    "href": "posts/L4-Marks-Channels/index.html#summary-so-far",
    "title": "LECTURE 4",
    "section": "SUMMARY SO FAR",
    "text": "SUMMARY SO FAR"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "href": "posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "title": "LECTURE 4",
    "section": "CHANNEL EFFECTIVENESS",
    "text": "CHANNEL EFFECTIVENESS\n\nAccuracy: how precisely can we tell the difference between encoded items?\nDiscriminability: how many unique steps can we perceive?\nSeparability: is our ability to use this channel affected by another one?\nPopout: can things jump out using this channel?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy-theory",
    "href": "posts/L4-Marks-Channels/index.html#accuracy-theory",
    "title": "LECTURE 4",
    "section": "ACCURACY (THEORY)",
    "text": "ACCURACY (THEORY)\nSteven’s Psychophisical Power Law: \\(S=I^N\\)\n\n\n\n\n\n\n\n\n\n\n\n\nLENGTH (N=1)\nELECTRIC SHOCK (N=3.5)\nSATURATION (N=1.7)\nAREA (N=0.7)\nBRIGHTNESS (N=0.5)"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "href": "posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "title": "LECTURE 4",
    "section": "ACCURACY (EXPERIMENTAL)",
    "text": "ACCURACY (EXPERIMENTAL)\n\n\n\n\n[Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods]"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy",
    "href": "posts/L4-Marks-Channels/index.html#accuracy",
    "title": "LECTURE 4",
    "section": "ACCURACY?",
    "text": "ACCURACY?\nDepends on the task. Which genre has the most game titles? Which genres are the top 4 in terms of game titles? Are there more Puzzle games than Strategy games?\n\nGeraline’s Video Games"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#discriminability",
    "href": "posts/L4-Marks-Channels/index.html#discriminability",
    "title": "LECTURE 4",
    "section": "DISCRIMINABILITY",
    "text": "DISCRIMINABILITY\nHow many usable steps are in the channel? Are the differences between items perceptible to the human as intended?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#discriminability-and-colors",
    "href": "posts/L4-Marks-Channels/index.html#discriminability-and-colors",
    "title": "LECTURE 4",
    "section": "DISCRIMINABILITY and COLORS",
    "text": "DISCRIMINABILITY and COLORS\n\nGeraline’s Gaming Platforms"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "href": "posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "title": "LECTURE 4",
    "section": "SEPARABILITY VS INTEGRALITY",
    "text": "SEPARABILITY VS INTEGRALITY\nSeparable channels are orthogonal and independent. Integral channels are inextricably combined. Attempts to encode different information with integral channels creates Interference.\n\n\nFigure 5.10. Pairs of visual channels fall along a continuum from fully separable to intrinsically integral. Color and location are separable channels well suited to encode different data attributes for two different groupings that can be selectively attended to. However, size interacts with hue, which is harder to perceive for small objects. The horizontal size and and vertical size channels are automatically fused into an integrated perception of area, yielding three groups. Attempts to code separate information along the red and green axes of the RGB color space fail, because we simply perceive four different hues."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#separability",
    "href": "posts/L4-Marks-Channels/index.html#separability",
    "title": "LECTURE 4",
    "section": "SEPARABILITY",
    "text": "SEPARABILITY\n\nRedundancy may be desirable, but area interferes with hue, with larger shapes having more visual salience."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout",
    "href": "posts/L4-Marks-Channels/index.html#popout",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\nVISUAL POPOUT is often called preattentive processing or tunable detection.\n\n\nfind the red dot! How long does it take?\nPopout results from our low-level visual system performing massively parallel processing on certain visual channels, eliminating the need for the viewer to consciously direct attention to items one by one (serial search).\n\nFigure 5.11. Visual popout. (a) The red circle pops out from a small set of blue circles. (b) The red circle pops out from a large set of blue circles just as quickly. (c) The red circle also pops out from a small set of square shapes, although a bit slower than with color. (d) The red circle also pops out of a large set of red squares. (e) The red circle does not take long to find from a small set of mixed shapes and colors. (f) The red circle does not pop out from a large set of red squares and blue circles, and it can only be found by searching one by one through all the objects."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout-1",
    "href": "posts/L4-Marks-Channels/index.html#popout-1",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\n\n\nMany channels are compatible with preattentive processing and facilitate popout:\n\ntilt\nsize\nshape\nproximity\nshadow direction\n\nBut not all!\n\nExample: parallel line pairs do not pop out from tilted pairs."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout-goes-the-weevil",
    "href": "posts/L4-Marks-Channels/index.html#popout-goes-the-weevil",
    "title": "LECTURE 4",
    "section": "POPOUT GOES THE WEEVIL?",
    "text": "POPOUT GOES THE WEEVIL?\n\n\n\n\n\n\nLucas Weevils before chemicals\n\n\n\n\n\n\n\nLucas Weevils after chemicals"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE VS ABSOLUTE JUDGEMENTS",
    "text": "RELATIVE VS ABSOLUTE JUDGEMENTS\nThe human perceptual system is fundamentally based on relative judgements, not absolute ones. This is why accuracy increases with common frame/scale and alignment.\nWeber’s Law: The detectable difference in stimulus intensity \\(I\\) as a fixed percentage \\(K\\) of the object magnitude: \\(dI/I=K\\) .\n\nThe filled rectangles differ in length by 1:9, and it is therefore difficult to detect the difference without aligment. The white rectangles differ in length by 1:2, it is easier to see this difference even when the objects are unaligned."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE JUDGEMENTS",
    "text": "RELATIVE JUDGEMENTS\n\nHeidi’s Microbiomes"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE LUMINANCE JUDGEMENTS",
    "text": "RELATIVE LUMINANCE JUDGEMENTS\nHuman perception of luminance is completely contextual, and is based on contrast with surrounding colors."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE COLOR JUDGEMENTS",
    "text": "RELATIVE COLOR JUDGEMENTS\nOur visual system evolved to provide color constancy so that the same surface is identifiable across a broad set of illumination conditions, even though a physical light meter would yield very different readings. While the visual system works very well in natural environments, many of its mechanisms work against simple approaches to visually encoding information with color.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.15 shows two colorful cubes. In Figure 5.15(a) corresponding squares both appear to be red. In Figure 5.15(b), masks show that the tile color in the image apparently illuminated by a yellowish light source is actually orange, and for the bluish light the tiles are actually purple.\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#last-lecture",
    "href": "posts/L3-TaskAbstraction/index.html#last-lecture",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "LAST LECTURE",
    "text": "LAST LECTURE\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#task-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TASK ABSTRACTION",
    "text": "TASK ABSTRACTION\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "FROM DOMAIN TO ABSTRACTION",
    "text": "FROM DOMAIN TO ABSTRACTION"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "KEY COMPONENTS OF TASK ABSTRACTION",
    "text": "KEY COMPONENTS OF TASK ABSTRACTION\n{action, target} pairs\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "href": "posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS AND TARGETS",
    "text": "ACTIONS AND TARGETS"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---analyze",
    "href": "posts/L3-TaskAbstraction/index.html#actions---analyze",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Analyze",
    "text": "ACTIONS - Analyze\n\n\n\nConsume: Information has already been generated and stored as data.\n\nDiscover: new knowledge, test hypothesis, generate new hypothesis, verify\nPresent: communicate something specific and already understood\nEnjoy: casual encounters with visualization\n\nProduce: generate new material or information\n\nAnnotate: addition of graphical or text to existing visualization elements\nRecord: saves or captures visualization elements as persistent artifacts (screenshots, lists, parameter sets, annotations)\nDerive: produce new data based on existing data (aka transform)"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---search",
    "href": "posts/L3-TaskAbstraction/index.html#actions---search",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Search",
    "text": "ACTIONS - Search\n\n\n\nLookup: Location and target both known\n\nExample: Look up humans in the Tree of Life, knowing they are mammals.\n\nLocate: Location unknown and target known\n\nExample: Look up rabbits in the Tree of Life, not knowing they are lagomorphs.\n\nBrowse: Location known and target unknown\n\nExample: Find any clades within Mammalia that have only one species.\n\nExplore: Location unknown and target unknown\n\nExample: Searching for anomalies in time series data."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---query",
    "href": "posts/L3-TaskAbstraction/index.html#actions---query",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Query",
    "text": "ACTIONS - Query\n\n\n\nQuery: How much of the data matters to the task?\n\nIdentify: One (specific Item, individual, cell, etc)\nCompare: Some (multiple targets)\nSummarize: All (very common, aka Overview)"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---all-data",
    "href": "posts/L3-TaskAbstraction/index.html#targets---all-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - All Data",
    "text": "TARGETS - All Data"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---attributes",
    "href": "posts/L3-TaskAbstraction/index.html#targets---attributes",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Attributes",
    "text": "TARGETS - Attributes"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---other-data",
    "href": "posts/L3-TaskAbstraction/index.html#targets---other-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Other Data",
    "text": "TARGETS - Other Data"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#summary",
    "href": "posts/L3-TaskAbstraction/index.html#summary",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "SUMMARY",
    "text": "SUMMARY\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "href": "posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "MANDATORY FUN",
    "text": "MANDATORY FUN\nWe will do these until everyone has done at least one example.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCANVAS…HOME"
  }
]