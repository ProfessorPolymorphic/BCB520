[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "SYLLABUS 2025",
    "section": "",
    "text": "Barrie D. Robison\nSpring 2025\n\n\nThis class will help students establish a core understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\n\n\n\nStudents completing this course will be able to:\n\nDescribe and manipulate tabular, network, and spatial data; transform these data into a form suitable for visualization.\nAnalyze data visualization design choices related to marks and channels, spatial arrangement, and components of color.\nDesign new data visualizations with appropriate use of visual channels for tabular, network, and spatial data with quantitative and categorical attributes.\nImplement their data visualization designs using existing tools in R (or other toolkits preferred by the student).\nExplain whether a visual encoding is perceptually appropriate for a specific combination of task and data.\nDemonstrate their skills with at least two novel visualizations suitable for inclusion in an online Data Science Portfolio.\n\n\n\n\nTamara Munzner. Visualization Analysis and Design. A K Peters Visualization Series, CRC Press, 2014. While the book is not required, I do emphasize the structure and approach to visualization that Dr. Munzner has developed.\nHard Copy on Amazon\nkindle/ebook on Amazon\n\n\n\n\n\n50% of your grade will be determined by homework exercises related to each course unit.\n20% of your grade will be determined by a mid term project (which would be a great item to include in your Data Science Portfolio).\n20% of your grade will be determined by a final project (which would be great item to include in your Data Science Portfolio).\n10% of your grade will be determined by participation in class discussions.\nGRADING SCALE: The grading scale is standard: A (90 -100 %), B (89 - 80 %), C (79 - 70 %), D (69-60 %), F( below 60 %).\n\n\n\nMissing a scheduled class session is at your discretion. I will be posting all the course materials online. If a discussion or in-class exercise occurs and you miss it, you will lose those participation points. There is no way to make up those points.\n\n\n\nThe R Markdown template I used for this syllabus was created by Dr. Steven V. Miller at Stockholm University. It contained this section, which I found amusing and have therefore retained. Professor Miller’s current university asks professors to have policies written into their syllabus about what students should do if the professor is more than 15 minutes late to class. Here is my version of that policy:\nI will inform students via e-mail in advance of class if class is cancelled for the day. Events that might create such a scenario include travel obligations that emerged after the semester has begun, a family emergency that encompasses multiple days, or some other thing. I will also contact our department secretary in emergent situations, such as something happening on the way to work. Failing that, assume the worst. Alien abduction, the return of one or more Old Ones to our plane, or some kind of attack by wizards are all viable explanations for my inability to attend class. I ask that the students make sure that my story gets the proper treatment on the “Mr. Ballen” YouTube channel. I also ask that my story be narrated by Morgan Freeman and that the role of me in the made for TV movie be played by Keanu Reeves or Danny DeVito.\n\n\n\nThe bad news is that there are NO make-ups for missed exams. Don’t bother asking. The good news is that there aren’t any exams.\n\n\n\nAll students are expected to uphold the highest standards of academic honesty. This includes but is not limited to: not cheating, not using the ideas of others without giving appropriate credit (including AI tools), and not falsifying data. Any incident of academic dishonesty will be handled according to the guidelines of the University of Idaho.\n\n\n\n\n\nlibrary(readxl)\nSchedule &lt;- read_excel(\"Schedule.xlsx\")\n\nknitr::kable(Schedule, caption = '')\n\n\n\n\nDATE\nTOPIC\nACTIVITY\nRESOURCES\n\n\n\n\n2025-01-09\nIntroduction and Overview\nNA\nNA\n\n\n2025-01-14\nThe Imporance of Visualization\nLiterate Programming\nTutorial 1, Tutorial 2\n\n\n2025-01-16\nWHAT?  Abstraction of Data\nNA\nNA\n\n\n2025-01-21\nWHY?  Task Abstraction\nNA\nNA\n\n\n2025-01-23\nNA\nNA\nNA\n\n\n2025-01-28\nNA\nNA\nNA\n\n\n2025-01-30\nNA\nNA\nNA\n\n\n2025-02-04\nBarrie in North Carolina\nNA\nNA\n\n\n2025-02-06\nBarrie in North Carolina\nNA\nNA\n\n\n2025-02-11\nMARKS. Geometric elements to depict data\nNA\nNA\n\n\n2025-02-13\nNA\nNA\nNA\n\n\n2025-02-18\nNA\nNA\nNA\n\n\n2025-02-20\nCHANNELS. Controlling the appearance of marks.\nNA\nNA\n\n\n2025-02-25\nRULES OF THUMB.\nMidterm Presentations\nNA\n\n\n2025-02-27\nTABULAR DATA I\nMidterm Presentations\nNA\n\n\n2025-03-04\nTABULAR DATA II\nNA\nNA\n\n\n2025-03-06\nSPATIAL DATA I: Geographic Maps\nNA\nNA\n\n\n2025-03-11\nSpring Recess\nNA\nNA\n\n\n2025-03-13\nSpring Recess\nNA\nNA\n\n\n2025-03-18\nBarrie in Vermont\nNA\nNA\n\n\n2025-03-20\nSPATIAL DATA II:  Spatial Fields\nNA\nNA\n\n\n2025-03-25\nNETWORK DATA I\nNA\nNA\n\n\n2025-03-27\nNETWORK DATA II\nNA\nNA\n\n\n2025-04-01\nCOLOR I\nNA\nNA\n\n\n2025-04-03\nCOLOR II\nNA\nNA\n\n\n2025-04-08\nINTERACTIVITY\nNA\nNA\n\n\n2025-04-10\nMULTIPLE VIEWS\nNA\nNA\n\n\n2025-04-15\nAGGREGATION\nNA\nNA\n\n\n2025-04-17\nBarrie in DC\nNA\nNA\n\n\n2025-04-22\nFILTERING\nNA\nNA\n\n\n2025-04-24\nEMBEDDING: Focus and Context\nNA\nNA\n\n\n2025-04-29\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2025-05-01\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2025-05-06\nFINALS WEEK\nNA\nNA"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "SYLLABUS 2025",
    "section": "",
    "text": "This class will help students establish a core understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "SYLLABUS 2025",
    "section": "",
    "text": "Students completing this course will be able to:\n\nDescribe and manipulate tabular, network, and spatial data; transform these data into a form suitable for visualization.\nAnalyze data visualization design choices related to marks and channels, spatial arrangement, and components of color.\nDesign new data visualizations with appropriate use of visual channels for tabular, network, and spatial data with quantitative and categorical attributes.\nImplement their data visualization designs using existing tools in R (or other toolkits preferred by the student).\nExplain whether a visual encoding is perceptually appropriate for a specific combination of task and data.\nDemonstrate their skills with at least two novel visualizations suitable for inclusion in an online Data Science Portfolio."
  },
  {
    "objectID": "syllabus.html#recommended-readings",
    "href": "syllabus.html#recommended-readings",
    "title": "SYLLABUS 2025",
    "section": "",
    "text": "Tamara Munzner. Visualization Analysis and Design. A K Peters Visualization Series, CRC Press, 2014. While the book is not required, I do emphasize the structure and approach to visualization that Dr. Munzner has developed.\nHard Copy on Amazon\nkindle/ebook on Amazon"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "SYLLABUS 2025",
    "section": "",
    "text": "50% of your grade will be determined by homework exercises related to each course unit.\n20% of your grade will be determined by a mid term project (which would be a great item to include in your Data Science Portfolio).\n20% of your grade will be determined by a final project (which would be great item to include in your Data Science Portfolio).\n10% of your grade will be determined by participation in class discussions.\nGRADING SCALE: The grading scale is standard: A (90 -100 %), B (89 - 80 %), C (79 - 70 %), D (69-60 %), F( below 60 %).\n\n\n\nMissing a scheduled class session is at your discretion. I will be posting all the course materials online. If a discussion or in-class exercise occurs and you miss it, you will lose those participation points. There is no way to make up those points.\n\n\n\nThe R Markdown template I used for this syllabus was created by Dr. Steven V. Miller at Stockholm University. It contained this section, which I found amusing and have therefore retained. Professor Miller’s current university asks professors to have policies written into their syllabus about what students should do if the professor is more than 15 minutes late to class. Here is my version of that policy:\nI will inform students via e-mail in advance of class if class is cancelled for the day. Events that might create such a scenario include travel obligations that emerged after the semester has begun, a family emergency that encompasses multiple days, or some other thing. I will also contact our department secretary in emergent situations, such as something happening on the way to work. Failing that, assume the worst. Alien abduction, the return of one or more Old Ones to our plane, or some kind of attack by wizards are all viable explanations for my inability to attend class. I ask that the students make sure that my story gets the proper treatment on the “Mr. Ballen” YouTube channel. I also ask that my story be narrated by Morgan Freeman and that the role of me in the made for TV movie be played by Keanu Reeves or Danny DeVito.\n\n\n\nThe bad news is that there are NO make-ups for missed exams. Don’t bother asking. The good news is that there aren’t any exams.\n\n\n\nAll students are expected to uphold the highest standards of academic honesty. This includes but is not limited to: not cheating, not using the ideas of others without giving appropriate credit (including AI tools), and not falsifying data. Any incident of academic dishonesty will be handled according to the guidelines of the University of Idaho."
  },
  {
    "objectID": "syllabus.html#class-schedule",
    "href": "syllabus.html#class-schedule",
    "title": "SYLLABUS 2025",
    "section": "",
    "text": "library(readxl)\nSchedule &lt;- read_excel(\"Schedule.xlsx\")\n\nknitr::kable(Schedule, caption = '')\n\n\n\n\nDATE\nTOPIC\nACTIVITY\nRESOURCES\n\n\n\n\n2025-01-09\nIntroduction and Overview\nNA\nNA\n\n\n2025-01-14\nThe Imporance of Visualization\nLiterate Programming\nTutorial 1, Tutorial 2\n\n\n2025-01-16\nWHAT?  Abstraction of Data\nNA\nNA\n\n\n2025-01-21\nWHY?  Task Abstraction\nNA\nNA\n\n\n2025-01-23\nNA\nNA\nNA\n\n\n2025-01-28\nNA\nNA\nNA\n\n\n2025-01-30\nNA\nNA\nNA\n\n\n2025-02-04\nBarrie in North Carolina\nNA\nNA\n\n\n2025-02-06\nBarrie in North Carolina\nNA\nNA\n\n\n2025-02-11\nMARKS. Geometric elements to depict data\nNA\nNA\n\n\n2025-02-13\nNA\nNA\nNA\n\n\n2025-02-18\nNA\nNA\nNA\n\n\n2025-02-20\nCHANNELS. Controlling the appearance of marks.\nNA\nNA\n\n\n2025-02-25\nRULES OF THUMB.\nMidterm Presentations\nNA\n\n\n2025-02-27\nTABULAR DATA I\nMidterm Presentations\nNA\n\n\n2025-03-04\nTABULAR DATA II\nNA\nNA\n\n\n2025-03-06\nSPATIAL DATA I: Geographic Maps\nNA\nNA\n\n\n2025-03-11\nSpring Recess\nNA\nNA\n\n\n2025-03-13\nSpring Recess\nNA\nNA\n\n\n2025-03-18\nBarrie in Vermont\nNA\nNA\n\n\n2025-03-20\nSPATIAL DATA II:  Spatial Fields\nNA\nNA\n\n\n2025-03-25\nNETWORK DATA I\nNA\nNA\n\n\n2025-03-27\nNETWORK DATA II\nNA\nNA\n\n\n2025-04-01\nCOLOR I\nNA\nNA\n\n\n2025-04-03\nCOLOR II\nNA\nNA\n\n\n2025-04-08\nINTERACTIVITY\nNA\nNA\n\n\n2025-04-10\nMULTIPLE VIEWS\nNA\nNA\n\n\n2025-04-15\nAGGREGATION\nNA\nNA\n\n\n2025-04-17\nBarrie in DC\nNA\nNA\n\n\n2025-04-22\nFILTERING\nNA\nNA\n\n\n2025-04-24\nEMBEDDING: Focus and Context\nNA\nNA\n\n\n2025-04-29\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2025-05-01\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2025-05-06\nFINALS WEEK\nNA\nNA"
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "snippets",
    "section": "",
    "text": "Write the synthetic data to CSV files\n\n\nFunction to create the CSV files from the synthetic data\ncreate_synthetic_datasets &lt;- function() { # Create diet_data dataframe diet_data &lt;- read.csv(“diet_fatty_acids.csv”)\n# Create milk_data dataframe milk_data &lt;- read.csv(“milk_fatty_acids.csv”)\nreturn(list(diet_data = diet_data, milk_data = milk_data)) }\n\n\nBasic visualization functions\n\n\nPlot individual fatty acid trajectories\nplot_fa_trajectory &lt;- function(data_long, target_fa) { require(ggplot2)\nfa_data &lt;- data_long %&gt;% filter(fatty_acid == target_fa)\np &lt;- ggplot(fa_data, aes(x = days_postpartum, y = abundance, group = woman_id, color = factor(woman_id))) + geom_line() + geom_point() + labs(title = paste0(target_fa, ” Abundance Over Time”), x = “Days Postpartum”, y = “Abundance (% of total FAs)”, color = “Woman ID”) + theme_minimal()\nreturn(p) }\n\n\nMultivariate visualization with PCA\npca_analysis &lt;- function(data, id_cols = c(“woman_id”, “timepoint”)) { require(ggplot2)\n# Extract just the FA columns for PCA fa_cols &lt;- data %&gt;% select(-all_of(id_cols))\n# Run PCA pca_result &lt;- prcomp(fa_cols, scale. = TRUE)\n# Extract scores pca_scores &lt;- as.data.frame(pca_result$x)\n# Add back identifiers for(col in id_cols) { pca_scores[[col]] &lt;- data[[col]] }\n# Create plot p &lt;- ggplot(pca_scores, aes(x = PC1, y = PC2, color = factor(woman_id), shape = factor(timepoint))) + geom_point(size = 3) + labs(title = “PCA of Fatty Acid Profiles”, color = “Woman ID”, shape = “Timepoint”) + theme_minimal()\n# Return both plot and PCA results return(list(plot = p, pca = pca_result, scores = pca_scores)) }\n\n\nCorrelation analysis between diet and milk\ncorrelation_analysis &lt;- function(diet_long, milk_long) { require(ggplot2)\n# Join diet and milk data for correlation analysis joined_data &lt;- diet_long %&gt;% inner_join(milk_long, by = c(“woman_id”, “timepoint”, “days_postpartum”, “fatty_acid”), suffix = c(“_diet”, “_milk”))\n# Calculate correlations for each fatty acid correlations &lt;- joined_data %&gt;% group_by(fatty_acid) %&gt;% summarize(correlation = cor(abundance_diet, abundance_milk, method = “spearman”), p_value = cor.test(abundance_diet, abundance_milk, method = “spearman”)$p.value)\n# Plot the correlations p &lt;- ggplot(correlations, aes(x = reorder(fatty_acid, correlation), y = correlation)) + geom_bar(stat = “identity”, aes(fill = p_value &lt; 0.05)) + coord_flip() + labs(title = “Correlation Between Diet and Milk Fatty Acid Abundance”, x = “Fatty Acid”, y = “Spearman Correlation”, fill = “P &lt; 0.05”) + theme_minimal()\nreturn(list(plot = p, correlations = correlations)) }\n\n\nHeatmap visualization by timepoint\nheatmap_by_timepoint &lt;- function(data, timepoint_val, scale = “column”) { require(pheatmap)\n# Prepare data for heatmap heatmap_data &lt;- data %&gt;% filter(timepoint == timepoint_val) %&gt;% select(-timepoint, -days_postpartum) %&gt;% column_to_rownames(“woman_id”)\n# Create heatmap pheatmap(heatmap_data, scale = scale, clustering_distance_rows = “euclidean”, clustering_distance_cols = “euclidean”, main = paste0(“Fatty Acid Profiles at Timepoint”, timepoint_val)) }\n\n\nCalculate fatty acid composition by class\ncalculate_fa_classes &lt;- function(fa_long_data) { # Create a function to classify fatty acids classify_fa &lt;- function(fa_name) { if (grepl(“C[0-9]+:0\\(\", fa_name)) {\n      return(\"Saturated\")\n    } else if (grepl(\"C[0-9]+:1\", fa_name)) {\n      if (grepl(\"t\\)”, fa_name)) { return(“Trans”) } else { return(“Monounsaturated”) } } else if (grepl(“n3\\(\", fa_name)) {\n      return(\"Omega-3\")\n    } else if (grepl(\"n6\\)”, fa_name)) { return(“Omega-6”) } else if (grepl(“t$”, fa_name)) { return(“Trans”) } else { return(“Other”) } }\n# Add classifications to the data fa_long_data\\(fa_class &lt;- sapply(fa_long_data\\)fatty_acid, classify_fa)\n# Calculate totals by class class_totals &lt;- fa_long_data %&gt;% group_by(woman_id, timepoint, days_postpartum, fa_class) %&gt;% summarize(total_abundance = sum(abundance), .groups = “drop”)\nreturn(list(classified_data = fa_long_data, class_totals = class_totals)) }\n\n\nCreate a mixed effects model for a specific fatty acid\nmixed_model_analysis &lt;- function(long_data, target_fa) { require(lme4) require(lmerTest)\n# Filter for the target fatty acid fa_data &lt;- long_data %&gt;% filter(fatty_acid == target_fa)\n# Linear mixed model with random intercept by woman model &lt;- lmer(abundance ~ days_postpartum + (1|woman_id), data = fa_data)\n# Visualize individual trajectories with overall trend p &lt;- ggplot(fa_data, aes(x = days_postpartum, y = abundance, group = woman_id)) + geom_line(alpha = 0.3) + geom_smooth(aes(group = 1), method = “lm”, color = “red”) + labs(title = paste0(target_fa, ” Abundance Over Time with Overall Trend”), x = “Days Postpartum”, y = paste0(target_fa, ” Abundance (%)“)) + theme_minimal()\nreturn(list(model = model, plot = p)) }\n\n\nComplete analysis pipeline\nrun_fatty_acid_analysis &lt;- function(diet_data, milk_data) { require(tidyverse)\n# Convert to long format diet_long &lt;- diet_data %&gt;% pivot_longer(cols = -c(woman_id, timepoint, days_postpartum), names_to = “fatty_acid”, values_to = “abundance”)\nmilk_long &lt;- milk_data %&gt;% pivot_longer(cols = -c(woman_id, timepoint, days_postpartum), names_to = “fatty_acid”, values_to = “abundance”)\n# List of key fatty acids to analyze key_fas &lt;- c(“C16:0”, “C18:0”, “C18:1n9”, “C18:2n6”, “C18:3n3”, “C20:4n6”, “C22:6n3”)\n# Run analyses results &lt;- list()\n# 1. Individual trajectories for key FAs results\\(trajectories &lt;- lapply(key_fas, function(fa) {\n    plot_fa_trajectory(milk_long, fa)\n  })\n  names(results\\)trajectories) &lt;- key_fas\n# 2. PCA analysis results$pca &lt;- pca_analysis( milk_data %&gt;% select(-days_postpartum), id_cols = c(“woman_id”, “timepoint”) )\n# 3. Diet-milk correlations results$correlations &lt;- correlation_analysis(diet_long, milk_long)\n# 4. Heatmaps by timepoint results$heatmaps &lt;- lapply(1:6, function(tp) { heatmap_by_timepoint(milk_data, tp) })\n# 5. FA class analysis milk_classes &lt;- calculate_fa_classes(milk_long) diet_classes &lt;- calculate_fa_classes(diet_long)\nresults$class_analysis &lt;- list( milk = milk_classes, diet = diet_classes )\n# 6. Mixed models for key FAs results\\(mixed_models &lt;- lapply(key_fas, function(fa) {\n    mixed_model_analysis(milk_long, fa)\n  })\n  names(results\\)mixed_models) &lt;- key_fas\nreturn(results) }\n\n\nExample usage:\n\n\ndata &lt;- create_synthetic_datasets()\n\n\nanalysis_results &lt;- run_fatty_acid_analysis(data\\(diet_data, data\\)milk_data)\n\n\nanalysis_results\\(trajectories\\)C22:6n3 # View DHA trajectory\n\n\nanalysis_results\\(pca\\)plot # View PCA plot"
  },
  {
    "objectID": "posts/T5-MarksChannels/index.html",
    "href": "posts/T5-MarksChannels/index.html",
    "title": "TUTORIAL 5 - Marks and Channels",
    "section": "",
    "text": "This tutorial is my attempt at Assignment 4!\n\n\nI’m interested in testing the hypothesis that this season’s Vancouver Canucks are a historically terrible team. One of the key factors in this hypothesis is that this year’s Canucks really, really suck at killing penalties. For this first assignment component, I’ll show you what I mean with a visualization!\nThe data I am using was downloaded from Moneypuck.com.\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(vroom)\n\nfiles &lt;- fs::dir_ls(glob = \"*teams.csv\")\n\nallseasons &lt;- vroom::vroom(files)\n\nallseasons$color&lt;-\"darkblue\"\nallseasons$color[allseasons$name==\"VAN\" & allseasons$season==\"2022\"]  &lt;- \"red\"\n  allseasons$teamseason&lt;- paste(allseasons$name, allseasons$season)\n  \n  allseasons$HDSAR&lt;-allseasons$highDangerShotsAgainst/allseasons$iceTime*120\n  \n  PKDATA&lt;-allseasons[c(\"situation\", \"teamseason\", \"goalsAgainst\", \"penaltiesFor\")]\n  PKpenalties&lt;-subset(PKDATA, situation == \"all\")\n  PKgoals&lt;-subset(PKDATA, situation == \"4on5\")\n  PK&lt;-PKgoals[c(\"teamseason\", \"goalsAgainst\")]\n  PK$penaltiesFor&lt;-PKpenalties$penaltiesFor\n  PK$PKpercent&lt;-100-(PK$goalsAgainst/PK$penaltiesFor*100)\n\n\nNow that I’ve massaged the data, I can tell you that the 2022-2023 Vancouver Canucks penalty kill percentage is 76.9633508. That means that when they get a penalty, there is about a 76.9633508 percent chance that they will prevent the opponent from scoring while they are short handed. However, unless you are a hockey super fan, you don’t have enough context to know if this is good or bad.\nThe figure below shows the distribution of penalty kill percentage for all teams and seasons since the 2010 season.\n\n\nCode\n  d &lt;- density(PK$PKpercent)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PK$PKpercent)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PK$PKpercent, 0.01)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22 &lt;- PK$PKpercent[PK$teamseason == \"VAN 2022\"]\n\nggplot(PK, aes(x=PKpercent))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.5, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Penalty Kill (%)\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nSo… yeah. The blue line is the 22 Canucks. The red dotted line is the 1st! percentile of the density function. This is fine. We know this to be true. Their PK sucks. It is one of the worst in the history of the NHL. But WHY is it so bad? Bad goaltending? Bad defense? Both?\n\n\n\nMy theory is that their defense hangs their goaltenders out to dry. This might be measured with the number of High Danger Shots Allowed.\nHigh Danger Shots are defined as unblocked shot attempts with greater than a 20% probability of being a goal, as calculated by hockey data nerds. High danger shots account for ~5% of shots and ~33% of goals in the NHL.\nBelow I calculate High Danger Shots Allowed Rate, which is the number of HDS during penalty kill situations standardized it to a rate per 2 minutes spent killing penalties (the normal duration of an NHL minor penalty).\n\n\nCode\nPKsituation&lt;-subset(allseasons, situation == \"4on5\")\n# Step 2: Estimate the density of the data\nd &lt;- density(PKsituation$HDSAR)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKsituation$HDSAR)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKsituation$HDSAR, 0.95)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22HD &lt;- PKsituation$HDSAR[PKsituation$teamseason == \"VAN 2022\"]\n\n\n\n\n\nggplot(PKsituation, aes(x=HDSAR))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.005, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22HD, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"High Danger Shots Allowed (rate per 2 minutes)\")\n\n\n\n\n\n\n\n\n\nFigure 1. Distribution of High Danger Shot Rate for NHL teams over the past 10 seasons. The 2022-2023 Vancouver Canucks are shown with the dark blue line (HDSAR = 0.3220996) and the 95th percentile is shown in red.\nIn terms of the semantics of Data Visualization, I am trying to DISCOVER the DISTRIBUTION of High Danger Shots Allowed, and then ANNOTATE the DISTRIBUTION with information about the current Vancouver Canucks team and their approximate percentile in the distribution.\nTo visualize this I am encoding a DERIVED attribute - the relative density of HDSAR as VERTICAL LENGTH of a bar (technically the MARK is a LINE) on an aligned axis. This is a pretty powerful encoding, which is why it is very common when visualizing distributions. In addtion, I am encoding HDSAR values themselves with HORIZONTAL POSITION. A nice, ordered channel for a nice, ordered attribute. The DENSITY FUNCTION is less clear to me. I’m obviously using a line, whose vertical and horizontal position encodes the relationship between the two variables. But the blue shaded fill? I kind of think that is using an AREA channel and REDUNDANT ENCODING to send a stronger signal.\nThe annotations of the 95th percentile and the 2022 Canucks’ HDSAR are encoded with COLOR and HORIZONTAL POSITION CHANNELS, and LINES as MARKS.\nThe 2022 Canucks indeed seem to be giving up HDS at a historically high rate.\n\n\n\nBut wait! You exclaim, shouting your objection into the uncaring Abyss. What about the goaltending? Surely that has also contributed to this season’s futility? “Perhaps..”, the Abyss murmurs back…\nLet’s see whether the futility of the penalty kill is also a function of bad goaltending. Now, we can’t just look at raw goals allowed, because if the defense is allowing really High Danger Shots, then even a good goaltender is going to allow some goals. Fortunately, the NHL nerds have developed an Expected Goals metric. Moneypuck provides this handy description in their data dictionary:\nExpected Goals:. The sum of the probabilities of unblocked shot attempts being goals. For example, a rebound shot in the slot may be worth 0.5 expected goals, while a shot from the blueline while short handed may be worth 0.01 expected goals. The expected value of each shot attempt is calculated by the MoneyPuck Expected Goals model. Expected goals is commonly abbreviated as xGoals. Blocked shot attempts are valued at 0 xGoals.\nCool!\n\n\nCode\n  Tending&lt;-allseasons[c(\"situation\", \"teamseason\", \"goalsAgainst\", \"xGoalsAgainst\", \"shotsOnGoalAgainst\", \"games_played\", \"iceTime\", \"highDangerShotsAgainst\")]\nTending$GArate&lt;-Tending$xGoalsAgainst/Tending$iceTime*120\n  PKTending&lt;-subset(Tending, situation == \"4on5\")\n\n  d &lt;- density(PKTending$GArate)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKTending$GArate)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKTending$GArate, 0.95)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22GA &lt;- PKTending$GArate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=GArate))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.005, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22GA, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Expected Goals Allowed while killing penalties (rate per 2 minutes)\")  \n\n\n\n\n\n\n\n\n\nThis figure uses Expected Goals. It shows that Vancouver is playing a defense that is generating a historically high amount of expected goals, a result in line with our analysis of High Danger Chances.\nTo truly evaluate the goaltending, tt might be better for us to look at a ratio of Expected Goals to actual Goals. If this number is close to 1, it indicates an expected level of NHL goaltending during the penalty kill. The goalies are making the saves that they should. If the number is higher than 1, it means the goaltenders are making more saves than they should and are playing well. If the number is lower than 1, it means the goaltenders are letting in easy goals.\n\n\nCode\nTending$XGrate&lt;-Tending$xGoalsAgainst/Tending$goalsAgainst\n  PKTending&lt;-subset(Tending, situation == \"4on5\")\n\n  d &lt;- density(PKTending$XGrate)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKTending$XGrate)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKTending$XGrate, 0.05)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22XG &lt;- PKTending$XGrate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=XGrate))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.01, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22XG, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Expected Goals / Goals Allowed ratio while killing penalties\")  \n\n\n\n\n\n\n\n\n\nFigure 3. Distribution of Expected Goals to Goals Allowed during 4 on 5 situations for NHL teams over the past 10 seasons. The 2022-2023 Vancouver Canucks are shown with the dark blue line (XGrate = 0.6963636) and the 5th percentile is shown in red.\nUh Oh. Not only is the defense giving up a historically high number of High Danger scoring chances, the goaltenders are performing well below expectation. They are making about 70% of the saves they are supposed to make. So…. yeah. Why is the penalty kill so bad? Bad defense and bad goaltending. I mean, HISTORICALLY BAD.\nIs it because of the system? I don’t think so. I think it is because of the players. They don’t have enough defensive talent on the team to effectively kill penalties. I am sad.\n\n\n\nNow, lets mess this up! First I’m going back to the HDSAR figure, and changing some of my choices for marks and channels.\n\n\nCode\nggplot(PKsituation, aes(x=teamseason, y=HDSAR)) +\n  geom_bar(stat=\"identity\", aes(fill=name, alpha=season)) +\n  xlab(\"NHL Team and Season\") +\n  ylab(\"High Danger Shots Allowed (rate per 2 minutes)\") +\n  ggtitle(\"Are the Vancouver Canucks Historically Bad?\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 0.1),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nFigure 2. Distribution of High Danger Shot Rate for NHL teams over the past 10 seasons.\nHoly Crap. What an absolute nightmate. In terms of MARKS, I’ve stayed with LINES. But the CHANNELS… wow. The COLOR CHANNEL is used to indicate the team. The ALPHA CHANNEL (transparency) is used to indicate season (time). These choices create HUGE problems related to discriminability (alpha channel) and separability. In addition, my choice to order these lines according to team and season is not at all compatible with my choice of channels. Not to mention that the important thing - to see where the 2022 Canucks stand in terms of historical futility - is completely wrecked because the horizontal position (a strong channel) is not closely tied to the most salient Attribute (HDSAR). Go ahead and pick out the the 2022 Canucks from this figure…. I’ll wait.\n\n\nFrom Munzner, 5.4.1:\nThe expressiveness principle: Visual encoding should express all of, and only, the information in the dataset attributes. Ordered data should be shown in a way that our perceptual system intrinsically senses as ordered. Well, using color sure violates that idea! Conversely, unordered data should not be shown in a way that perceptually implies an ordering that does not exist.\nThe effectiveness principle: The importance of the attribute should match the salience of the channel (its noticeablity). The most important attributes should be encoded with the most effective channels. Definitely not! Even though the Line Marks are using Length on an aligned axis, their width and ordering on the x axis completely obscures the focal team.\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1 and 2), create a visualization called Figure 3 that uses a magnitude channel with a number of bins that facilitate discriminiability of an attribute. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 4 that uses this same channel for WAY TOO MANY BINS, violating the guidelines for discriminability. Write a caption that explains why this number of bins is bad compared to Figure 3. Look at the NHL Rainbow! So many colors… so little ability to differentiate between teams. For example, go ahead and try to pick out Edmonton vs Florida\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1-4), create a visualization called Figure 5 that uses a two or more channels to encode two or more attributes while maintaining separability. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 6 that uses channels that are integral, or at least much less separable. Write a caption that explains why choosing these channels is bad compared to Figure 5.\nFor this one I’d like to try something else. Let’s look at the Expected Goals and Allowed Goals as a scatterplot:\n\n\nCode\nV22&lt;-subset(PKTending, teamseason == \"VAN 2022\")\nVAN22XG &lt;- PKTending$XGrate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=xGoalsAgainst/games_played*82, y =goalsAgainst/games_played*82 ))+\n  geom_point(aes(alpha=highDangerShotsAgainst, size=highDangerShotsAgainst))+\n  geom_abline(intercept = 0, slope = 1)+\n  geom_abline(intercept = 90, slope = -1)+\n \n  xlim(0,100)+\n  ylim(0,100)+\n  geom_vline(xintercept = mean(PKTending$xGoalsAgainst/PKTending$games_played*82), color = \"blue\", size=1.0, linetype=\"dotted\")+\n  geom_hline(yintercept = mean(PKTending$goalsAgainst/PKTending$games_played*82), color = \"blue\", size = 1, linetype=\"dotted\")+\n  geom_point(data = V22, aes(x=xGoalsAgainst/games_played*82, y =goalsAgainst/games_played*82),\n             color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1-6… you get the idea), create a visualization called Figure 7 that effectively uses the concept of popout. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 8 that makes the identification task in Figure 7 much more difficult. Write a caption that explains why Figure 8 sucks compared to Figure 7. The popout concept is best shown by my use of a dark blue (color) line mark to indicate the 22 Canucks. Without that, you cannot achieve the task because the 22 Canucks are too hard to find without doing a serial search."
  },
  {
    "objectID": "posts/T5-MarksChannels/index.html#penalty-kill",
    "href": "posts/T5-MarksChannels/index.html#penalty-kill",
    "title": "TUTORIAL 5 - Marks and Channels",
    "section": "",
    "text": "I’m interested in testing the hypothesis that this season’s Vancouver Canucks are a historically terrible team. One of the key factors in this hypothesis is that this year’s Canucks really, really suck at killing penalties. For this first assignment component, I’ll show you what I mean with a visualization!\nThe data I am using was downloaded from Moneypuck.com.\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(vroom)\n\nfiles &lt;- fs::dir_ls(glob = \"*teams.csv\")\n\nallseasons &lt;- vroom::vroom(files)\n\nallseasons$color&lt;-\"darkblue\"\nallseasons$color[allseasons$name==\"VAN\" & allseasons$season==\"2022\"]  &lt;- \"red\"\n  allseasons$teamseason&lt;- paste(allseasons$name, allseasons$season)\n  \n  allseasons$HDSAR&lt;-allseasons$highDangerShotsAgainst/allseasons$iceTime*120\n  \n  PKDATA&lt;-allseasons[c(\"situation\", \"teamseason\", \"goalsAgainst\", \"penaltiesFor\")]\n  PKpenalties&lt;-subset(PKDATA, situation == \"all\")\n  PKgoals&lt;-subset(PKDATA, situation == \"4on5\")\n  PK&lt;-PKgoals[c(\"teamseason\", \"goalsAgainst\")]\n  PK$penaltiesFor&lt;-PKpenalties$penaltiesFor\n  PK$PKpercent&lt;-100-(PK$goalsAgainst/PK$penaltiesFor*100)\n\n\nNow that I’ve massaged the data, I can tell you that the 2022-2023 Vancouver Canucks penalty kill percentage is 76.9633508. That means that when they get a penalty, there is about a 76.9633508 percent chance that they will prevent the opponent from scoring while they are short handed. However, unless you are a hockey super fan, you don’t have enough context to know if this is good or bad.\nThe figure below shows the distribution of penalty kill percentage for all teams and seasons since the 2010 season.\n\n\nCode\n  d &lt;- density(PK$PKpercent)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PK$PKpercent)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PK$PKpercent, 0.01)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22 &lt;- PK$PKpercent[PK$teamseason == \"VAN 2022\"]\n\nggplot(PK, aes(x=PKpercent))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.5, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Penalty Kill (%)\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nSo… yeah. The blue line is the 22 Canucks. The red dotted line is the 1st! percentile of the density function. This is fine. We know this to be true. Their PK sucks. It is one of the worst in the history of the NHL. But WHY is it so bad? Bad goaltending? Bad defense? Both?"
  },
  {
    "objectID": "posts/T5-MarksChannels/index.html#high-danger-shots-allowed",
    "href": "posts/T5-MarksChannels/index.html#high-danger-shots-allowed",
    "title": "TUTORIAL 5 - Marks and Channels",
    "section": "",
    "text": "My theory is that their defense hangs their goaltenders out to dry. This might be measured with the number of High Danger Shots Allowed.\nHigh Danger Shots are defined as unblocked shot attempts with greater than a 20% probability of being a goal, as calculated by hockey data nerds. High danger shots account for ~5% of shots and ~33% of goals in the NHL.\nBelow I calculate High Danger Shots Allowed Rate, which is the number of HDS during penalty kill situations standardized it to a rate per 2 minutes spent killing penalties (the normal duration of an NHL minor penalty).\n\n\nCode\nPKsituation&lt;-subset(allseasons, situation == \"4on5\")\n# Step 2: Estimate the density of the data\nd &lt;- density(PKsituation$HDSAR)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKsituation$HDSAR)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKsituation$HDSAR, 0.95)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22HD &lt;- PKsituation$HDSAR[PKsituation$teamseason == \"VAN 2022\"]\n\n\n\n\n\nggplot(PKsituation, aes(x=HDSAR))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.005, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22HD, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"High Danger Shots Allowed (rate per 2 minutes)\")\n\n\n\n\n\n\n\n\n\nFigure 1. Distribution of High Danger Shot Rate for NHL teams over the past 10 seasons. The 2022-2023 Vancouver Canucks are shown with the dark blue line (HDSAR = 0.3220996) and the 95th percentile is shown in red.\nIn terms of the semantics of Data Visualization, I am trying to DISCOVER the DISTRIBUTION of High Danger Shots Allowed, and then ANNOTATE the DISTRIBUTION with information about the current Vancouver Canucks team and their approximate percentile in the distribution.\nTo visualize this I am encoding a DERIVED attribute - the relative density of HDSAR as VERTICAL LENGTH of a bar (technically the MARK is a LINE) on an aligned axis. This is a pretty powerful encoding, which is why it is very common when visualizing distributions. In addtion, I am encoding HDSAR values themselves with HORIZONTAL POSITION. A nice, ordered channel for a nice, ordered attribute. The DENSITY FUNCTION is less clear to me. I’m obviously using a line, whose vertical and horizontal position encodes the relationship between the two variables. But the blue shaded fill? I kind of think that is using an AREA channel and REDUNDANT ENCODING to send a stronger signal.\nThe annotations of the 95th percentile and the 2022 Canucks’ HDSAR are encoded with COLOR and HORIZONTAL POSITION CHANNELS, and LINES as MARKS.\nThe 2022 Canucks indeed seem to be giving up HDS at a historically high rate."
  },
  {
    "objectID": "posts/T5-MarksChannels/index.html#goaltending",
    "href": "posts/T5-MarksChannels/index.html#goaltending",
    "title": "TUTORIAL 5 - Marks and Channels",
    "section": "",
    "text": "But wait! You exclaim, shouting your objection into the uncaring Abyss. What about the goaltending? Surely that has also contributed to this season’s futility? “Perhaps..”, the Abyss murmurs back…\nLet’s see whether the futility of the penalty kill is also a function of bad goaltending. Now, we can’t just look at raw goals allowed, because if the defense is allowing really High Danger Shots, then even a good goaltender is going to allow some goals. Fortunately, the NHL nerds have developed an Expected Goals metric. Moneypuck provides this handy description in their data dictionary:\nExpected Goals:. The sum of the probabilities of unblocked shot attempts being goals. For example, a rebound shot in the slot may be worth 0.5 expected goals, while a shot from the blueline while short handed may be worth 0.01 expected goals. The expected value of each shot attempt is calculated by the MoneyPuck Expected Goals model. Expected goals is commonly abbreviated as xGoals. Blocked shot attempts are valued at 0 xGoals.\nCool!\n\n\nCode\n  Tending&lt;-allseasons[c(\"situation\", \"teamseason\", \"goalsAgainst\", \"xGoalsAgainst\", \"shotsOnGoalAgainst\", \"games_played\", \"iceTime\", \"highDangerShotsAgainst\")]\nTending$GArate&lt;-Tending$xGoalsAgainst/Tending$iceTime*120\n  PKTending&lt;-subset(Tending, situation == \"4on5\")\n\n  d &lt;- density(PKTending$GArate)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKTending$GArate)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKTending$GArate, 0.95)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22GA &lt;- PKTending$GArate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=GArate))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.005, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22GA, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Expected Goals Allowed while killing penalties (rate per 2 minutes)\")  \n\n\n\n\n\n\n\n\n\nThis figure uses Expected Goals. It shows that Vancouver is playing a defense that is generating a historically high amount of expected goals, a result in line with our analysis of High Danger Chances.\nTo truly evaluate the goaltending, tt might be better for us to look at a ratio of Expected Goals to actual Goals. If this number is close to 1, it indicates an expected level of NHL goaltending during the penalty kill. The goalies are making the saves that they should. If the number is higher than 1, it means the goaltenders are making more saves than they should and are playing well. If the number is lower than 1, it means the goaltenders are letting in easy goals.\n\n\nCode\nTending$XGrate&lt;-Tending$xGoalsAgainst/Tending$goalsAgainst\n  PKTending&lt;-subset(Tending, situation == \"4on5\")\n\n  d &lt;- density(PKTending$XGrate)\n\n# Step 3: Create an ECDF\ne &lt;- ecdf(PKTending$XGrate)\n\n# Step 4: Calculate the quantile value\nq &lt;- quantile(PKTending$XGrate, 0.05)\n\n# Step 5: Calculate the empirical cumulative probability for the quantile value\np &lt;- e(q)\n\n# Step 6: Estimate the density value for the quantile value\nf &lt;- approxfun(d$x, d$y)\nd_q &lt;- f(q)\n\n# Step 7: Combine the results into a list\npercentile &lt;- list(q = q, p = p, d = d_q)\n\n\nVAN22XG &lt;- PKTending$XGrate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=XGrate))+\n  geom_histogram(aes(y = ..density..), binwidth = 0.01, color = \"black\", fill=\"white\")+\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)+\n  geom_vline(xintercept = VAN22XG, color = \"blue\", size=1.0)+\n  geom_vline(xintercept = q, color = \"red\", size = 1, linetype=\"dotted\")+\n  xlab(\"Expected Goals / Goals Allowed ratio while killing penalties\")  \n\n\n\n\n\n\n\n\n\nFigure 3. Distribution of Expected Goals to Goals Allowed during 4 on 5 situations for NHL teams over the past 10 seasons. The 2022-2023 Vancouver Canucks are shown with the dark blue line (XGrate = 0.6963636) and the 5th percentile is shown in red.\nUh Oh. Not only is the defense giving up a historically high number of High Danger scoring chances, the goaltenders are performing well below expectation. They are making about 70% of the saves they are supposed to make. So…. yeah. Why is the penalty kill so bad? Bad defense and bad goaltending. I mean, HISTORICALLY BAD.\nIs it because of the system? I don’t think so. I think it is because of the players. They don’t have enough defensive talent on the team to effectively kill penalties. I am sad."
  },
  {
    "objectID": "posts/T5-MarksChannels/index.html#changing-channels",
    "href": "posts/T5-MarksChannels/index.html#changing-channels",
    "title": "TUTORIAL 5 - Marks and Channels",
    "section": "",
    "text": "Now, lets mess this up! First I’m going back to the HDSAR figure, and changing some of my choices for marks and channels.\n\n\nCode\nggplot(PKsituation, aes(x=teamseason, y=HDSAR)) +\n  geom_bar(stat=\"identity\", aes(fill=name, alpha=season)) +\n  xlab(\"NHL Team and Season\") +\n  ylab(\"High Danger Shots Allowed (rate per 2 minutes)\") +\n  ggtitle(\"Are the Vancouver Canucks Historically Bad?\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 0.1),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nFigure 2. Distribution of High Danger Shot Rate for NHL teams over the past 10 seasons.\nHoly Crap. What an absolute nightmate. In terms of MARKS, I’ve stayed with LINES. But the CHANNELS… wow. The COLOR CHANNEL is used to indicate the team. The ALPHA CHANNEL (transparency) is used to indicate season (time). These choices create HUGE problems related to discriminability (alpha channel) and separability. In addition, my choice to order these lines according to team and season is not at all compatible with my choice of channels. Not to mention that the important thing - to see where the 2022 Canucks stand in terms of historical futility - is completely wrecked because the horizontal position (a strong channel) is not closely tied to the most salient Attribute (HDSAR). Go ahead and pick out the the 2022 Canucks from this figure…. I’ll wait.\n\n\nFrom Munzner, 5.4.1:\nThe expressiveness principle: Visual encoding should express all of, and only, the information in the dataset attributes. Ordered data should be shown in a way that our perceptual system intrinsically senses as ordered. Well, using color sure violates that idea! Conversely, unordered data should not be shown in a way that perceptually implies an ordering that does not exist.\nThe effectiveness principle: The importance of the attribute should match the salience of the channel (its noticeablity). The most important attributes should be encoded with the most effective channels. Definitely not! Even though the Line Marks are using Length on an aligned axis, their width and ordering on the x axis completely obscures the focal team.\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1 and 2), create a visualization called Figure 3 that uses a magnitude channel with a number of bins that facilitate discriminiability of an attribute. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 4 that uses this same channel for WAY TOO MANY BINS, violating the guidelines for discriminability. Write a caption that explains why this number of bins is bad compared to Figure 3. Look at the NHL Rainbow! So many colors… so little ability to differentiate between teams. For example, go ahead and try to pick out Edmonton vs Florida\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1-4), create a visualization called Figure 5 that uses a two or more channels to encode two or more attributes while maintaining separability. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 6 that uses channels that are integral, or at least much less separable. Write a caption that explains why choosing these channels is bad compared to Figure 5.\nFor this one I’d like to try something else. Let’s look at the Expected Goals and Allowed Goals as a scatterplot:\n\n\nCode\nV22&lt;-subset(PKTending, teamseason == \"VAN 2022\")\nVAN22XG &lt;- PKTending$XGrate[PKTending$teamseason == \"VAN 2022\"]\n    \nggplot(PKTending, aes(x=xGoalsAgainst/games_played*82, y =goalsAgainst/games_played*82 ))+\n  geom_point(aes(alpha=highDangerShotsAgainst, size=highDangerShotsAgainst))+\n  geom_abline(intercept = 0, slope = 1)+\n  geom_abline(intercept = 90, slope = -1)+\n \n  xlim(0,100)+\n  ylim(0,100)+\n  geom_vline(xintercept = mean(PKTending$xGoalsAgainst/PKTending$games_played*82), color = \"blue\", size=1.0, linetype=\"dotted\")+\n  geom_hline(yintercept = mean(PKTending$goalsAgainst/PKTending$games_played*82), color = \"blue\", size = 1, linetype=\"dotted\")+\n  geom_point(data = V22, aes(x=xGoalsAgainst/games_played*82, y =goalsAgainst/games_played*82),\n             color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nUsing whatever data set you choose (it need not be the same data used for Figures 1-6… you get the idea), create a visualization called Figure 7 that effectively uses the concept of popout. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 8 that makes the identification task in Figure 7 much more difficult. Write a caption that explains why Figure 8 sucks compared to Figure 7. The popout concept is best shown by my use of a dark blue (color) line mark to indicate the 22 Canucks. Without that, you cannot achieve the task because the 22 Canucks are too hard to find without doing a serial search."
  },
  {
    "objectID": "posts/T2-Anscombe/index.html",
    "href": "posts/T2-Anscombe/index.html",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "",
    "text": "Do the summary statistics reveal the truth? Or are they FILLED WITH LIES? A simple demonstration with Anscombe’s Quartet.\nAgain, the video is a couple years old. Expect a few dated references."
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#more-quarto",
    "href": "posts/T2-Anscombe/index.html#more-quarto",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "",
    "text": "Do the summary statistics reveal the truth? Or are they FILLED WITH LIES? A simple demonstration with Anscombe’s Quartet.\nAgain, the video is a couple years old. Expect a few dated references."
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#the-data",
    "href": "posts/T2-Anscombe/index.html#the-data",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "The Data",
    "text": "The Data\nAnscombe’s Quartet is comprised of four pairs of x,y data:\n\n\nCode\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(datasets)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\n\n\nCode\ndatasets::anscombe\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89"
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#example-hypotheses",
    "href": "posts/T2-Anscombe/index.html#example-hypotheses",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Example Hypotheses",
    "text": "Example Hypotheses\n\nBetsy has four replicates in which she is measuring the abundance of two bacteria (x, y) on 11 milk samples.\nPriya has four replicates in which she is measuring gene expression of two genes (x, y) in 11 different pipefish tissue samples.\nHenry has four replicate years in which he is measuring the abundance of an insect pest (x) and crop productivity (y) in 11 fields.\n\nYour hypothesis is that the four replicates do not differ in the correlation between x and y."
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#summary-statistics",
    "href": "posts/T2-Anscombe/index.html#summary-statistics",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\ntidy_anscombe &lt;- anscombe %&gt;%\n pivot_longer(cols = everything(),\n              names_to = c(\".value\", \"set\"),\n              names_pattern = \"(.)(.)\")\ntidy_anscombe_summary &lt;- tidy_anscombe %&gt;%\n  group_by(set) %&gt;%\n  summarise(across(.cols = everything(),\n                   .fns = lst(min,max,median,mean,sd,var),\n                   .names = \"{col}_{fn}\"))\n#&gt; `summarise()` ungrouping output (override with `.groups` argument)\n\nvars&lt;-c(\"set\", \"x_mean\", \"x_var\",  \"y_mean\", \"y_var\")\nthing&lt;- as.data.frame(tidy_anscombe_summary[vars])\nknitr::kable(thing)\n\n\n\n\n\nset\nx_mean\nx_var\ny_mean\ny_var\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249"
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#visualization-reveals-hidden-patterns",
    "href": "posts/T2-Anscombe/index.html#visualization-reveals-hidden-patterns",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Visualization reveals hidden patterns!",
    "text": "Visualization reveals hidden patterns!\n\n\nCode\nggplot(tidy_anscombe,\n       aes(x = x,\n           y = y)) +\n  geom_point() +\n  geom_point(data = tidy_anscombe_summary, aes(x=x_mean, y = y_mean, color = \"red\", size = 5),\n             show.legend = FALSE)+\n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/T8-SpatialDataExample/index.html",
    "href": "posts/T8-SpatialDataExample/index.html",
    "title": "BCB 520 - Spatial Data Practical Exercise",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\n\n\nZOMBIES.\nZombies have long been a frequent trope of horror fiction. The concept of zombies first emerged from Haitian folklore and Voodoo, where they were depicted as reanimated corpses that were controlled by a sorcerer or bokor. However, in modern fiction, the magical or supernatural origin of zombies has been largely replaced by a scientific one. Zombies are now typically portrayed as mindless, flesh-eating creatures that are reanimated by a virus or some other infectious agent.\nThe evolution of zombies as a horror trope can be divided into several distinct eras:\n\nClassic Zombies: The classic zombie was the original Haitian zombie, which was introduced to Western audiences in the early 20th century through literature and film. These zombies were depicted as slow-moving, mind-controlled creatures that were raised from the dead by Voodoo magic.\nRomero Zombies: George A. Romero’s 1968 film “Night of the Living Dead” redefined the zombie genre by introducing the idea that zombies were reanimated by a mysterious virus that spread through bites and scratches. Romero’s zombies were slow-moving, flesh-eating creatures that could only be killed by destroying the brain. Many modern zombie franchises (e.g. Resident Evi, Walking Dead) still use the classic shambling undead Romeero Zombie concept.\nFast Zombies: In the early 2000s, a new type of zombie emerged in fiction that could run and move at incredible speeds. These fast zombies were popularized by films like 28 Days Later and World War Z. Fast zombies are often depicted as being more aggressive and intelligent than their slow-moving counterparts. Note that in many of these zombie franchises, the zombies were still created by an infectious agent such as a virus.\nPost-Apocalyptic Zombies: In recent years, zombies have been featured in a number of post-apocalyptic settings, where they are often portrayed as the cause of a global pandemic that has devastated humanity. These stories often focus on the struggle of survivors to rebuild civilization in a world overrun by the undead. These can be any of the above types (Classic, Romero, Fast) of zombie. This is the scenario that motivates this Blog post.\n\nWould an outbreak of a “zombie virus” actually consume the world and bring forth an apocalyptic new age of shambling horror?\nLet’s use disease modeling to find out!"
  },
  {
    "objectID": "posts/T8-SpatialDataExample/index.html#preamble",
    "href": "posts/T8-SpatialDataExample/index.html#preamble",
    "title": "BCB 520 - Spatial Data Practical Exercise",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\n\n\nZOMBIES.\nZombies have long been a frequent trope of horror fiction. The concept of zombies first emerged from Haitian folklore and Voodoo, where they were depicted as reanimated corpses that were controlled by a sorcerer or bokor. However, in modern fiction, the magical or supernatural origin of zombies has been largely replaced by a scientific one. Zombies are now typically portrayed as mindless, flesh-eating creatures that are reanimated by a virus or some other infectious agent.\nThe evolution of zombies as a horror trope can be divided into several distinct eras:\n\nClassic Zombies: The classic zombie was the original Haitian zombie, which was introduced to Western audiences in the early 20th century through literature and film. These zombies were depicted as slow-moving, mind-controlled creatures that were raised from the dead by Voodoo magic.\nRomero Zombies: George A. Romero’s 1968 film “Night of the Living Dead” redefined the zombie genre by introducing the idea that zombies were reanimated by a mysterious virus that spread through bites and scratches. Romero’s zombies were slow-moving, flesh-eating creatures that could only be killed by destroying the brain. Many modern zombie franchises (e.g. Resident Evi, Walking Dead) still use the classic shambling undead Romeero Zombie concept.\nFast Zombies: In the early 2000s, a new type of zombie emerged in fiction that could run and move at incredible speeds. These fast zombies were popularized by films like 28 Days Later and World War Z. Fast zombies are often depicted as being more aggressive and intelligent than their slow-moving counterparts. Note that in many of these zombie franchises, the zombies were still created by an infectious agent such as a virus.\nPost-Apocalyptic Zombies: In recent years, zombies have been featured in a number of post-apocalyptic settings, where they are often portrayed as the cause of a global pandemic that has devastated humanity. These stories often focus on the struggle of survivors to rebuild civilization in a world overrun by the undead. These can be any of the above types (Classic, Romero, Fast) of zombie. This is the scenario that motivates this Blog post.\n\nWould an outbreak of a “zombie virus” actually consume the world and bring forth an apocalyptic new age of shambling horror?\nLet’s use disease modeling to find out!"
  },
  {
    "objectID": "posts/T8-SpatialDataExample/index.html#data",
    "href": "posts/T8-SpatialDataExample/index.html#data",
    "title": "BCB 520 - Spatial Data Practical Exercise",
    "section": "Data",
    "text": "Data\nI’m going to use our new interactive simulation, OUTBREAK SIMULATOR, to understand the dynamics of a zombie virus outbreak.\n\nParameters\nIn order to model this outbreak, we’ll need to set some of the classic parameters of an SIR compartment model. Outbreak Simulator uses a compartment model of disease (Weissman et al., 2020) in which the population is divided into categories (Figure 1): Susceptible (S), Exposed (E), Infected (I), Vaccinated (V), Recovered (R), or Deceased (D). The model estimates the rates of exchange between categories over a given time interval (t) using differential equations. When the model parameters are known and key assumptions are met, the differential equations can estimate the epidemic curve of an outbreak. The two most critical assumptions are that the population is homogeneous and well mixed and is fixed in size.\n\nI’ve provided my own estimates of these parameters for various infectious zombie franchises in the table below.\n\n\nCode\nparams&lt;-read_xlsx(\"params.xlsx\")\n\nknitr::kable(params)\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDefinition\nResident Evil\nWorld War Z\nWalking Dead\n\n\n\n\nBETA\nRate of transmission\n19.8\n198\n20\n\n\ncontact rate\nNumber of infectious contacts (bites) per hour\n20\n200\n20\n\n\ninfection probability\nChance that a bite causes infection\n0.99\n0.99\n1\n\n\nALPHA\nTransformation rate\n0.5\n1000\n2.0833333333333332E-2\n\n\nlatency\naverage number of hours until transformation to Infected\n2\n1E-3\n48\n\n\nGAMMA\nRecovery rate\n0\n0\n0\n\n\nDELTA\nMortality rate\n1\n1\n1\n\n\ninfection duration\naverage lifespan of a zombie in hours\n250\n250\n250\n\n\nSIGMA\nVaccination rate\n0\n0\n0\n\n\nR0 Initial\nInitial basic reproduction rate of the virus\n4950\n49500\n5000\n\n\nSpeed\nHow fast can the zombies move?\nSlow\nFast\nSlow\n\n\n\n\n\n\n\nData Dictionary\nOutbreak Simulator runs the compartment model on a spatially explicit grid of the continental US. It writes the number of individuals in each compartment (SIERD) at each time step (an hour) for each of the 48 states and the total population.\nThis is a video of the simulation:\nVideo\n\n\nCode\ndf&lt;-read.csv(\"ZombieData.csv\")\n\n\nI’ll manipulate the raw data a bit to get to the visualizations I need. First, I want a tidy data set with only the total US population.\n\n\nCode\ndf&lt;-df%&gt;%\n  mutate_at(c(1:295), as.numeric)\n\ndftotal &lt;- df%&gt;%\n  select(Time, starts_with(\"Totals_\"))%&gt;%\n  rename(Time=Time,\n          S=Totals_S,\n         E=Totals_E,\n         V=Totals_V,\n         I=Totals_I,\n         R=Totals_R,\n         D=Totals_D)\n\nfor (i in 1:length(dftotal$Time)){\n  \n  dftotal$dS[i] &lt;- dftotal$S[i]-dftotal$S[i+1]\n  dftotal$dR[i] &lt;- dftotal$R[i+1]-dftotal$R[i]\n  dftotal$dI[i] &lt;- dftotal$I[i+1]-dftotal$I[i]\n  dftotal$dD[i] &lt;- dftotal$D[i+1]-dftotal$D[i]\n\n}\n\ndftotal&lt;- dftotal%&gt;%\n  mutate(N= S+E+I+V+R+D)%&gt;%\n  mutate(Beta = dS*N/(S*I+1))%&gt;%\n  mutate(Gamma = dD/(I+1))%&gt;%\n  mutate(R0 = Beta/Gamma)%&gt;%\n  filter(Beta&lt;10)%&gt;%\n  filter(R0&lt;10^3)\n\ndflong&lt;-dftotal%&gt;%\n  pivot_longer(cols = c(\"S\", \"E\", \"I\",\n                        \"V\", \"R\", \"D\"),names_to = \"Compartment\", values_to = \"Count\")\n\ndflong &lt;- dflong%&gt;%\n  mutate(Compartment = recode(Compartment, \n         S = \"Susceptible\",\n         E = \"Exposed\",\n         I = \"Infected\",\n         V = \"Vaccinated\",\n         R = \"Recovered\",\n         D = \"Deceased\"))%&gt;%\n  filter(Compartment != \"Vaccinated\")%&gt;%\n  filter(Compartment != \"Recovered\")\n\n\nThis allows me to produce the classic Epidemilogical Curve:\n\n\nCode\nggplot(dflong, aes(x=Time, y = Count/10^6, color = Compartment))+\n  geom_line()+\n   labs(\n    title = \"Epidemiological Curve\",\n    subtitle = \"Resident Evil - T Virus\",\n    caption = \"Data from a single run of Outbreak Simulator.\",\n    tag = \"Figure 1\",\n    x = \"Time (h)\",\n    y = \"Number of Individuals (millions)\",\n    colour = \"Compartment\"\n  )+\n   scale_colour_brewer(type = \"seq\", palette = \"Spectral\")"
  },
  {
    "objectID": "posts/T8-SpatialDataExample/index.html#state-level-dynamics",
    "href": "posts/T8-SpatialDataExample/index.html#state-level-dynamics",
    "title": "BCB 520 - Spatial Data Practical Exercise",
    "section": "STATE LEVEL DYNAMICS",
    "text": "STATE LEVEL DYNAMICS\nWe can use the output from the simulation to show the Epidemilogical Curve for each state, but the problem is the number of states. There are 48 in the continental US - too many to ask a user to meaningfully process.\nHere are the curves for four states - the ones that collaborate on the Tickbase project that funds this work.\n\n\nCode\nstateslong &lt;- df %&gt;%\n  pivot_longer(cols = 2:295,\n               names_to = c(\"State\", \"Compartment\"),\n  names_pattern = \"(.+?)_(.)\",\n  values_to = \"Count\")\n\nfewerstates&lt;-stateslong%&gt;%\n  filter(State == \"New.Mexico\" | State == \"Idaho\"\n         | State ==\"Nevada\" | State == \"New.Hampshire\"\n         )%&gt;%\n    mutate(Compartment = recode(Compartment, \n         S = \"Susceptible\",\n         E = \"Exposed\",\n         I = \"Infected\",\n         V = \"Vaccinated\",\n         R = \"Recovered\",\n         D = \"Deceased\"))%&gt;%\n  filter(Compartment != \"Vaccinated\")%&gt;%\n  filter(Compartment != \"Recovered\")\n\n\nggplot(fewerstates, aes(x=Time, y = Count/10^6, color = Compartment))+\n  geom_line()+\n  facet_wrap(~State)+\n   labs(\n    title = \"Epidemiological Curve\",\n    subtitle = \"H1N1 Influenza\",\n    caption = \"Data from a single run of Outbreak Simulator.\",\n    tag = \"Figure 2\",\n    x = \"Time (h)\",\n    y = \"Number of Individuals (millions)\",\n    colour = \"Compartment\"\n  )+\n   scale_colour_brewer(type = \"seq\", palette = \"Spectral\")\n\n\n\n\n\n\n\n\n\nCode\nstatecolor&lt;-stateslong%&gt;%\n  filter(Compartment == \"S\" & State != \"Totals\" & Time == 0)%&gt;%\n  mutate(rank = rank(Count))\n\nstateslong2 &lt;- left_join(stateslong, statecolor, by = c(\"State\",\n                                                        \"Compartment\"))\n\nZomstates &lt;- stateslong2 %&gt;%\n  filter(Compartment == \"S\" & State != \"Totals\")%&gt;%\n  mutate(date = as_date(Time.x),\n         name = State,\n         category = rank,\n         value = Count.x)%&gt;%\n  select(c(8:11))\n\n\n\n\n\nwrite.csv(Zomstates, \"Zomstates.csv\")"
  },
  {
    "objectID": "posts/T8-SpatialDataExample/index.html#visualizations",
    "href": "posts/T8-SpatialDataExample/index.html#visualizations",
    "title": "BCB 520 - Spatial Data Practical Exercise",
    "section": "VISUALIZATIONS",
    "text": "VISUALIZATIONS\nI’m really interested in an animated visualization that captures the changing population dynamics in each state. I’m going to use Observable for this, modifying an existing workbook.\n\n\nCode\ndata = d3.csvParse(await FileAttachment(\"Zomstates.csv\").text(), d3.autoType)\n\nviewof replay = html`&lt;button&gt;Replay`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nchart = {\n  \n  replay;\n\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height]);\n\n  const updateBars = bars(svg);\n  const updateAxis = axis(svg);\n  const updateLabels = labels(svg);\n  const updateTicker = ticker(svg);\n\n  yield svg.node();\n\n  for (const keyframe of keyframes) {\n    const transition = svg.transition()\n        .duration(duration)\n        .ease(d3.easeLinear);\n\n    // Extract the top bar’s value.\n    x.domain([0, keyframe[1][0].value]);\n\n    updateAxis(keyframe, transition);\n    updateBars(keyframe, transition);\n    updateLabels(keyframe, transition);\n    updateTicker(keyframe, transition);\n\n    invalidation.then(() =&gt; svg.interrupt());\n    await transition.end();\n  }\n}\n\n\nduration = 25\nn = 50\nk = 10\nnames = new Set(data.map(d =&gt; d.name))\n\n\ndatevalues = Array.from(d3.rollup(data, ([d]) =&gt; d.value, d =&gt; +d.date, d =&gt; d.name))\n  .map(([date, data]) =&gt; [new Date(date), data])\n  .sort(([a], [b]) =&gt; d3.ascending(a, b))\n  \n  \nfunction rank(value) {\n  const data = Array.from(names, name =&gt; ({name, value: value(name)}));\n  data.sort((a, b) =&gt; d3.descending(a.value, b.value));\n  for (let i = 0; i &lt; data.length; ++i) data[i].rank = Math.min(n, i);\n  return data;\n}\n\nkeyframes = {\n  const keyframes = [];\n  let ka, a, kb, b;\n  for ([[ka, a], [kb, b]] of d3.pairs(datevalues)) {\n    for (let i = 0; i &lt; k; ++i) {\n      const t = i / k;\n      keyframes.push([\n        new Date(ka * (1 - t) + kb * t),\n        rank(name =&gt; (a.get(name) || 0) * (1 - t) + (b.get(name) || 0) * t)\n      ]);\n    }\n  }\n  keyframes.push([new Date(kb), rank(name =&gt; b.get(name) || 0)]);\n  return keyframes;\n}\n\nnameframes = d3.groups(keyframes.flatMap(([, data]) =&gt; data), d =&gt; d.name)\n\nprev = new Map(nameframes.flatMap(([, data]) =&gt; d3.pairs(data, (a, b) =&gt; [b, a])))\n\nnext = new Map(nameframes.flatMap(([, data]) =&gt; d3.pairs(data)))\n\nfunction bars(svg) {\n  let bar = svg.append(\"g\")\n      .attr(\"fill-opacity\", 0.6)\n    .selectAll(\"rect\");\n\n  return ([date, data], transition) =&gt; bar = bar\n    .data(data.slice(0, n), d =&gt; d.name)\n    .join(\n      enter =&gt; enter.append(\"rect\")\n        .attr(\"fill\", color)\n        .attr(\"height\", y.bandwidth())\n        .attr(\"x\", x(0))\n        .attr(\"y\", d =&gt; y((prev.get(d) || d).rank))\n        .attr(\"width\", d =&gt; x((prev.get(d) || d).value) - x(0)),\n      update =&gt; update,\n      exit =&gt; exit.transition(transition).remove()\n        .attr(\"y\", d =&gt; y((next.get(d) || d).rank))\n        .attr(\"width\", d =&gt; x((next.get(d) || d).value) - x(0))\n    )\n    .call(bar =&gt; bar.transition(transition)\n      .attr(\"y\", d =&gt; y(d.rank))\n      .attr(\"width\", d =&gt; x(d.value) - x(0)));\n}\n\n\nfunction labels(svg) {\n  let label = svg.append(\"g\")\n      .style(\"font\", \"bold 12px var(--sans-serif)\")\n      .style(\"font-variant-numeric\", \"tabular-nums\")\n      .attr(\"text-anchor\", \"end\")\n    .selectAll(\"text\");\n\n  return ([date, data], transition) =&gt; label = label\n    .data(data.slice(0, n), d =&gt; d.name)\n    .join(\n      enter =&gt; enter.append(\"text\")\n        .attr(\"transform\", d =&gt; `translate(${x((prev.get(d) || d).value)},${y((prev.get(d) || d).rank)})`)\n        .attr(\"y\", y.bandwidth() / 2)\n        .attr(\"x\", -6)\n        .attr(\"dy\", \"-0.25em\")\n        .text(d =&gt; d.name)\n        .call(text =&gt; text.append(\"tspan\")\n          .attr(\"fill-opacity\", 0.7)\n          .attr(\"font-weight\", \"normal\")\n          .attr(\"x\", -6)\n          .attr(\"dy\", \"1.15em\")),\n      update =&gt; update,\n      exit =&gt; exit.transition(transition).remove()\n        .attr(\"transform\", d =&gt; `translate(${x((next.get(d) || d).value)},${y((next.get(d) || d).rank)})`)\n        .call(g =&gt; g.select(\"tspan\").tween(\"text\", d =&gt; textTween(d.value, (next.get(d) || d).value)))\n    )\n    .call(bar =&gt; bar.transition(transition)\n      .attr(\"transform\", d =&gt; `translate(${x(d.value)},${y(d.rank)})`)\n      .call(g =&gt; g.select(\"tspan\").tween(\"text\", d =&gt; textTween((prev.get(d) || d).value, d.value))))\n}\n\nfunction textTween(a, b) {\n  const i = d3.interpolateNumber(a, b);\n  return function(t) {\n    this.textContent = formatNumber(i(t));\n  };\n}\n\nformatNumber = d3.format(\",d\")\n\nfunction axis(svg) {\n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(0,${margin.top})`);\n\n  const axis = d3.axisTop(x)\n      .ticks(width / 160)\n      .tickSizeOuter(0)\n      .tickSizeInner(-barSize * (n + y.padding()));\n\n  return (_, transition) =&gt; {\n    g.transition(transition).call(axis);\n    g.select(\".tick:first-of-type text\").remove();\n    g.selectAll(\".tick:not(:first-of-type) line\").attr(\"stroke\", \"white\");\n    g.select(\".domain\").remove();\n  };\n}\n\nfunction ticker(svg) {\n  const now = svg.append(\"text\")\n      .style(\"font\", `bold ${barSize}px var(--sans-serif)`)\n      .style(\"font-variant-numeric\", \"tabular-nums\")\n      .attr(\"text-anchor\", \"end\")\n      .attr(\"x\", width - 6)\n      .attr(\"y\", margin.top + barSize * (n - 0.45))\n      .attr(\"dy\", \"0.32em\")\n      .text(formatDate(keyframes[0][0]));\n\n  return ([date], transition) =&gt; {\n    transition.end().then(() =&gt; now.text(formatDate(date)));\n  };\n}\n\nformatDate = d3.utcFormat(\"%Y\")\n\ncolor = {\n  const scale = d3.scaleSequential(d3.interpolate(\"red\", \"blue\")).domain([1, 48]);\n  if (data.some(d =&gt; d.category !== undefined)) {\n    const categoryByName = new Map(data.map(d =&gt; [d.name, d.category]))\n    scale.domain(Array.from(categoryByName.values()));\n    return d =&gt; scale(categoryByName.get(d.name));\n  }\n  return d =&gt; scale(d.name);\n}\n\n\n&lt;!-- color = { --&gt;\n&lt;!--   const scale = d3.scaleSequential(d3.interpolate(\"red\", \"blue\")).domain([1, 48]); --&gt;\n&lt;!--   if (data.some(d =&gt; d.category !== undefined)) { --&gt;\n&lt;!--     const categoryByName = new Map(data.map(d =&gt; [d.name, d.category])); --&gt;\n&lt;!--     const categories = Array.from(categoryByName.values()).filter((d, i, arr) =&gt; arr.indexOf(d) === i); --&gt;\n&lt;!--     const scaleByCategory = typeof categories[0] === \"number\" ?  --&gt;\n&lt;!--       d3.scaleSequential(d3.interpolateSpectral).domain(d3.extent(categories)) : --&gt;\n&lt;!--       d3.scaleOrdinal().domain(categories).range(d3.quantize(d3.interpolateSpectral, categories.length)); --&gt;\n&lt;!--     return d =&gt; scale(scaleByCategory(categoryByName.get(d.name))); --&gt;\n&lt;!--   } --&gt;\n&lt;!--   return (d, i) =&gt; scale(i); --&gt;\n&lt;!-- } --&gt;\n\nx = d3.scaleLinear([0, 1], [margin.left, width - margin.right])\n\ny = d3.scaleBand()\n    .domain(d3.range(n + 1))\n    .rangeRound([margin.top, margin.top + barSize * (n + 1 + 0.1)])\n    .padding(0.1)\n    \nheight = margin.top + barSize * n + margin.bottom\n\nbarSize = 48\n\nmargin = ({top: 16, right: 6, bottom: 6, left: 0})\n\nd3 = require(\"d3@6\")"
  },
  {
    "objectID": "posts/T8-SpatialDataExample/index.html#summary",
    "href": "posts/T8-SpatialDataExample/index.html#summary",
    "title": "BCB 520 - Spatial Data Practical Exercise",
    "section": "SUMMARY",
    "text": "SUMMARY\nIt is pretty clear from the simulations that an outbreak of the T-Virus is really bad news for the continental United States. We go from ~308 million people to ~10 million people in about 500 hours. In addition, the most populated states at the beginning of the outbreak are hit disproportionately hard, presumably because their population density helps sustain the infection."
  },
  {
    "objectID": "posts/T8-SpatialDataExample/index.html#expand-to-spatial-analsysis",
    "href": "posts/T8-SpatialDataExample/index.html#expand-to-spatial-analsysis",
    "title": "BCB 520 - Spatial Data Practical Exercise",
    "section": "EXPAND TO SPATIAL ANALSYSIS",
    "text": "EXPAND TO SPATIAL ANALSYSIS\n\n\nCode\nlibrary(ggplot2)\nlibrary(sf)\n\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nCode\nlibrary(dplyr)\n\n\n\n\nCode\nus_counties &lt;- tigris::counties(cb = TRUE, resolution = \"20m\", year = 2020, class = \"sf\")\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\nset.seed(123)\ndata &lt;- data.frame(\n  GEOID = us_counties$GEOID,\n  value = runif(length(us_counties$GEOID), 0, 100)\n)\n\n\nus_counties_data &lt;- left_join(us_counties, data, by = \"GEOID\")\n\nggplot() +\n  geom_sf(data = us_counties_data, aes(fill = value), color = \"white\", size = 0.1) +\n  scale_fill_viridis_c() +  # You can choose other color palettes\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nCode\nus_counties_contiguous &lt;- us_counties %&gt;% \n  filter(\n    !(STATEFP %in% c(\"02\", \"15\", \"60\", \"66\", \"69\", \"72\", \"78\"))\n  )\n\nus_counties_data_contiguous &lt;- left_join(us_counties_contiguous, data, by = \"GEOID\")\n\n\nggplot() +\n  geom_sf(data = us_counties_data_contiguous, aes(fill = value), color = \"white\", size = 0.1) +\n  scale_fill_viridis_c() +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank())"
  },
  {
    "objectID": "posts/A5-TabularData/index.html",
    "href": "posts/A5-TabularData/index.html",
    "title": "ASSIGNMENT 5",
    "section": "",
    "text": "In this assignment, we are going to practice creating visualizations for tabular data. Unlike previous assignments, however, this time we will all be using the same data sets. I’m doing this because I want everyone to engage in the same logic process and have the same design objectives in mind."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#hart-memorial-trophy",
    "href": "posts/A5-TabularData/index.html#hart-memorial-trophy",
    "title": "ASSIGNMENT 5",
    "section": "Hart Memorial Trophy",
    "text": "Hart Memorial Trophy\nAwarded to the “player judged most valuable to his team.” This isn’t necessarily the best overall player, but rather the one who contributes most significantly to his team’s success."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#vezina-trophy",
    "href": "posts/A5-TabularData/index.html#vezina-trophy",
    "title": "ASSIGNMENT 5",
    "section": "Vezina Trophy",
    "text": "Vezina Trophy\nPresented to the goaltender “adjudged to be the best at this position.” NHL general managers vote on this award."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#james-norris-memorial-trophy",
    "href": "posts/A5-TabularData/index.html#james-norris-memorial-trophy",
    "title": "ASSIGNMENT 5",
    "section": "James Norris Memorial Trophy",
    "text": "James Norris Memorial Trophy\nAwarded to the defenseman who demonstrates “the greatest all-around ability” at the position."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#calder-memorial-trophy",
    "href": "posts/A5-TabularData/index.html#calder-memorial-trophy",
    "title": "ASSIGNMENT 5",
    "section": "Calder Memorial Trophy",
    "text": "Calder Memorial Trophy\nGiven to the player “adjudged to be the most proficient in his first year of competition.” This is essentially the rookie of the year award."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#frank-j.-selke-trophy",
    "href": "posts/A5-TabularData/index.html#frank-j.-selke-trophy",
    "title": "ASSIGNMENT 5",
    "section": "Frank J. Selke Trophy",
    "text": "Frank J. Selke Trophy\nAwarded to the forward who best excels in the defensive aspects of the game."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#lady-byng-memorial-trophy",
    "href": "posts/A5-TabularData/index.html#lady-byng-memorial-trophy",
    "title": "ASSIGNMENT 5",
    "section": "Lady Byng Memorial Trophy",
    "text": "Lady Byng Memorial Trophy\nPresented to the player who exhibits “the best type of sportsmanship and gentlemanly conduct combined with a high standard of playing ability.”\nNote that I’ve eliminated the awards that are determined by raw counting statistics (most goals, most points). The awards above are all based on votes from NHL media and executives, and the criteria are at least somewhat subjective.\n\nHere is the goal:\nFor each award, create your ballot of five players, ranked from 1 (your first choice) to 5 (your fifth choice). For each ballot, provide one to three visualizations that explain and justify your ballot."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#directions",
    "href": "posts/A5-TabularData/index.html#directions",
    "title": "ASSIGNMENT 5",
    "section": "DIRECTIONS",
    "text": "DIRECTIONS\nCreate a new post in your portfolio for this assignment. Call it something cool, like NHL player award analysis, or Hockey Analytics, or John Wick….\nCopy the data files from the repository, and maybe also the .qmd file.\nUse the .qmd file as the backbone of your assignment, changing the code and the markdown text as you go."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#the-data",
    "href": "posts/A5-TabularData/index.html#the-data",
    "title": "ASSIGNMENT 5",
    "section": "THE DATA",
    "text": "THE DATA\nAt minimum, we will use the data from naturalstattrick.com for player statistics for the season to date (Oct 2024 to March 2025). You can find the Data Dictionary here\n\n\nCode\nIndivdual.Skater &lt;- read.csv(\"SkaterIndividualstats.csv\")\nOnIce.Skater &lt;- read.csv(\"SkaterOnicestats.csv\")\nGoalie &lt;- read.csv(\"Goalies.csv\")\nIndividual.Skater.Rookie &lt;- read.csv(\"RookieSkaterindividual.csv\")\nOnIce.Skater.Rookie &lt;- read.csv(\"RookieSkaterOnIce.csv\")\nRookie.Goalie &lt;- read.csv(\"RookieGoalies.csv\")\n\n\nThis code reads in the data files into dataframes. You will have to decide which dataframes correspond to which awards. Some awards will be informed by more than one dataframe."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#how-to-get-started",
    "href": "posts/A5-TabularData/index.html#how-to-get-started",
    "title": "ASSIGNMENT 5",
    "section": "HOW TO GET STARTED",
    "text": "HOW TO GET STARTED\nThere is a lot of data here, and you probably don’t need all of it. The counting stats that you should start with are pretty simple. For skaters, it is Total Points, which is the sum of Goals and Assists. Assists can be primary (the person who made the pass to the scorer) or secondary (the person who made the pass to the primary assist player), but I wouldn’t worry about that for now. For goaltenders, it is save percentage and goals against average.\nHOWEVER, counting stats don’t tell the whole story in hockey. Hockey is a low event sport, so goals are somewhat rare. This is where the advanced stats come in to play. These are usually found in the “OnIce” data sets. Here, I’d start with Corsi Statistics and Expected Goals Statistics, which are defined below.\n\nCorsi Statistics\nCorsi: Any shot attempt (goals, shots on net, misses and blocks) outside of the shootout. Referred to as SAT by the NHL.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCF\nCount of Corsi for that player’s team while that player is on the ice.\n\n\nCA\nCount of Corsi against that player’s team while that player is on the ice.\n\n\nCF%\nPercentage of total Corsi while that player is on the ice that are for that player’s team. CF*100/(CF+CA)\n\n\n\n\n\nExpected Goals\nExpected goals (xG) is a statistical measure that evaluates shot quality by assigning a goal probability to each shot attempt.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nxGF\nExpected Goals For. The sum of the probability values of all shot attempts for that player’s team while that player is on the ice. Represents the number of goals the team should have scored based on shot quality.\n\n\nxGA\nExpected Goals Against. The sum of the probability values of all shot attempts against that player’s team while that player is on the ice. Represents the number of goals the team should have conceded based on shot quality.\n\n\nxGF%\nExpected Goals Percentage. The percentage of the total expected goals while that player is on the ice that are for that player’s team. xGF*100/(xGF+xGA)\n\n\n\nLet’s have a look at the distributions of some of these stats!"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#counting-stats-for-skaters",
    "href": "posts/A5-TabularData/index.html#counting-stats-for-skaters",
    "title": "ASSIGNMENT 5",
    "section": "Counting Stats for Skaters",
    "text": "Counting Stats for Skaters\n\n\nCode\nggplot(Indivdual.Skater, aes(x=Total.Points))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Total Points\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Total Points\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(Indivdual.Skater, aes(x=Goals))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Goals Scored\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Goals Scored\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(Indivdual.Skater, aes(x=Total.Assists))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Total Assists\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Total Assists\",\n       subtitle = \"2024-2025 season stats as of March 4\")"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#on-ice-stats-for-skaters",
    "href": "posts/A5-TabularData/index.html#on-ice-stats-for-skaters",
    "title": "ASSIGNMENT 5",
    "section": "On Ice Stats for Skaters",
    "text": "On Ice Stats for Skaters\n\n\nCode\nggplot(OnIce.Skater, aes(x=CF.))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Corsi Percentage\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Corsi Percentage\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(OnIce.Skater, aes(x=as.numeric(GF.)))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Goals For Percentage\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Goals For Percentage\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\nWarning in FUN(X[[i]], ...): NAs introduced by coercion\n\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nCode\nggplot(OnIce.Skater, aes(x=as.numeric(xGF.)))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Expected Goals For Percentage\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Expected Goals For Percentage\",\n       subtitle = \"2024-2025 season stats as of March 4\")"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#stats-for-goalies",
    "href": "posts/A5-TabularData/index.html#stats-for-goalies",
    "title": "ASSIGNMENT 5",
    "section": "Stats for Goalies",
    "text": "Stats for Goalies\n\n\nCode\nggplot(Goalie, aes(x=SV.))+\n  geom_histogram(binwidth = .01)+\n  labs(x = \"Save Percentage\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Save Percentage\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(Goalie, aes(x=GAA))+\n  geom_histogram(binwidth = .2)+\n  labs(x = \"Goals Against Average\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Goals Against Average\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(Goalie, aes(x=GSAA))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Goals Saved Above Average\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Goals Saved Above Average\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nOf all of the awards above, the easiest one to start with is the Vezina Trophy, which is presented to the goaltender “adjudged to be the best at this position.” NHL general managers vote on this award, but its pretty easy to use goaltending stats to inform your vote.\nThink about how our task maps on to our data. What are we trying to do here? What visual channels are best used to accomplish the task?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#the-tidyverse",
    "href": "posts/A5-TabularData/index.html#the-tidyverse",
    "title": "ASSIGNMENT 5",
    "section": "THE TIDYVERSE",
    "text": "THE TIDYVERSE\nI’m using the Tidyverse to manipulate my data. I set up the original data frame to conform to the tidy data principles (every column is a variable, every row is an observation), which is pretty much the base form of how we’ve discussed Tabular Data in class.\nYou are likely to use the functions that FILTER, GROUP, and SUMMARIZE the data, often creating new dataframes for downstream analysis or visualization. Hey, look! A handy cheatsheet for data transformation using the tidyverse!\nThese functions come from the dplyr package that gets installed as part of the tidyverse. The basic categories of actions are:\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names.\nfilter() picks cases based on their values.\nsummarise() reduces multiple values down to a single summary.\narrange() changes the ordering of the rows.\n\nAll of these work with group_by() so you can perform whichever operation on the groups that might be present in your data set."
  },
  {
    "objectID": "posts/E4-NHLBallots/Math.html",
    "href": "posts/E4-NHLBallots/Math.html",
    "title": "Math",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readxl)\nlibrary(pheatmap)\nlibrary(RColorBrewer)\n\n\nMartin Necas Martin Necas\n\n\nCode\njustice&lt;- read_excel(\"justice.xlsx\")%&gt;%\n  mutate(Voter = \"Justice\")\nBarrie&lt;- read_excel(\"Barrie.xlsx\")\nBetsy &lt;- read_excel(\"BetsyChurchHockeyBallot.xlsx\")%&gt;%\n  rename(\n         Player = `Player Name` )%&gt;%\n  rename(Rank = `Nomination position`)%&gt;%\n  rename(Voter = Name)\nClaire &lt;- read.csv(\"Hockey_Ranking_CS.csv\")\nIan&lt;- read_excel(\"IAN HOCKEY NOMINATIONS.xlsx\")%&gt;%\n  rename(Award = Trophy)\nJohn &lt;- read.csv(\"JohnCambareriHockeyAwards.csv\")%&gt;%\n  rename(Rank = Place)%&gt;%\n  mutate(Voter = \"John\")\nMarwa &lt;- read_excel(\"Marwa.xlsx\")\nHenry &lt;- read_excel(\"NHL award ballot HT.xlsx\")\nPriya &lt;- read_excel(\"NHL_Award_Ballet.xlsx\")%&gt;%\n  mutate(Voter = \"Priya\")%&gt;%\n  pivot_longer(\n    cols = c(VEZINA, HART, `JAMES NORRIS`, CALDER, `FRANK J SELKE`, `LADY BING`),\n    names_to = \"Award\",\n    values_to = \"Player\"\n  )%&gt;%\n  select(-Voters)\nSharon &lt;- readRDS(\"sharon_ballot.rds\")\n\nAll_Votes &lt;- bind_rows(Barrie, Sharon, Betsy, Claire, Henry, Ian, John, justice, Marwa, Priya)%&gt;%\n  mutate(\n    Award.Standard = case_when(\n      str_detect(Award, \"(?i)Norris\") ~ \"Norris\",\n      str_detect(Award, \"(?i)Byng\") ~ \"Byng\",\n      str_detect(Award, \"(?i)Selk\") ~ \"Selke\",\n      str_detect(Award, \"(?i)Calder\") ~ \"Calder\",\n      str_detect(Award, \"(?i)Vezina\") ~ \"Vezina\",\n      str_detect(Award, \"(?i)Hart\") ~ \"Hart\",\n       str_detect(Award, \"(?i)Lady\") ~ \"Byng\",\n      str_detect(Award, \"(?i)Heart\") ~ \"Hart\",\n      TRUE ~ \"Other\"\n    ))\n\n\n\n\nCode\n# Add this improved cleaning function before creating the Scores data frame\nstandardize_player_names &lt;- function(name) {\n  # Convert to character if not already\n  name &lt;- as.character(name)\n  # Remove leading/trailing whitespace\n  name &lt;- str_trim(name)\n  # Replace multiple spaces with a single space\n  name &lt;- str_replace_all(name, \"\\\\s+\", \" \")\n  # Standardize capitalization\n  name &lt;- str_to_title(name)\n  # Remove accents (requires stringi package)\n  if (!requireNamespace(\"stringi\", quietly = TRUE)) {\n    install.packages(\"stringi\")\n  }\n  name &lt;- stringi::stri_trans_general(name, \"Latin-ASCII\")\n  # Handle specific cases like missing spaces\n  name &lt;- str_replace(name, \"Martinnecas\", \"Martin Necas\")\n  return(name)\n}\n\n# Apply the improved cleaning in your data processing\nAll_Votes &lt;- All_Votes %&gt;%\n  mutate(\n    Player = standardize_player_names(Player)\n  ) %&gt;%\n  mutate(\n    Score = case_when(\n      Rank == 1 ~ 10,\n      Rank == 2 ~ 7,\n      Rank == 3 ~ 5,\n      Rank == 4 ~ 3,\n      Rank == 5 ~ 1,\n      TRUE ~ 0\n    )\n  )\n\n# Then continue with your grouping and summarizing\nScores &lt;- All_Votes %&gt;%\n  group_by(Award.Standard, Player) %&gt;%\n  summarize(Total = sum(Score), .groups = \"drop\")\n\n# Debug duplicates (optional)\nduplicates &lt;- Scores %&gt;%\n  filter(Award.Standard == \"Byng\") %&gt;%\n  group_by(Player) %&gt;%\n  filter(n() &gt; 1)\n\nif(nrow(duplicates) &gt; 0) {\n  print(\"Found duplicates:\")\n  print(duplicates)\n}\n\nggplot(Scores, aes(x = reorder(Player, Total), y = Total)) +\n  geom_col(fill = \"#3182bd\") +  # Use a pleasing blue color\n  coord_flip() +\n  facet_wrap(~Award.Standard, scales = \"free_y\") +  # Note: 'scales' not 'scale'\n  labs(\n    title = \"NHL Award Voting Results\",\n    subtitle = \"Players ranked by total vote score in each award category\",\n    x = \"Player\",\n    y = \"Total Vote Score\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.background = element_rect(fill = \"#f0f0f0\"),\n    strip.text = element_text(face = \"bold\", size = 12),\n    axis.text.y = element_text(size = 9),\n    panel.spacing = unit(1, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nCode\n# If you want to focus on top players only in each category\n# to prevent plots from becoming too cluttered\ntop_n_players &lt;- 10\n\nScores_top &lt;- Scores %&gt;%\n  group_by(Award.Standard) %&gt;%\n  slice_max(order_by = Total, n = top_n_players) %&gt;%\n  ungroup()\n\nggplot(Scores_top, aes(x = reorder(Player, Total), y = Total)) +\n  geom_col(fill = \"#3182bd\") +\n  coord_flip() +\n  facet_wrap(~Award.Standard, scales = \"free_y\") +\n  labs(\n    title = \"NHL Award Voting Results\",\n    subtitle = paste0(\"Top \", top_n_players, \" players by vote score in each award category\"),\n    x = \"Player\",\n    y = \"Total Vote Score\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.background = element_rect(fill = \"#f0f0f0\"),\n    strip.text = element_text(face = \"bold\", size = 12),\n    axis.text.y = element_text(size = 9),\n    panel.spacing = unit(1, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Apply the standardization function to ensure consistent player names\nAll_Votes &lt;- All_Votes %&gt;%\n  mutate(\n    Player = standardize_player_names(Player)\n  ) %&gt;%\n  mutate(\n    Score = case_when(\n      Rank == 1 ~ 10,\n      Rank == 2 ~ 7,\n      Rank == 3 ~ 5,\n      Rank == 4 ~ 3,\n      Rank == 5 ~ 1,\n      TRUE ~ 0\n    )\n  )\n\n# Calculate scores with consistent player names\nScores &lt;- All_Votes %&gt;%\n  group_by(Award.Standard, Player) %&gt;%\n  summarize(Total = sum(Score), .groups = \"drop\")\n\n# Get list of all awards\nawards &lt;- unique(Scores$Award.Standard)\n\n# Function to create a plot for a single award\ncreate_award_plot &lt;- function(award_name) {\n  award_data &lt;- Scores %&gt;% \n    filter(Award.Standard == award_name)\n  \n  # If there's no data for this award, return NULL\n  if(nrow(award_data) == 0) return(NULL)\n  \n  # Create the plot\n  p &lt;- ggplot(award_data, aes(x = reorder(Player, Total), y = Total)) +\n    geom_col(fill = \"#3182bd\") +\n    coord_flip() +\n    labs(\n      title = paste(award_name, \"Trophy Voting Results\"),\n      x = \"Player\",\n      y = \"Total Vote Score\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(face = \"bold\"),\n      axis.text.y = element_text(size = 10)\n    )\n  \n  return(p)\n}\n\n# Create a list to store all the plots\naward_plots &lt;- list()\n\n# Generate a plot for each award\nfor(award in awards) {\n  award_plots[[award]] &lt;- create_award_plot(award)\n}\n\n# Display each plot (in a non-loop environment like RStudio, \n# you would typically view these one at a time)\n# In R Markdown, you can include each plot separately\n\n# Example of how to display a specific award plot:\naward_plots[[\"Hart\"]] \n\n\n\n\n\n\n\n\n\nCode\naward_plots[[\"Norris\"]] \n\n\n\n\n\n\n\n\n\nCode\naward_plots[[\"Vezina\"]] \n\n\n\n\n\n\n\n\n\nCode\naward_plots[[\"Byng\"]] \n\n\n\n\n\n\n\n\n\nCode\naward_plots[[\"Calder\"]] \n\n\n\n\n\n\n\n\n\nCode\naward_plots[[\"Selke\"]] # Display the Hart Trophy plot\n\n\n\n\n\n\n\n\n\nCode\n# To save all plots to files\nfor(award in awards) {\n  if(!is.null(award_plots[[award]])) {\n    filename &lt;- paste0(\"NHL_\", award, \"_Award_Votes.png\")\n    ggsave(\n      filename = filename,\n      plot = award_plots[[award]],\n      width = 6,\n      height = 8,\n      dpi = 300\n    )\n  }\n}\n\n\n\n\nCode\n# Create a single matrix combining all awards\nall_votes_wide &lt;- All_Votes %&gt;%\n  # Create player-award combinations\n  mutate(PlayerAward = paste(Player, Award, sep = \" - \")) %&gt;%\n  select(Voter, PlayerAward, Score) %&gt;%\n  pivot_wider(\n    names_from = Voter,\n    values_from = Score,\n    values_fill = list(Score = 0)\n  ) %&gt;%\n  column_to_rownames(\"PlayerAward\")\n\n# Plot the combined heatmap with clustering\nheat&lt;-pheatmap(\n  all_votes_wide,\n  main = \"Clustered NHL Award Voting Patterns\",\n  color = colorRampPalette(brewer.pal(9, \"Blues\"))(50),\n  cluster_rows = FALSE,\n  cluster_cols = TRUE,\n  display_numbers = FALSE,\n  number_color = \"black\",\n  fontsize_number = 7,\n  fontsize = 4,\n  angle_col = 45,\n  height = 20,\n  width = 10\n)\n\n\n\n\n\n\n\n\n\nCode\npng(filename = \"heatmap.png\", width = 1000, height = 1500, res = 200)\nheat\ndev.off()\n\n\nquartz_off_screen \n                2"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#the-three-major-data-types",
    "href": "posts/L6-TabularData1/index.html#the-three-major-data-types",
    "title": "LECTURE 6 - Tabular Data",
    "section": "THE THREE MAJOR DATA TYPES",
    "text": "THE THREE MAJOR DATA TYPES"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#what",
    "href": "posts/L6-TabularData1/index.html#what",
    "title": "LECTURE 6 - Tabular Data",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#semantics-for-data",
    "href": "posts/L6-TabularData1/index.html#semantics-for-data",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SEMANTICS FOR DATA",
    "text": "SEMANTICS FOR DATA\n\n\n\n\n\n\nSemantics\n\nitem: individual entity, discrete\n\neg patient, car, stock, city\n“independent variable”\n\nattribute: property that is measured, observed, logged…\n\neg height, blood pressure for patient\neg horsepower, make for car\n“dependent variable”\n\n\n\n\nData Table\n\n\n\n\nITEM: Person\nATTRIBUTES: Name, Age, Shirt Size, Favorite Fruit"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#dataset-types-tables",
    "href": "posts/L6-TabularData1/index.html#dataset-types-tables",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DATASET TYPES: TABLES",
    "text": "DATASET TYPES: TABLES\nFlat Table\n\n\n\nOne ITEM per row\n\noften called an observation\n\nEach column is an ATTRIBUTE\n\noften called a variable\n\nA cell holds the VALUE for an item/attribute pair\nA unique KEY can be used (implicitly or explicitly) to identify each item even if they share all measured attributes"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#flat-table-example",
    "href": "posts/L6-TabularData1/index.html#flat-table-example",
    "title": "LECTURE 6 - Tabular Data",
    "section": "FLAT TABLE EXAMPLE",
    "text": "FLAT TABLE EXAMPLE\n\n\n\n\n\n\n\n\nA dumb spreadsheet\n\n\n\n\n\n\n\nA cool spreadsheet\n\n\n\n\n\n\nFigure 1: Flat Tables"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#multidimensional-tables",
    "href": "posts/L6-TabularData1/index.html#multidimensional-tables",
    "title": "LECTURE 6 - Tabular Data",
    "section": "MULTIDIMENSIONAL TABLES",
    "text": "MULTIDIMENSIONAL TABLES\nIndexing based on multiple keys (eg genes, patients)"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#keys-and-values",
    "href": "posts/L6-TabularData1/index.html#keys-and-values",
    "title": "LECTURE 6 - Tabular Data",
    "section": "KEYS AND VALUES",
    "text": "KEYS AND VALUES\n\n\nKEY: an independent attribute used as unique index to look up items.\n\nSimple tables: 1 key\nMultidimensional tables: multiple keys\n\nValue: a dependent attribute, value of cell\nWe will classify visualization idioms by the number of keys used.\n0, 1, 2, …"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#framework",
    "href": "posts/L6-TabularData1/index.html#framework",
    "title": "LECTURE 6 - Tabular Data",
    "section": "FRAMEWORK",
    "text": "FRAMEWORK"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-scatterplot",
    "href": "posts/L6-TabularData1/index.html#idiom-scatterplot",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Scatterplot",
    "text": "IDIOM: Scatterplot\n0 KEYS: Only values are available or salient to the task.\n\n\nExpress values (magnitudes) of quantitative attributes.\nData: 2 quantitative attributes\nMark: points\nChannels: horizontal and verical position\nTasks: find trends, outliers, distribution, correlation, clusters\nScalability: hundreds of items\n\n\n\n\n\n\nHeight and Weight of drafted NHL players."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-scatterplot-1",
    "href": "posts/L6-TabularData1/index.html#idiom-scatterplot-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Scatterplot",
    "text": "IDIOM: Scatterplot\n0 KEYS: Only values are available or salient to the task.\n\n\nExpress values (magnitudes) of quantitative attributes.\nData: 2 quantitative attributes\nMark: points\nChannels: horizontal and verical position\nTasks: find trends, outliers, distribution, correlation, clusters\nScalability: hundreds of items\n\n\n\n\n\n\nHeight and Weight of drafted NHL players."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#scatterplots-more-channels",
    "href": "posts/L6-TabularData1/index.html#scatterplots-more-channels",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SCATTERPLOTS: More channels",
    "text": "SCATTERPLOTS: More channels\nAdditional channels are viable with scatterplots since we are using point marks.\n\n\nExamples:\nColor: But pay attention to Discriminability!\nSize: But pay attention to using a quantitative attribute used to control 2D area. Directly encoding radius would mislead. Take the square root since area grows quadratically.\nShape:\n\n\n\n\n\n\nHeight and Weight of drafted NHL goaltenders."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#scatterplot-tasks",
    "href": "posts/L6-TabularData1/index.html#scatterplot-tasks",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SCATTERPLOT TASKS",
    "text": "SCATTERPLOT TASKS\nCorrelation\n\nClusters/groups, and clusters vs classes"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#keys-categorical-regions",
    "href": "posts/L6-TabularData1/index.html#keys-categorical-regions",
    "title": "LECTURE 6 - Tabular Data",
    "section": "KEYS: Categorical Regions",
    "text": "KEYS: Categorical Regions\n\n\n\nRegions: contiguous bounded areas distinct from each other\n\nNO OVERPLOTTING: separate into spatial regions: one mark per region (for now)\n\nuse categorical or ordered attribute to separate into regions\n\nno conflict with expressiveness principle for categorical attributes\n\nuse ordered attribute to order and align regions"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#separated-but-not-aligned-or-ordered",
    "href": "posts/L6-TabularData1/index.html#separated-but-not-aligned-or-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated but not aligned or ordered",
    "text": "Separated but not aligned or ordered\nLimitation: hard to make comparisons with size (vs aligned position)"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#separatedaligned---not-ordered",
    "href": "posts/L6-TabularData1/index.html#separatedaligned---not-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated/aligned - not ordered",
    "text": "Separated/aligned - not ordered\nLimitation: hard to know rank. what’s 4th? what’s 7th?"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#separated-and-aligned-and-ordered",
    "href": "posts/L6-TabularData1/index.html#separated-and-aligned-and-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated and aligned and ordered",
    "text": "Separated and aligned and ordered\nBest case!"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-bar-chart",
    "href": "posts/L6-TabularData1/index.html#idiom-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Bar Chart",
    "text": "IDIOM: Bar Chart\nOne key, One value\n\n\nData: 1 categorical attribute, 1 quantitative attribute\nMark: Lines\nChannels: Length to express quantitative value\nSpatial regions: one per mark - separated horizontally, aligned vertically. Ordered by quantitative attribute: by label (alphabetical), by length attribute (data-driven)\nTasks: compare, lookup values\nScalability: dozens to hundreds of levels for key attribute (bars), hundreds for values."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-stacked-bar-chart",
    "href": "posts/L6-TabularData1/index.html#idiom-stacked-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Stacked Bar Chart",
    "text": "IDIOM: Stacked Bar Chart\nTwo keys, One value\n\n\nData: 2 categorical attributes, 1 quantitative attribute\nMark: vertical stack of line marks\nGlyph: composite object, internal structure from multiple marks\nChannels: length and color hue\nSpatial regions: one per glyph\nAligned: full glyph, lowest bar component\nUnaligned: other bar components\nTask: part-to-whole relationship\nScalability: asymmetric for stacked key attribute, 10-12 levels segments for main key attribute, dozens to hundreds of levels bars"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-streamgraph",
    "href": "posts/L6-TabularData1/index.html#idiom-streamgraph",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Streamgraph",
    "text": "IDIOM: Streamgraph\nGeneralized stacked graph emphasizing horizontal continuity vs vertical items (example)\n\n\nData: 1 categorical key attribute (movies) 1 ordered key attribute (time) 1 quantitative value attribute (counts)\nDerived data geometry: layers, where height encodes counts of 1 quantitative attribute (layer ordering)\nMark: vertical stack of line marks\nChannels: length and color hue\nTask: part-to-whole relationship\nScalability: hundreds of time keys dozens to hundreds of movies keys more than stacked bars: most layers don’t extend across whole chart"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-dotline-chart",
    "href": "posts/L6-TabularData1/index.html#idiom-dotline-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Dot/Line Chart",
    "text": "IDIOM: Dot/Line Chart\nOne key, One value\n\n\nData: 2 quantitative attributes\nMark: points AND line connection marks between them\nChannels: aligned lengths to express quantitative value separated and ordered by key attribute into horizontal regions\nTask: find trend connection marks, emphasize ordering of items along key axis by explicitly showing relationship between one item and the next\nScalability: hundreds of key levels, hundreds of value levels"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#bar-vs-line-charts",
    "href": "posts/L6-TabularData1/index.html#bar-vs-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "BAR vs LINE CHARTS",
    "text": "BAR vs LINE CHARTS\nChoice depends on the type of key attributes.\nBar charts if categorical, Line charts if ordered. Do not use line charts for categorical key attributes as it violates the expressiveness principle. The implication of trend is so strong that it overrides semantics. “The more male a person is, the taller he/she is”"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#chart-axes",
    "href": "posts/L6-TabularData1/index.html#chart-axes",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CHART AXES",
    "text": "CHART AXES\n\n\nBest practice to label axes, with few exceptions. Individual small multiple views could share axis label.\nInclude 0 at bottom left or the slope misleads. There are some exceptions (arbitrary 0, small change matters).\nTruncating the Y-Axis: Threat or Menace? Correll, Bertini, & Franconeri, CHI 2020."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-indexed-line-charts",
    "href": "posts/L6-TabularData1/index.html#idiom-indexed-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Indexed Line Charts",
    "text": "IDIOM: Indexed Line Charts\n\n\nData: 2 quantitative attributes 1 key + 1 value\nDerived data: new quantitative value attribute index plot instead of original value\nTask: show change over time\nPrinciple: normalized, not absolute\nScalability: same as standard line chart\n\n\nCool Example"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-gantt-charts",
    "href": "posts/L6-TabularData1/index.html#idiom-gantt-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Gantt Charts",
    "text": "IDIOM: Gantt Charts\none key, two (related) values\n\n\nData: 1 categorical attribute, 2 quantitative attributes\nMark: line length: duration\nChannels: horizontal position: start time (+end from duration)\nTask: emphasize temporal overlaps & start/end dependencies between items\nScalability: dozens of key levels bars hundreds of value levels durations"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-slopegraphs",
    "href": "posts/L6-TabularData1/index.html#idiom-slopegraphs",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Slopegraphs",
    "text": "IDIOM: Slopegraphs\ntwo values\n\n\nData: 2 quantitative value attributes (1 derived attribute: change magnitude)\nMark: point + line line connecting mark between points\nChannels: 2 vertical positions: express attribute value (linewidth/size, color)\nTask: emphasize changes in rank/value\nScalability: hundreds of value levels dozens of item"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#keys",
    "href": "posts/L6-TabularData1/index.html#keys",
    "title": "LECTURE 6 - Tabular Data",
    "section": "2 KEYS",
    "text": "2 KEYS"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-heatmap",
    "href": "posts/L6-TabularData1/index.html#idiom-heatmap",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Heatmap",
    "text": "IDIOM: Heatmap\ntwo keys, one value\n\n\nData: 2 categorical attributes (gene, experimental condition) 1 quantitative attribute (expression levels)\nMark: point separate and align in 2D matrix, indexed by 2 categorical attributes\nChannels: color by quantitative attribute (ordered diverging colormap)\nTask: find clusters, outliers\nScalability: 1M items, 100s of categ levels, ~10 quant attrib levels"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#heatmap-reordering",
    "href": "posts/L6-TabularData1/index.html#heatmap-reordering",
    "title": "LECTURE 6 - Tabular Data",
    "section": "HEATMAP REORDERING",
    "text": "HEATMAP REORDERING"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#clustered-heatmap",
    "href": "posts/L6-TabularData1/index.html#clustered-heatmap",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CLUSTERED HEATMAP",
    "text": "CLUSTERED HEATMAP\nUse derived data: compute 2 cluster hierarchies and represent as a dendrogram. Parent-child relationships represented in tree with connection line marks, leaves aligned so interior branch heights are easy to compare. Heatmap marks are (re-)ordered by cluster hierarchy traversal. Task: assess quality of clusters found by automatic methods"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#axis-orientation",
    "href": "posts/L6-TabularData1/index.html#axis-orientation",
    "title": "LECTURE 6 - Tabular Data",
    "section": "AXIS ORIENTATION",
    "text": "AXIS ORIENTATION"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#discuss",
    "href": "posts/L6-TabularData1/index.html#discuss",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DISCUSS",
    "text": "DISCUSS"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#radial-bar-chart",
    "href": "posts/L6-TabularData1/index.html#radial-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADIAL BAR CHART",
    "text": "RADIAL BAR CHART\n\n\n\n\nStar plot: line mark, radial axes meet at central point\nRadial bar chart: line mark, radial axes meet at central ring\nChannels: length, angle/orientation\nBar chart: rectilinear axes, aligned vertically\nAccuracy: length not aligned with radial layouts, making them less accurately perceived than rectilinear aligned layouts.\nVismon: Facilitating Risk Assessment and Decision Making In Fisheries Management. Booshehrian, Möller, Peterman, and Munzner. Technical Report TR 2011-04, Simon Fraser University, School of Computing Science, 2011."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#radar-plot",
    "href": "posts/L6-TabularData1/index.html#radar-plot",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADAR PLOT",
    "text": "RADAR PLOT\nRadial line chart with point marks, radial layout, and connecting line marks. Avoid unless data are cyclic."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#radar-plot-example",
    "href": "posts/L6-TabularData1/index.html#radar-plot-example",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADAR PLOT EXAMPLE",
    "text": "RADAR PLOT EXAMPLE\n\nTheFunctionalArt"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#pie-and-coxcomb-charts",
    "href": "posts/L6-TabularData1/index.html#pie-and-coxcomb-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE AND COXCOMB CHARTS",
    "text": "PIE AND COXCOMB CHARTS\n1 categorical key attribute, 1 quantitative value attribute\n\n\nPie chart: interlocking area marks with angle channel causes variation in 2D area. Keys are separated & ordered radially with uniform height. accuracy: area less accurate than rectilinear aligned line length.\nTask: part-to-whole judgements.\nCoxcomb chart: line marks with length channel, creating variation in only 1D length. Keys are separated & ordered radially with uniform width. These charts are a direct analog to radial bar charts."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#nightengale-rose",
    "href": "posts/L6-TabularData1/index.html#nightengale-rose",
    "title": "LECTURE 6 - Tabular Data",
    "section": "NIGHTENGALE ROSE",
    "text": "NIGHTENGALE ROSE\nCoxcomb / Nightengale Rose/ Polar Area Chart Invented by Florence Nightingale."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#coxcomb-perception",
    "href": "posts/L6-TabularData1/index.html#coxcomb-perception",
    "title": "LECTURE 6 - Tabular Data",
    "section": "COXCOMB: Perception",
    "text": "COXCOMB: Perception\n\n\nEncode: 1D length\nDecode/perceive: 2D area\nThe nonuniform relationship between line/sector width and mark length causes area variation to scale nonlinearly with line mark length!\nA standard bar chart is safer: bars are uniform width, so area is linear with line mark length in both radial & rectilinear cases."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#pie-charts-perception",
    "href": "posts/L6-TabularData1/index.html#pie-charts-perception",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE CHARTS: Perception",
    "text": "PIE CHARTS: Perception\n\n\nSome empirical evidence that people respond to arc length.\nDecode/perceive: arc length, maybe also areas, but not angles.\nDonut charts are no worse than pie charts.\n\n\n\nArcs, Angles, or Areas: Individual Data Encodings in Pie and Donut Charts. Skau and Kosara. Proc. EuroVis 2016\nExplore this concept"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#pie-charts-best-practices",
    "href": "posts/L6-TabularData1/index.html#pie-charts-best-practices",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE CHARTS: Best Practices",
    "text": "PIE CHARTS: Best Practices\nNot so bad for two (or few) levels and part-to-whole tasks. Dubious for several levels if details matter. Terrible for many levels."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#normalized-stacked-bar",
    "href": "posts/L6-TabularData1/index.html#normalized-stacked-bar",
    "title": "LECTURE 6 - Tabular Data",
    "section": "NORMALIZED STACKED BAR",
    "text": "NORMALIZED STACKED BAR\n\n\nTask: part-to-whole judgements\nNormalized stacked bar chart: stacked bar chart, normalized to full vertical height. A single stacked bar is equivalent to a full pie chart.\nHigh information density can be achieved with narrow rectangles. Pie chart information density is much more limited and requires a large circle."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#glyphmaps",
    "href": "posts/L6-TabularData1/index.html#glyphmaps",
    "title": "LECTURE 6 - Tabular Data",
    "section": "GLYPHMAPS",
    "text": "GLYPHMAPS\nRectilinear glyphs are good for linear vs nonlinear trends. Radial glyphs are good for cyclic patterns and evaluating periodicity.\n\nGlyph-maps for Visually Exploring Temporal Patterns in Climate Data and Models. Wickham, Hofmann, Wickham, and Cook. Environmetrics 23:5 (2012), 382–393."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-axes",
    "href": "posts/L6-TabularData1/index.html#parallel-axes",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL AXES",
    "text": "PARALLEL AXES"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#splom",
    "href": "posts/L6-TabularData1/index.html#splom",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SPLOM",
    "text": "SPLOM\n\n\nScatterplot matrix (SPLOM): Rectilinear axes with point mark. All possible pairs of axes are visualized.\nScalability: about a dozen attributes and dozens to hundreds of items."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-coordinates",
    "href": "posts/L6-TabularData1/index.html#parallel-coordinates",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES",
    "text": "PARALLEL COORDINATES\nScatterplot limitation: visual representation with orthogonal axes can show only two attributes with spatial position channel."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-coordinates-1",
    "href": "posts/L6-TabularData1/index.html#parallel-coordinates-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES",
    "text": "PARALLEL COORDINATES\n\n\nAlternative: Line up axes in parallel to show many attributes with position. Items are encoded with a line with n segments (n is the number of attributes shown). Ordering is a major challenge.\nScalability: dozens of attributes and hundreds of items."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-coordinates-limitations",
    "href": "posts/L6-TabularData1/index.html#parallel-coordinates-limitations",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES: Limitations",
    "text": "PARALLEL COORDINATES: Limitations\nPatterns only visible between neighboring axis pairs. How to pick axis order? The usual solution is reorderable axes and interactive exploration. The downside of interaction is human-powered search.\n\nAs usual, OBSERVABLE really shines for these types of interactive plots!"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#orientation-limitations",
    "href": "posts/L6-TabularData1/index.html#orientation-limitations",
    "title": "LECTURE 6 - Tabular Data",
    "section": "ORIENTATION LIMITATIONS",
    "text": "ORIENTATION LIMITATIONS\n\n\nRectilinear: Scalability is limited with regard to the number of axes. (2 axes best, 3 problematic, 4+ impossible.)\nParallel: Unfamiliarity and training time.\nRadial: Perceptual limits include polar coordinate asymmetry, lower precision with angle channles compated to length channels, nonuniform sector width/size depending on radial distance. While thes limits are frequently problematic, they can sometimes be deliberately exploited. (Example: for 2 attribs of very unequal importance)\n\n\n\nUncovering Strengths and Weaknesses of Radial Visualizations - an Empirical Approach. Diehl, Beck and Burch. IEEE TVCG (Proc. InfoVis) 16(6):935–942, 2010."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#chart-axes-1",
    "href": "posts/L6-TabularData1/index.html#chart-axes-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CHART AXES",
    "text": "CHART AXES\n\n\nLabelled axes are critical! Avoid cropping the y-axis (include 0 at bottom left) or the slope misleads the viewer.\n\n\n\nIf you see bullshit…"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#dual-axis-line-charts",
    "href": "posts/L6-TabularData1/index.html#dual-axis-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DUAL AXIS LINE CHARTS",
    "text": "DUAL AXIS LINE CHARTS\nThis approach is controversial. Dual axes are acceptable if they are commensurate, but beware, as they make it very easy to mislead!"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#connected-scatterplots",
    "href": "posts/L6-TabularData1/index.html#connected-scatterplots",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CONNECTED SCATTERPLOTS",
    "text": "CONNECTED SCATTERPLOTS\nScatterplot with line connection marks. These are popular in journalism.\n\n\nHoriz + vert axes: value attributes\nLine connection marks: temporal order\nAlternative to dual-axis charts (horiz: time vert: two value attributes).\nempirical study suggests these are engaging, but correlations are unclear.\n\n\n\nhttp://steveharoz.com/research/connected_scatterplot/"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#breaking-conventions",
    "href": "posts/L6-TabularData1/index.html#breaking-conventions",
    "title": "LECTURE 6 - Tabular Data",
    "section": "BREAKING CONVENTIONS",
    "text": "BREAKING CONVENTIONS\nThe inverted y axis is evocative of blood dripping down on Poe.\n\nhttps://public.tableau.com/profile/ben.jones#!/vizhome/EdgarAllanPoeViz/EdgarAllanPoeViz"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#arranging-tabular-data",
    "href": "posts/L6-TabularData1/index.html#arranging-tabular-data",
    "title": "LECTURE 6 - Tabular Data",
    "section": "ARRANGING TABULAR DATA",
    "text": "ARRANGING TABULAR DATA\nWhat did we miss?\nggplot cheatsheet\n\n\n\nHOME"
  },
  {
    "objectID": "posts/A8-NetworkData/index.html",
    "href": "posts/A8-NetworkData/index.html",
    "title": "Practice with Network Data",
    "section": "",
    "text": "In this assignment, we’ll consider some of the tools and techniques for visualizing network data. Network data is characterized by two unique items that are not found in tabular or spatial data - Nodes and Links. In addition, there is a sub-type of network data that we will consider - Hierarchical or Tree data. Let’s practice a few visualizations to get a feel for how these things work!"
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#igraph",
    "href": "posts/A8-NetworkData/index.html#igraph",
    "title": "Practice with Network Data",
    "section": "IGRAPH",
    "text": "IGRAPH\nLet’s start with igraph, which is an open source toolset for network analysis. The great thing about igraph is that you can use these tools in R, Python, Mathematica, and C++. It is very flexible and very powerful.\n\nigraph in R\nFirst up, we’ll install R/igraph and load the library (note that I’ve commented out the package installation because I’ve already installed igraph on my machine):\n\n\nCode\n# install.packages(\"igraph\")\nlibrary(igraph)\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n\nNow I’m going to walk you through a modified version of the igraph tutorial, which you can find here"
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#creating-a-graph",
    "href": "posts/A8-NetworkData/index.html#creating-a-graph",
    "title": "Practice with Network Data",
    "section": "Creating a graph",
    "text": "Creating a graph\nigraph offers many ways to create a graph. The simplest one is the function make_empty_graph:\n\n\nCode\ng &lt;- make_empty_graph()\n\n\nThe most common way to create a graph is make_graph, which constructs a network based on specified edges. For example, to make a graph with 10 nodes (numbered 1 to 10) and two edges connecting nodes 1-2 and 1-5:\n\n\nCode\ng &lt;- make_graph(edges = c(1,2, 1,5), n=10, directed = FALSE)\n\n\nWe can print the graph to get a summary of its nodes and edges:\n\n\nCode\ng\n\n\nIGRAPH ca2b351 U--- 10 2 -- \n+ edges from ca2b351:\n[1] 1--2 1--5\n\n\nThis means: Undirected Named graph with 10 vertices and 2 edges, with the exact edges listed out. If the graph has a [name] attribute, it is printed as well.\n\n\n\n\n\n\nNote\n\n\n\nsummary does not list the edges, which is convenient for large graphs with millions of edges:\n\n\n\n\nCode\nsummary(g)\n\n\nIGRAPH ca2b351 U--- 10 2 -- \n\n\nThe same function make_graph can create some notable graphs by just specifying their name. For example you can create the graph that represents the social network of Zachary’s karate club, that shows the friendship between 34 members of a karate club at a US university in the 1970s:\n\n\nCode\ng &lt;- make_graph('Zachary')\n\n\nTo visualize a graph you can use plot:\n\n\nCode\nplot(g)"
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#vertex-and-edge-ids",
    "href": "posts/A8-NetworkData/index.html#vertex-and-edge-ids",
    "title": "Practice with Network Data",
    "section": "Vertex and edge IDs",
    "text": "Vertex and edge IDs\nVertices and edges have numerical vertex IDs in igraph. Vertex IDs are always consecutive and they start with 1. For a graph with n vertices the vertex IDs are always between 1 and n. If some operation changes the number of vertices in the graphs, e.g. a subgraph is created via induced_subgraph, then the vertices are renumbered to satisfy this criterion.\nThe same is true for the edges as well: edge IDs are always between 1 and m, the total number of edges in the graph.\nIn addition to IDs, vertices and edges can be assigned a name and other attributes. That makes it easier to track them whenever the graph is altered."
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#addingdeleting-vertices-and-edges",
    "href": "posts/A8-NetworkData/index.html#addingdeleting-vertices-and-edges",
    "title": "Practice with Network Data",
    "section": "Adding/deleting vertices and edges",
    "text": "Adding/deleting vertices and edges\nLet’s continue working with the Karate club graph. To add one or more vertices to an existing graph, use add_vertices:\n\n\nCode\ng &lt;- add_vertices(g, 3)\n\n\nSimilarly, to add edges you can use add_edges:\n\n\nCode\ng &lt;- add_edges(g, edges = c(1,35, 1,36, 34,37))\n\n\nEdges are added by specifying the source and target vertex IDs for each edge. This call added three edges, one connecting vertices 1 and 35, one connecting vertices 1 and 36, and one connecting vertices 34 and 37.\nIn addition to the add_vertices and add_edges functions, the plus operator can be used to add vertices or edges to graph. The actual operation that is performed depends on the type of the right hand side argument:\n\n\nCode\ng &lt;- g + edges(c(1,35, 1,36, 34,37))\n\n\nYou can add a single vertex/edge at a time using add_vertex and add_edge.\nLet us add some more vertices and edges to our graph. In igraph we can use the magrittr package, which provides a mechanism for chaining commands with the operator %\\&gt;%:\n\n\nCode\ng &lt;- g %&gt;% add_edges(edges=c(1,34)) %&gt;% add_vertices(3) %&gt;%\n     add_edges(edges=c(38,39, 39,40, 40,38, 40,37))\ng\n\n\nIGRAPH 8279327 U--- 40 86 -- Zachary\n+ attr: name (g/c)\n+ edges from 8279327:\n [1]  1-- 2  1-- 3  1-- 4  1-- 5  1-- 6  1-- 7  1-- 8  1-- 9  1--11  1--12\n[11]  1--13  1--14  1--18  1--20  1--22  1--32  2-- 3  2-- 4  2-- 8  2--14\n[21]  2--18  2--20  2--22  2--31  3-- 4  3-- 8  3--28  3--29  3--33  3--10\n[31]  3-- 9  3--14  4-- 8  4--13  4--14  5-- 7  5--11  6-- 7  6--11  6--17\n[41]  7--17  9--31  9--33  9--34 10--34 14--34 15--33 15--34 16--33 16--34\n[51] 19--33 19--34 20--34 21--33 21--34 23--33 23--34 24--26 24--28 24--33\n[61] 24--34 24--30 25--26 25--28 25--32 26--32 27--30 27--34 28--34 29--32\n[71] 29--34 30--33 30--34 31--33 31--34 32--33 32--34 33--34  1--35  1--36\n+ ... omitted several edges\n\n\nCode\nplot(g)\n\n\n\n\n\n\n\n\n\nWe now have an undirected graph with 40 vertices and 86 edges. Vertex and edge IDs are always contiguous, so if you delete a vertex all subsequent vertices will be renumbered. When a vertex is renumbered, edges are not renumbered, but their source and target vertices will be. Use delete_vertices and delete_edges to perform these operations. For instance, to delete the edge connecting vertices 1-34, get its ID and then delete it:\n\n\nCode\nget.edge.ids(g, c(1,34))\n\n\nWarning: `get.edge.ids()` was deprecated in igraph 2.1.0.\nℹ Please use `get_edge_ids()` instead.\n\n\n[1] 82\n\n\n\n\nCode\ng &lt;- delete_edges(g, 82)\n\n\nAs an example, to create a broken ring:\n\n\nCode\ng &lt;- make_ring(10) %&gt;% delete_edges(\"10|1\")\nplot(g)\n\n\n\n\n\n\n\n\n\nThe example above shows that you can also refer to edges with strings containing the IDs of the source and target vertices, connected by a pipe symbol |. \"10|1\" in the above example means the edge that connects vertex 10 to vertex 1. Of course you can also use the edge IDs directly, or retrieve them with the get.edge.ids function:\n\n\nCode\ng &lt;- make_ring(5)\ng &lt;- delete_edges(g, get.edge.ids(g, c(1,5, 4,5)))\nplot(g)"
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#constructing-graphs",
    "href": "posts/A8-NetworkData/index.html#constructing-graphs",
    "title": "Practice with Network Data",
    "section": "Constructing graphs",
    "text": "Constructing graphs\nIn addition to make_empty_graph, make_graph, and make_graph_from_literal, igraph includes many other function to construct a graph. Some are deterministic, i.e. they produce the same graph each single time, e.g. make_tree:\n\n\nCode\ngraph1 &lt;- make_tree(127, 2, mode = \"undirected\")\nsummary(graph1)\n\n\nIGRAPH 799932d U--- 127 126 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c)\n\n\nCode\nplot(graph1)\n\n\n\n\n\n\n\n\n\nThis generates a regular tree graph with 127 vertices, each vertex having two children. No matter how many times you call make_tree, the generated graph will always be the same if you use the same parameters:\n\n\nCode\ngraph2 &lt;- make_tree(127, 2, mode = \"undirected\")\n\n\n\n\nCode\nidentical_graphs(graph1,graph2)\n\n\n[1] TRUE\n\n\nOther functions generate graphs stochastically, i.e. they produce a different graph each time. For instance sample_grg:\n\n\nCode\ngraph1 &lt;- sample_grg(100, 0.2)\nsummary(graph1)\n\n\nIGRAPH 520d57d U--- 100 501 -- Geometric random graph\n+ attr: name (g/c), radius (g/n), torus (g/l)\n\n\nCode\nplot(graph1)\n\n\n\n\n\n\n\n\n\nThis generates a geometric random graph: n points are chosen randomly and uniformly inside the unit square and pairs of points closer to each other than a predefined distance d are connected by an edge. If you generate GRGs with the same parameters, they will be different:\n\n\nCode\ngraph2 &lt;- sample_grg(100, 0.2)\nidentical_graphs(graph1, graph2)\n\n\n[1] FALSE\n\n\nCode\nplot(graph2)\n\n\n\n\n\n\n\n\n\nA slightly looser way to check if the graphs are equivalent is via isomorphic. Two graphs are said to be isomorphic if they have the same number of components (vertices and edges) and maintain a one-to-one correspondence between vertices and edges, i.e., they are connected in the same way.\n\n\nCode\nisomorphic(graph1, graph2)\n\n\n[1] FALSE\n\n\nChecking for isomorphism can take a while for large graphs (in this case, the answer can quickly be given by checking the degree sequence of the two graphs). identical_graph is a stricter criterion than isomorphic: the two graphs must have the same list of vertices and edges, in exactly the same order, with same directedness, and the two graphs must also have identical graph, vertex and edge attributes."
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#setting-and-retrieving-attributes",
    "href": "posts/A8-NetworkData/index.html#setting-and-retrieving-attributes",
    "title": "Practice with Network Data",
    "section": "Setting and retrieving attributes",
    "text": "Setting and retrieving attributes\nIn addition to IDs, vertex and edges can have attributes such as a name, coordinates for plotting, metadata, and weights. The graph itself can have such attributes too (e.g. a name, which will show in summary). In a sense, every graph, vertex and edge can be used as an R namespace to store and retrieve these attributes.\nTo demonstrate the use of attributes, let us create a simple social network:\n\n\nCode\ng &lt;- make_graph(~ Alice-Bob:Claire:Frank, Claire-Alice:Dennis:Frank:Esther,\n                George-Dennis:Frank, Dennis-Esther)\n\n\nEach vertex represents a person, so we want to store ages, genders and types of connection between two people (is_formal refers to whether a connection between one person or another is formal or informal, i.e. colleagues or friends). The \\$ operator is a shortcut to get and set graph attributes. It is shorter and just as readable as graph_attr and set_graph_attr.\n\n\nCode\nV(g)$age &lt;- c(25, 31, 18, 23, 47, 22, 50) \nV(g)$gender &lt;- c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")\nE(g)$is_formal &lt;- c(FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE)\nsummary(g)\n\n\nIGRAPH 9862b5c UN-- 7 9 -- \n+ attr: name (v/c), age (v/n), gender (v/c), is_formal (e/l)\n\n\nV and E are the standard way to obtain a sequence of all vertices and edges, respectively. This assigns an attribute to all vertices/edges at once. Another way to generate our social network is with the use of set_vertex_attr and set_edge_attr and the operator %\\&gt;%:\n\n\nCode\ng &lt;- make_graph(~ Alice-Bob:Claire:Frank, Claire-Alice:Dennis:Frank:Esther,\n                George-Dennis:Frank, Dennis-Esther) %&gt;%\n  set_vertex_attr(\"age\", value = c(25, 31, 18, 23, 47, 22, 50)) %&gt;%\n  set_vertex_attr(\"gender\", value = c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")) %&gt;%\n  set_edge_attr(\"is_formal\", value = c(FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE))\nsummary(g)\n\n\nTo assign or modify an attribute for a single vertex/edge:\n\n\nCode\nE(g)$is_formal\n\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nCode\nE(g)$is_formal[1] &lt;- TRUE\nE(g)$is_formal\n\n\n[1]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nAttribute values can be set to any R object, but note that storing the graph in some file formats might result in the loss of complex attribute values. Vertices, edges and the graph itself can all be used to set attributes, e.g. to add a date to the graph:\n\n\nCode\ng$date &lt;- c(\"2022-02-11\")\ngraph_attr(g, \"date\")\n\n\n[1] \"2022-02-11\"\n\n\nTo retrieve attributes, you can also use graph_attr, vertex_attr, and edge_attr. To find the ID of a vertex you can use the function match:\n\n\nCode\nmatch(c(\"George\"), V(g)$name)\n\n\n[1] 7\n\n\nTo assign attributes to a subset of vertices or edges, you can use:\n\n\nCode\nV(g)$name[1:3] &lt;- c(\"Alejandra\", \"Bruno\", \"Carmina\")\nV(g)\n\n\n+ 7/7 vertices, named, from 9862b5c:\n[1] Alejandra Bruno     Carmina   Frank     Dennis    Esther    George   \n\n\nTo delete attributes:\n\n\nCode\ng &lt;- delete_vertex_attr(g, \"gender\")\nV(g)$gender\n\n\nNULL\n\n\nIf you want to save a graph in R with all the attributes use the R’s standard function dput function and retrieve it later with dget. You can also just save the R workspace and restore it later."
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#structural-properties-of-graphs",
    "href": "posts/A8-NetworkData/index.html#structural-properties-of-graphs",
    "title": "Practice with Network Data",
    "section": "Structural properties of graphs",
    "text": "Structural properties of graphs\nigraph provides a large set of functions to calculate various structural properties of graphs. It is beyond the scope of this tutorial to document all of them, hence this section will only introduce a few of them for illustrative purposes. We will work on the small social network constructed in the previous section.\nPerhaps the simplest property one can think of is the degree. The degree of a vertex equals the number of edges adjacent to it. In case of directed networks, we can also define in-degree (the number of edges pointing towards the vertex) and out-degree (the number of edges originating from the vertex). igraph is able to calculate all of them using a simple syntax:\n\n\nCode\ndegree(g)\n\n\nAlejandra     Bruno   Carmina     Frank    Dennis    Esther    George \n        3         1         4         3         3         2         2 \n\n\nIf the graph was directed, we would have been able to calculate the in- and out-degrees separately using degree(mode=\"in\") and degree(mode=\"out\"). You can also pass a single vertex ID or a list of vertex IDs to degree if you want to calculate the degrees for only a subset of vertices:\n\n\nCode\ndegree(g, 7)\n\n\nGeorge \n     2 \n\n\n\n\nCode\ndegree(g, v=c(3,4,5))\n\n\nCarmina   Frank  Dennis \n      4       3       3 \n\n\nMost functions that accept vertex IDs also accept vertex names (i.e. the values of the name vertex attribute) as long as the names are unique:\n\n\nCode\ndegree(g, v=c(\"Carmina\", \"Frank\", \"Dennis\"))\n\n\nCarmina   Frank  Dennis \n      4       3       3 \n\n\nIt also works for single vertices:\n\n\nCode\ndegree(g, \"Bruno\")\n\n\nBruno \n    1 \n\n\nA similar syntax is used for most of the structural properties igraph can calculate. For vertex properties, the functions accept a vertex ID, a vertex name, or a list of vertex IDs or names (and if they are omitted, the default is the set of all vertices). For edge properties, the functions accept a single edge ID or a list of edge IDs.\n\nNOTE: For some measures, it does not make sense to calculate them only for a few vertices or edges instead of the whole graph, as it would take the same time anyway. In this case, the functions won’t accept vertex or edge IDs, but you can still restrict the resulting list later using standard operations. One such example is eigenvector centrality (evcent()).\n\nBesides degree, igraph includes built-in routines to calculate many other centrality properties, including vertex and edge betweenness (edge_betweenness) or Google’s PageRank (page_rank) just to name a few. Here we just illustrate edge betweenness:\n\n\nCode\nedge_betweenness(g)\n\n\n[1] 6 6 4 3 4 4 4 2 3\n\n\nNow we can also figure out which connections have the highest betweenness centrality:\n\n\nCode\nebs &lt;- edge_betweenness(g)\nas_edgelist(g)[ebs == max(ebs), ]\n\n\n     [,1]        [,2]     \n[1,] \"Alejandra\" \"Bruno\"  \n[2,] \"Alejandra\" \"Carmina\""
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#querying-vertices-and-edges-based-on-attributes",
    "href": "posts/A8-NetworkData/index.html#querying-vertices-and-edges-based-on-attributes",
    "title": "Practice with Network Data",
    "section": "Querying vertices and edges based on attributes",
    "text": "Querying vertices and edges based on attributes\n\nSelecting vertices\nImagine that in a given social network, you want to find out who has the largest degree. You can do that with the tools presented so far and the which.max function:\n\n\nCode\nwhich.max(degree(g))\n\n\nCarmina \n      3 \n\n\nAnother example would be to select only vertices that have only odd IDs but not even ones, using the V function:\n\n\nCode\ngraph &lt;- graph.full(n=10)\n\n\nWarning: `graph.full()` was deprecated in igraph 2.1.0.\nℹ Please use `make_full_graph()` instead.\n\n\nCode\nonly_odd_vertices &lt;- which(V(graph)%%2==1)\nlength(only_odd_vertices)\n\n\n[1] 5\n\n\nOf course, it is possible to select vertices or edges by positional indices:\n\n\nCode\nseq &lt;- V(graph)[2, 3, 7]\nseq\n\n\n+ 3/10 vertices, from b3c6a0f:\n[1] 2 3 7\n\n\n\n\nCode\nseq &lt;- seq[1, 3]    # filtering an existing vertex set\nseq\n\n\n+ 2/10 vertices, from b3c6a0f:\n[1] 2 7\n\n\nSelecting a vertex that does not exist results in an error:\n\n\nCode\nseq &lt;- V(graph)[2, 3, 7, \"foo\", 3.5]\n## Error in simple_vs_index(x, ii, na_ok) : Unknown vertex selected\n\n\nAttribute names can also be used as-is within the indexing brackets of V() and E(). This can be combined with R’s ability to use boolean vectors for indexing to obtain very concise and readable expressions to retrieve a subset of the vertex or edge set of a graph. For instance, the following command gives you the names of the individuals younger than 30 years in our social network:\n\n\nCode\nV(g)[age &lt; 30]$name\n\n\n[1] \"Alejandra\" \"Carmina\"   \"Frank\"     \"Esther\"   \n\n\nOf course, &lt; is not the only boolean operator that can be used for this. Other possibilities include the following:\n\n\n\n\n\n\n\nOperator\nMeaning\n\n\n\n\n==\nThe attribute/property value must be equal to\n\n\n!=\nThe attribute/property value must not be equal to\n\n\n&lt;\nThe attribute/property value must be less than\n\n\n&lt;=\nThe attribute/property value must be less than or equal to\n\n\n&gt;\nThe attribute/property value must be greater than\n\n\n&gt;=\nThe attribute/property value must be greater than or equal to\n\n\n%in%\nThe attribute/property value must be included in\n\n\n\nYou can also create a “not in” operator from %in% using the Negate function:\n\n\nCode\n`%notin%` &lt;- Negate(`%in%`)\n\n\nIf an attribute has the same name as an igraph function, you should be careful as the syntax can become a little confusing. For instance, if there is an attribute named degree that represents the grades of an exam for each person, that should not be confused with the igraph function that computes the degrees of vertices in a network sense:\n\n\nCode\nV(g)$degree &lt;- c(\"A\", \"B\", \"B+\", \"A+\", \"C\", \"A\", \"B\")\nV(g)$degree[degree(g) == 3]\n\n\n[1] \"A\"  \"A+\" \"C\" \n\n\n\n\nCode\nV(g)$name[degree(g) == 3]\n\n\n[1] \"Alejandra\" \"Frank\"     \"Dennis\"   \n\n\n\n\nSelecting edges\nEdges can be selected based on attributes just like vertices. As mentioned above, the standard way to get edges is E. Moreover, there are a few special structural properties for selecting edges.\nUsing .from allows you to filter the edge sequence based on the source vertices of the edges. E.g., to select all the edges originating from Carmina (who has vertex index 3):\n\n\nCode\nE(g)[.from(3)]\n\n\n+ 4/9 edges from 9862b5c (vertex names):\n[1] Alejandra--Carmina Carmina  --Frank   Carmina  --Dennis  Carmina  --Esther \n\n\nOf course it also works with vertex names:\n\n\nCode\nE(g)[.from(\"Carmina\")]\n\n\n+ 4/9 edges from 9862b5c (vertex names):\n[1] Alejandra--Carmina Carmina  --Frank   Carmina  --Dennis  Carmina  --Esther \n\n\nUsing .to filters edge sequences based on the target vertices. This is different from .from if the graph is directed, while it gives the same answer for undirected graphs. Using .inc selects only those edges that are incident on a single vertex or at least one of the vertices, irrespectively of the edge directions.\nThe %--% operator can be used to select edges between specific groups of vertices, ignoring edge directions in directed graphs. For instance, the following expression selects all the edges between Carmina (vertex index 3), Dennis (vertex index 5) and Esther (vertex index 6):\n\n\nCode\nE(g) [ 3:5 %--% 5:6 ]\n\n\n+ 3/9 edges from 9862b5c (vertex names):\n[1] Carmina--Dennis Carmina--Esther Dennis --Esther\n\n\nTo make the %--% operator work with names, you can build string vectors containing the names and then use these vectors as operands. For instance, to select all the edges that connect men to women, we can do the following after re-adding the gender attribute that we deleted earlier:\n\n\nCode\nV(g)$gender &lt;- c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")\n\n\n\n\nCode\nmen &lt;- V(g)[gender == \"m\"]$name\nmen\n\n\n[1] \"Bruno\"  \"Frank\"  \"Dennis\" \"George\"\n\n\n\n\nCode\nwomen &lt;- V(g)[gender == \"f\"]$name\nwomen\n\n\n[1] \"Alejandra\" \"Carmina\"   \"Esther\"   \n\n\n\n\nCode\nE(g)[men %--% women]\n\n\n+ 5/9 edges from 9862b5c (vertex names):\n[1] Alejandra--Bruno  Alejandra--Frank  Carmina  --Frank  Carmina  --Dennis\n[5] Dennis   --Esther"
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#treating-a-graph-as-an-adjacency-matrix",
    "href": "posts/A8-NetworkData/index.html#treating-a-graph-as-an-adjacency-matrix",
    "title": "Practice with Network Data",
    "section": "Treating a graph as an adjacency matrix",
    "text": "Treating a graph as an adjacency matrix\nThe adjacency matrix is another way to represent a graph. In an adjacency matrix, rows and columns are labeled by graph vertices, and the elements of the matrix indicate the number of edges between vertices i and j. The adjacency matrix for the example graph is:\n\n\nCode\nget.adjacency(g)\n\n\nWarning: `get.adjacency()` was deprecated in igraph 2.0.0.\nℹ Please use `as_adjacency_matrix()` instead.\n\n\n7 x 7 sparse Matrix of class \"dgCMatrix\"\n          Alejandra Bruno Carmina Frank Dennis Esther George\nAlejandra         .     1       1     1      .      .      .\nBruno             1     .       .     .      .      .      .\nCarmina           1     .       .     1      1      1      .\nFrank             1     .       1     .      .      .      1\nDennis            .     .       1     .      .      1      1\nEsther            .     .       1     .      1      .      .\nGeorge            .     .       .     1      1      .      .\n\n\nFor example, Carmina (1, 0, 0, 1, 1, 1, 0) is directly connected to Alejandra (who has vertex index 1), Frank (index 4), Dennis (index 5) and Esther (index 6), but not to Bruno (index 2) or to George (index 7)."
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#layouts-and-plotting",
    "href": "posts/A8-NetworkData/index.html#layouts-and-plotting",
    "title": "Practice with Network Data",
    "section": "Layouts and plotting",
    "text": "Layouts and plotting\nA graph is an abstract mathematical object without a specific representation in 2D, 3D or any other geometric space. This means that whenever we want to visualise a graph, we have to find a mapping from vertices to coordinates in two- or three-dimensional space first, preferably in a way that is useful and/or pleasing for the eye. A separate branch of graph theory, namely graph drawing, tries to solve this problem via several graph layout algorithms. igraph implements quite a few layout algorithms and is also able to draw them onto the screen or to any output format that R itself supports.\n\nLayout algorithms\nThe layout functions in igraph always start with layout. The following table summarises them:\n\n\n\n\n\n\n\nMethod name\nAlgorithm description\n\n\n\n\nlayout_randomly\nPlaces the vertices completely randomly\n\n\nlayout_in_circle\nDeterministic layout that places the vertices on a circle\n\n\nlayout_on_sphere\nDeterministic layout that places the vertices evenly on the surface of a sphere\n\n\nlayout_with_drl\nThe Drl (Distributed Recursive Layout) algorithm for large graphs\n\n\nlayout_with_fr\nFruchterman-Reingold force-directed algorithm\n\n\nlayout_with_kk\nKamada-Kawai force-directed algorithm\n\n\nlayout_with_lgl\nThe LGL (Large Graph Layout) algorithm for large graphs\n\n\nlayout_as_tree\nReingold-Tilford tree layout, useful for (almost) tree-like graphs\n\n\nlayout_nicely\nLayout algorithm that automatically picks one of the other algorithms based on certain properties of the graph\n\n\n\nLayout algorithms can be called directly with a graph as its first argument. They will return a matrix with two columns and as many rows as the number of vertices in the graph; each row will correspond to the position of a single vertex, ordered by vertex IDs. Some algorithms have a 3D variant; in this case they return three columns instead of 2.\n\n\nCode\nlayout &lt;- layout_with_kk(g)\n\n\nSome layout algorithms take additional arguments; e.g., when laying out a graph as a tree, it might make sense to specify which vertex is to be placed at the root of the layout:\n\n\nCode\nlayout &lt;- layout_as_tree(g, root = 2)\n\n\n\n\nDrawing a graph using a layout\nWe can plot our imaginary social network with the Kamada-Kawai layout algorithm as follows:\n\n\nCode\nlayout &lt;- layout_with_kk(g)\n\n\n\n\nCode\nplot(g, layout = layout, main = \"Social network with the Kamada-Kawai layout algorithm\")\n\n\n\n\n\n\n\n\n\nThis should open a new window showing a visual representation of the network. Remember that the exact placement of nodes may be different on your machine since the layout is not deterministic.\nThe layout argument also accepts functions; in this case, the function will be called with the graph as its first argument. This makes it possible to just pass the name of a layout function directly, without creating a layout variable:\n\n\nCode\nplot(g, layout = layout_with_fr,\n     main = \"Social network with the Fruchterman-Reingold layout algorithm\")\n\n\n\n\n\n\n\n\n\nTo improve the visuals, a trivial addition would be to color the vertices according to the gender. We should also try to place the labels slightly outside the vertices to improve readability:\n\n\nCode\nV(g)$color &lt;- ifelse(V(g)$gender == \"m\", \"yellow\", \"red\")\nplot(g, layout = layout, vertex.label.dist = 3.5,\n     main = \"Social network - with genders as colors\")\n\n\n\n\n\n\n\n\n\nYou can also treat the gender attribute as a factor and provide the colors with an argument to plot(), which takes precedence over the color vertex attribute. Colors will be assigned automatically to levels of a factor:\n\n\nCode\nplot(g, layout=layout, vertex.label.dist=3.5, vertex.color=as.factor(V(g)$gender))\n\n\n\n\n\n\n\n\n\nAs seen above with the vertex.color argument, you can specify visual properties as arguments to plot instead of using vertex or edge attributes. The following plot shows the formal ties with thick lines while informal ones with thin lines:\n\n\nCode\nplot(g, layout=layout, vertex.label.dist=3.5, vertex.size=20,\n     vertex.color=ifelse(V(g)$gender == \"m\", \"yellow\", \"red\"),\n     edge.width=ifelse(E(g)$is_formal, 5, 1))\n\n\n\n\n\n\n\n\n\nThis latter approach is preferred if you want to keep the properties of the visual representation of your graph separate from the graph itself.\nIn summary, there are special vertex and edge properties that correspond to the visual representation of the graph. These attributes override the default settings of igraph (i.e color, weight, name, shape,layout,etc.). The following two tables summarise the most frequently used visual attributes for vertices and edges, respectively:\n\n\nVertex attributes controlling graph plots\n\n\n\n\n\n\n\n\nAttribute name\nKeyword argument\nPurpose\n\n\n\n\ncolor\nvertex.color\nColor of the vertex\n\n\nlabel\nvertex.label\nLabel of the vertex. They will be converted to character. Specify NA to omit vertex labels. The default vertex labels are the vertex ids.\n\n\nlabel.cex\nvertex.label.cex\nFont size of the vertex label, interpreted as a multiplicative factor, similarly to R’s text function\n\n\nlabel.color\nvertex.label.color\nColor of the vertex label\n\n\nlabel.degree\nvertex.label.degree\nIt defines the position of the vertex labels, relative to the center of the vertices. It is interpreted as an angle in radian, zero means ‘to the right’, and ‘pi’ means to the left, up is -pi/2 and down is pi/2. The default value is -pi/4\n\n\nlabel.dist\nvertex.label.dist\nDistance of the vertex label from the vertex itself, relative to the vertex size\n\n\nlabel.family\nvertex.label.family\nFont family of the vertex, similarly to R’s text function\n\n\nlabel.font\nvertex.label.font\nFont within the font family of the vertex, similarly to R’s text function\n\n\nshape\nvertex.shape\nThe shape of the vertex, currently “circle”, “square”, “csquare”, “rectangle”, “crectangle”, “vrectangle”, “pie” (see vertex.shape.pie), ‘sphere’, and “none” are supported, and only by the plot.igraph command.\n\n\nsize\nvertex.size\nThe size of the vertex, a numeric scalar or vector, in the latter case each vertex sizes may differ\n\n\n\n\n\nEdge attributes controlling graph plots\n\n\n\n\n\n\n\n\nAttribute name\nKeyword argument\nPurpose\n\n\n\n\ncolor\nedge.color\nColor of the edge\n\n\ncurved\nedge.curved\nA numeric value specifies the curvature of the edge; zero curvature means straight edges, negative values means the edge bends clockwise, positive values the opposite. TRUE means curvature 0.5, FALSE means curvature zero\n\n\narrow.size\nedge.arrow.size\nCurrently this is a constant, so it is the same for every edge. If a vector is submitted then only the first element is used, ie. if this is taken from an edge attribute then only the attribute of the first edge is used for all arrows.\n\n\narrow.width\nedge.arrow.width\nThe width of the arrows. Currently this is a constant, so it is the same for every edge\n\n\nwidth\nedge.width\nWidth of the edge in pixels\n\n\nlabel\nedge.label\nIf specified, it adds a label to the edge.\n\n\nlabel.cex\nedge.label.cex\nFont size of the edge label, interpreted as a multiplicative factor, similarly to R’s text function\n\n\nlabel.color\nedge.label.color\nColor of the edge label\n\n\nlabel.family\nedge.label.family\nFont family of the edge, similarly to R’s text function\n\n\nlabel.font\nedge.label.font\nFont within the font family of the edge, similarly to R’s text function\n\n\n\n\n\nGeneric arguments of plot()\nThese settings can be specified as arguments to the plot function to control the overall appearance of the plot.\n\n\n\n\n\n\n\nKeyword argument\nPurpose\n\n\n\n\nlayout\nThe layout to be used. It can be an instance of Layout, a list of tuples containing X-Y coordinates, or the name of a layout algorithm. The default is auto, which selects a layout algorithm automatically based on the size and connectedness of the graph.\n\n\nmargin\nThe amount of empty space below, over, at the left and right of the plot, it is a numeric vector of length four."
  },
  {
    "objectID": "posts/A8-NetworkData/index.html#assignment",
    "href": "posts/A8-NetworkData/index.html#assignment",
    "title": "Practice with Network Data",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nImprove the network above by:\n\nUpdating the data in this file.\nColoring the edges according to Advisor / BCB520 attribute.\nColoring the nodes according to Department.\nAdujsting the labels to improve readability.\nFind the best layout you can for this garbage. What a nightmare.\nCreate a network from the edge list data.\nTry making a hierarchical network from the org chart data."
  },
  {
    "objectID": "posts/L7-SpatialData/Spatial.html",
    "href": "posts/L7-SpatialData/Spatial.html",
    "title": "Spatial Visualization",
    "section": "",
    "text": "Interview with Fil Riviere\nWorld Cities Urquhart\nLoir-et-Cher isodémographique\nRandom walk dithering\nLatent Interfaces\nUS Presidential Election\nObservable Map Tutorial\nObservable Maps Collection"
  },
  {
    "objectID": "posts/T1-Lit-Prog/index.html",
    "href": "posts/T1-Lit-Prog/index.html",
    "title": "TUTORIAL 1 - Literate Programming",
    "section": "",
    "text": "Learning new tools is hard. Plowing though the tomes of the Data Science Mythos is hard. Perhaps this tutorial will guide you through the mind shattering truths of… LITERATE PROGRAMMING.\nI recorded this video a couple years ago. It should cover the basics, but some elements might be dated."
  },
  {
    "objectID": "posts/T1-Lit-Prog/index.html#intro-to-quarto",
    "href": "posts/T1-Lit-Prog/index.html#intro-to-quarto",
    "title": "TUTORIAL 1 - Literate Programming",
    "section": "",
    "text": "Learning new tools is hard. Plowing though the tomes of the Data Science Mythos is hard. Perhaps this tutorial will guide you through the mind shattering truths of… LITERATE PROGRAMMING.\nI recorded this video a couple years ago. It should cover the basics, but some elements might be dated."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#last-lecture",
    "href": "posts/L3-TaskAbstraction/index.html#last-lecture",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "LAST LECTURE",
    "text": "LAST LECTURE\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#task-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TASK ABSTRACTION",
    "text": "TASK ABSTRACTION\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "FROM DOMAIN TO ABSTRACTION",
    "text": "FROM DOMAIN TO ABSTRACTION"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "KEY COMPONENTS OF TASK ABSTRACTION",
    "text": "KEY COMPONENTS OF TASK ABSTRACTION\n{action, target} pairs\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "href": "posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS AND TARGETS",
    "text": "ACTIONS AND TARGETS"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---analyze",
    "href": "posts/L3-TaskAbstraction/index.html#actions---analyze",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Analyze",
    "text": "ACTIONS - Analyze\n\n\n\nConsume: Information has already been generated and stored as data.\n\nDiscover: new knowledge, test hypothesis, generate new hypothesis, verify\nPresent: communicate something specific and already understood\nEnjoy: casual encounters with visualization\n\nProduce: generate new material or information\n\nAnnotate: addition of graphical or text to existing visualization elements\nRecord: saves or captures visualization elements as persistent artifacts (screenshots, lists, parameter sets, annotations)\nDerive: produce new data based on existing data (aka transform)"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---search",
    "href": "posts/L3-TaskAbstraction/index.html#actions---search",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Search",
    "text": "ACTIONS - Search\n\n\n\nLookup: Location and target both known\n\nExample: Look up humans in the Tree of Life, knowing they are mammals.\n\nLocate: Location unknown and target known\n\nExample: Look up rabbits in the Tree of Life, not knowing they are lagomorphs.\n\nBrowse: Location known and target unknown\n\nExample: Find any clades within Mammalia that have only one species.\n\nExplore: Location unknown and target unknown\n\nExample: Searching for anomalies in time series data."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---query",
    "href": "posts/L3-TaskAbstraction/index.html#actions---query",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Query",
    "text": "ACTIONS - Query\n\n\n\nQuery: How much of the data matters to the task?\n\nIdentify: One (specific Item, individual, cell, etc)\nCompare: Some (multiple targets)\nSummarize: All (very common, aka Overview)"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---all-data",
    "href": "posts/L3-TaskAbstraction/index.html#targets---all-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - All Data",
    "text": "TARGETS - All Data"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---attributes",
    "href": "posts/L3-TaskAbstraction/index.html#targets---attributes",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Attributes",
    "text": "TARGETS - Attributes"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---other-data",
    "href": "posts/L3-TaskAbstraction/index.html#targets---other-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Other Data",
    "text": "TARGETS - Other Data"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#summary",
    "href": "posts/L3-TaskAbstraction/index.html#summary",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "SUMMARY",
    "text": "SUMMARY\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "href": "posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "MANDATORY FUN",
    "text": "MANDATORY FUN\nWe will do these until everyone has done at least one example.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nHOME"
  },
  {
    "objectID": "posts/T7-NHLAwards/index.html",
    "href": "posts/T7-NHLAwards/index.html",
    "title": "TUTORIAL 7",
    "section": "",
    "text": "Code\nIndivdual.Skater &lt;- read.csv(\"SkaterIndividualstats.csv\")\nOnIce.Skater &lt;- read.csv(\"SkaterOnicestats.csv\")\nGoalie &lt;- read.csv(\"Goalies.csv\")\nIndividual.Skater.Rookie &lt;- read.csv(\"RookieSkaterindividual.csv\")\nOnIce.Skater.Rookie &lt;- read.csv(\"RookieSkaterOnIce.csv\")\nRookie.Goalie &lt;- read.csv(\"RookieGoalies.csv\")"
  },
  {
    "objectID": "posts/T7-NHLAwards/index.html#vezina-ballot",
    "href": "posts/T7-NHLAwards/index.html#vezina-ballot",
    "title": "TUTORIAL 7",
    "section": "VEZINA BALLOT",
    "text": "VEZINA BALLOT\nWho is the best goalie? I’ll start by examining the stats from the assignment.\n\n\nCode\nggplot(Goalie, aes(x=SV.))+\n  geom_histogram(binwidth = .01)+\n  labs(x = \"Save Percentage\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Save Percentage\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(Goalie, aes(x=GAA))+\n  geom_histogram(binwidth = .2)+\n  labs(x = \"Goals Against Average\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Goals Against Average\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(Goalie, aes(x=GSAA))+\n  geom_histogram(binwidth = 1)+\n  labs(x = \"Goals Saved Above Average\",\n       y = \"Number of Players\",\n       caption = \"source: https://www.naturalstattrick.com/\",\n       title = \"Distribution of Goals Saved Above Average\",\n       subtitle = \"2024-2025 season stats as of March 4\")\n\n\n\n\n\n\n\n\n\nNot super helpful. This is because our task is related to comparing values for individual players. Let’s do that for Save Percentage.\n\n\nCode\nggplot(Goalie, aes(x=SV., y=reorder(Player, SV.))) +\n  geom_col(fill=\"#1F77B4\") +\n  labs(x = \"Save Percentage\",\n       y = \"Player Name\",\n       caption = \"Source: https://www.naturalstattrick.com/\",\n       title = \"Individual Save Percentage\",\n       subtitle = \"2024-2025 season stats as of March 4\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\nYay! We are done! My Vezina Ballot will be the top five players on this plot in order. Right?"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#last-lecture",
    "href": "posts/L2-Data-Abstraction/index.html#last-lecture",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "LAST LECTURE",
    "text": "LAST LECTURE\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#what",
    "href": "posts/L2-Data-Abstraction/index.html#what",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\n Before you design a visualization, you need to understand the data. Here, we consider the semantics to describe the DATA TYPES and DATA ATTRIBUTES."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#the-three-major-data-types",
    "href": "posts/L2-Data-Abstraction/index.html#the-three-major-data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "THE THREE MAJOR DATA TYPES",
    "text": "THE THREE MAJOR DATA TYPES"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-attributes",
    "href": "posts/L2-Data-Abstraction/index.html#data-attributes",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA ATTRIBUTES",
    "text": "DATA ATTRIBUTES"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#what-1",
    "href": "posts/L2-Data-Abstraction/index.html#what-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\n\n\n\nWhat does this sequence of six numbers mean?\n14, 2.6, 30, 30, 15, 100001\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-1",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\n\n\n\nWhat does this sequence of six numbers mean?\n14, 2.6, 30, 30, 15, 100001 Two points far from each other in 3D space?\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-2",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\n\n\n\nWhat does this sequence of six numbers mean?\n14, 2.6, 30, 30, 15, 100001 Two points close to each other in 2D space, with 15 links between them, and a weight of 100001 for the link?\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-3",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-3",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\n\n\n\nWhat about this data?\nBasil, 7, S, Pear:"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-4",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-4",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\n\n\n\nWhat about this data?\nBasil, 7, S, Pear: Food shipment of produce (basil & pear) arrived in satisfactory condition on 7th day of month\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-5",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-5",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\n\n\n\nWhat about this data?\nBasil, 7, S, Pear: Basil Point neighborhood of city had 7 inches of snow cleared by the Pear Creek Limited snow removal service\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-6",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-6",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\n\n\n\nWhat about this data?\nBasil, 7, S, Pear: Lab rat Basil made 7 attempts to find way through south section of maze, these trials used pear as reward food\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#semantics",
    "href": "posts/L2-Data-Abstraction/index.html#semantics",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SEMANTICS",
    "text": "SEMANTICS\nThe meaning of a word, phrase, sentence, or text.\nBasil, 7, S, Pear"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#semantics-for-data",
    "href": "posts/L2-Data-Abstraction/index.html#semantics-for-data",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SEMANTICS FOR DATA",
    "text": "SEMANTICS FOR DATA\n\n\n\n\n\n\nSemantics\n\nitem: individual entity, discrete\n\neg patient, car, stock, city\n“independent variable”\n\nattribute: property that is measured, observed, logged…\n\neg height, blood pressure for patient\neg horsepower, make for car\n“dependent variable”\n\n\n\n\nData Table\n\n\n\n\nITEM: Person\nATTRIBUTES: Name, Age, Shirt Size, Favorite Fruit"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#other-data-types",
    "href": "posts/L2-Data-Abstraction/index.html#other-data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "OTHER DATA TYPES",
    "text": "OTHER DATA TYPES\n\nLinks\n\nexpress relationship between two items\ne.g/ friendship on facebook, interaction between proteins\n\nPositions\n\nspatial data: location in 2D or 3D\ne.g. pixels in photo, voxels in MRI scan, latitude/longitude\n\nGrids\n\nsampling strategy for continuous data"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#what-2",
    "href": "posts/L2-Data-Abstraction/index.html#what-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#dataset-types-tables",
    "href": "posts/L2-Data-Abstraction/index.html#dataset-types-tables",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATASET TYPES: TABLES",
    "text": "DATASET TYPES: TABLES\nFlat Table\n\n\n\nOne ITEM per row\n\noften called an observation\n\nEach column is an ATTRIBUTE\n\noften called a variable\n\nA cell holds the VALUE for an item/attribute pair\nA UNIQUE KEY can be used (implicitly or explicitly) to identify each item even if they share all measured attributes"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#flat-table-example",
    "href": "posts/L2-Data-Abstraction/index.html#flat-table-example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "FLAT TABLE EXAMPLE",
    "text": "FLAT TABLE EXAMPLE"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#multidimensional-tables",
    "href": "posts/L2-Data-Abstraction/index.html#multidimensional-tables",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MULTIDIMENSIONAL TABLES",
    "text": "MULTIDIMENSIONAL TABLES\nindexing based on multiple keys (eg genes, patients)"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#networks",
    "href": "posts/L2-Data-Abstraction/index.html#networks",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "NETWORKS",
    "text": "NETWORKS\nNetwork/graph nodes (vertices) connected by links (edges). A tree is special case: no cycles, often have roots, and are directed."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#fields",
    "href": "posts/L2-Data-Abstraction/index.html#fields",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "FIELDS",
    "text": "FIELDS"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#spatial-fields-1",
    "href": "posts/L2-Data-Abstraction/index.html#spatial-fields-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 1",
    "text": "SPATIAL FIELDS 1\nAttribute values associated with cells.\nEach cell contains a value from a continuous domain (eg temperature, pressure, wind velocity measured or simulated)."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#spatial-fields-2",
    "href": "posts/L2-Data-Abstraction/index.html#spatial-fields-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 2",
    "text": "SPATIAL FIELDS 2\n\n\n\n\n\n\n\nAttribute values associated with cells.\nEach cell contains value from continuous domain\n\n(eg temperature, pressure, wind velocity)\n\nMeasured or simulated.\nMajor concerns and potential limitations:\n\nSampling: where are the attributes measured and at what density?\nInterpolation: how to model attributes in cells with no measurements?\nGrid types: optimizing size, shape, etc."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#spatial-fields-3",
    "href": "posts/L2-Data-Abstraction/index.html#spatial-fields-3",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 3",
    "text": "SPATIAL FIELDS 3\n\n\n\nData structures can be complex, scaling with the number of attributes per cell:\n\nScalar (1)\nVector (2)\nTensor (many)"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#geometry",
    "href": "posts/L2-Data-Abstraction/index.html#geometry",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "GEOMETRY",
    "text": "GEOMETRY"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#geometry-2",
    "href": "posts/L2-Data-Abstraction/index.html#geometry-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "GEOMETRY 2",
    "text": "GEOMETRY 2\n\n\n\nShape of items or observations is defined.\nExplicit spatial positions / regions\n\npoints, lines, curves, surfaces, volumes\n\n\n\n Lopez et.al., 2024"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#collections",
    "href": "posts/L2-Data-Abstraction/index.html#collections",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "COLLECTIONS",
    "text": "COLLECTIONS"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#collections-2",
    "href": "posts/L2-Data-Abstraction/index.html#collections-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "COLLECTIONS 2",
    "text": "COLLECTIONS 2\n\n\nGrouping Items:\n\nSets\n\nunique items\nunordered\n\nLists\n\nordered\nduplicates possible\n\nClusters\n\ngroups of similar items"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-types",
    "href": "posts/L2-Data-Abstraction/index.html#data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA TYPES",
    "text": "DATA TYPES"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#attribute-types",
    "href": "posts/L2-Data-Abstraction/index.html#attribute-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "ATTRIBUTE TYPES",
    "text": "ATTRIBUTE TYPES\n\n\n\nCategorical (nominal):\n\ncompare equality\nno implicit ordering\n\nOrdered:\n\nordinal\nless/greater than defined\n\nQuantitative:\n\nmeaningful magnitude\narithmetic possible"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#example",
    "href": "posts/L2-Data-Abstraction/index.html#example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "EXAMPLE",
    "text": "EXAMPLE\nCategorical\nOrdinal\nQuantitative"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#example-1",
    "href": "posts/L2-Data-Abstraction/index.html#example-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "EXAMPLE",
    "text": "EXAMPLE\nCategorical\nOrdinal\nQuantitative"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#additional-semantic-components",
    "href": "posts/L2-Data-Abstraction/index.html#additional-semantic-components",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "ADDITIONAL SEMANTIC COMPONENTS",
    "text": "ADDITIONAL SEMANTIC COMPONENTS"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-abstraction",
    "href": "posts/L2-Data-Abstraction/index.html#data-abstraction",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA ABSTRACTION",
    "text": "DATA ABSTRACTION\n\nGOAL: Translate from domain-specific language to generic (and consistent) visualization language.\n\nIdentify dataset type(s) and attribute types.\nIdentify cardinality.\n\nhow many items in the dataset?\nwhat is cardinality of each attribute?\n\nnumber of levels for categorical data?\nrange for quantitative data\n\n\nConsider whether to transform the data.\n\nguided by your understanding of the task."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#models-data-vs-conceptual",
    "href": "posts/L2-Data-Abstraction/index.html#models-data-vs-conceptual",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MODELS: DATA VS CONCEPTUAL",
    "text": "MODELS: DATA VS CONCEPTUAL\n\n\nData Model\n\nmathematical abstraction\nsets with operations, eg floats with * / - + or variable data types in programming languages\n\nConceptual Model\n\nmental construction (semantics)\nsupports reasoning\ntypically based on understanding of tasks\n\nThe Data Abstraction process relies on conceptual model for transforming data if needed"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#models-example",
    "href": "posts/L2-Data-Abstraction/index.html#models-example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MODELS: EXAMPLE",
    "text": "MODELS: EXAMPLE\n\nData Model: floats\n\n32.52, 54.06, -14.35,\n\nConceptual Model:\n\ntemperature\n\nPossible data abstractions:\n\nQUANTITATIVE: continuous to 2 significant figures:\n\nTASK: Forecasting the weather\n\nORDINAL: Hot, Warm, Cold:\n\nTASK: Deciding if my bath water is ready\n\nCATEGORICAL: Above Freezing, Below Freezing:\n\nTASK: Deciding if I should leave the house today"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#derived-attributes",
    "href": "posts/L2-Data-Abstraction/index.html#derived-attributes",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DERIVED ATTRIBUTES",
    "text": "DERIVED ATTRIBUTES\nDerived attribute: Data computed from original (collected, observed) attributes."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#summary",
    "href": "posts/L2-Data-Abstraction/index.html#summary",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SUMMARY",
    "text": "SUMMARY\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\n\n\n\nHOME"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#vad-model",
    "href": "posts/L4-Marks-Channels/index.html#vad-model",
    "title": "LECTURE 4",
    "section": "VAD MODEL",
    "text": "VAD MODEL"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#understand-the-data",
    "href": "posts/L4-Marks-Channels/index.html#understand-the-data",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE DATA",
    "text": "UNDERSTAND THE DATA\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#understand-the-task",
    "href": "posts/L4-Marks-Channels/index.html#understand-the-task",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE TASK",
    "text": "UNDERSTAND THE TASK\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#visual-encoding",
    "href": "posts/L4-Marks-Channels/index.html#visual-encoding",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING",
    "text": "VISUAL ENCODING\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#other-frameworks",
    "href": "posts/L4-Marks-Channels/index.html#other-frameworks",
    "title": "LECTURE 4",
    "section": "OTHER FRAMEWORKS",
    "text": "OTHER FRAMEWORKS\n\nThe Tidyverse\nThe Grammar of Graphics\nTufte"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#tidyverse",
    "href": "posts/L4-Marks-Channels/index.html#tidyverse",
    "title": "LECTURE 4",
    "section": "TIDYVERSE",
    "text": "TIDYVERSE\nR packages for data science:\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The best way to explore and understand the tidyverse is with cheetsheets, like this one for tidyr!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "href": "posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "title": "LECTURE 4",
    "section": "GRAMMAR OF GRAPHICS",
    "text": "GRAMMAR OF GRAPHICS\nThe ggplot2 cheatsheet!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#tufte",
    "href": "posts/L4-Marks-Channels/index.html#tufte",
    "title": "LECTURE 4",
    "section": "TUFTE",
    "text": "TUFTE\nTufte’s Website\nA Quarto Page Layout Example"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#analysis-framework",
    "href": "posts/L4-Marks-Channels/index.html#analysis-framework",
    "title": "LECTURE 4",
    "section": "ANALYSIS FRAMEWORK",
    "text": "ANALYSIS FRAMEWORK\nFour levels, three questions\n\n\n\nDomain situation defines the target users.\nAbstraction translate from specifics of domain to vocabulary of vis\n\nWHAT is shown? data abstraction\nWHY is the user looking at it? task abstraction\n\nIdiom defines the visualization\n\nHOW is it shown?\n\nvisual encoding idiom: how to draw\ninteraction idiom: how to manipulate\n\n\nAlgorithm creates the visualization\n\nevaluated with computational efficiency"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#encoding",
    "href": "posts/L4-Marks-Channels/index.html#encoding",
    "title": "LECTURE 4",
    "section": "ENCODING",
    "text": "ENCODING\nWe are defining the structure of the visualization (the idiom).\nTo do this, we use MARKS and CHANNELS:\n\nMARKS represent ITEMS or LINKS (aka OBSERVATIONS)\nCHANNELS change the appearance of MARKS based on ATTRIBUTES (aka VARIABLES)"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#marks-for-items",
    "href": "posts/L4-Marks-Channels/index.html#marks-for-items",
    "title": "LECTURE 4",
    "section": "MARKS FOR ITEMS",
    "text": "MARKS FOR ITEMS"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#marks-for-links",
    "href": "posts/L4-Marks-Channels/index.html#marks-for-links",
    "title": "LECTURE 4",
    "section": "MARKS FOR LINKS",
    "text": "MARKS FOR LINKS\n\n Bubblesets\n Force Directed Graph"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "href": "posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "title": "LECTURE 4",
    "section": "OBSERVABLE IN QUARTO!",
    "text": "OBSERVABLE IN QUARTO!\n\n\nCode\nd3 = require(\"d3@7\")\n\n\nchart = ForceGraph(miserables, {\n  nodeId: d =&gt; d.id,\n  nodeGroup: d =&gt; d.group,\n  nodeTitle: d =&gt; `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l =&gt; Math.sqrt(l.value),\n  width,\n  height: 1000,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d =&gt; d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) =&gt; source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) =&gt; target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 1000, // outer width, in pixels\n  height = 1000, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) =&gt; N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) =&gt; ({id: N[i]}));\n  links = d3.map(links, (_, i) =&gt; ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) =&gt; N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) =&gt; W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) =&gt; L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) =&gt; color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) =&gt; T[i]);\n  if (invalidation != null) invalidation.then(() =&gt; simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d =&gt; d.source.x)\n      .attr(\"y1\", d =&gt; d.source.y)\n      .attr(\"x2\", d =&gt; d.target.x)\n      .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\""
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channels",
    "href": "posts/L4-Marks-Channels/index.html#channels",
    "title": "LECTURE 4",
    "section": "CHANNELS",
    "text": "CHANNELS\n\n\n\nCHANNELS control the appearance of MARKS.\nThey are proportional to or based on ATTRIBUTES (aka VARIABLES).\nTheir properties differ in the type and amount of information that can be conveyed to the human perceptual system."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#this-is-important",
    "href": "posts/L4-Marks-Channels/index.html#this-is-important",
    "title": "LECTURE 4",
    "section": "THIS IS IMPORTANT",
    "text": "THIS IS IMPORTANT\nChannel properties differ in the type and amount of information that can be conveyed to the human perceptual system."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "href": "posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING EXAMPLE",
    "text": "VISUAL ENCODING EXAMPLE\nLet’s analyze the idiom structures below in terms of marks and channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#analyze-the-marks-and-channels",
    "href": "posts/L4-Marks-Channels/index.html#analyze-the-marks-and-channels",
    "title": "LECTURE 4",
    "section": "ANALYZE THE MARKS AND CHANNELS",
    "text": "ANALYZE THE MARKS AND CHANNELS\nMarks are defined by the ITEMS or OBSERVATIONS they represent.\nChannels are defined by the visually detectable properties that are mapped on to ATTRIBUTES or VARIABLES.\n\n\n\n\n\n\n\n\n\nSharon’s Microbiomes\n\n\n\n\n\n\n\nMarwa’s Reddit Network\n\n\n\n\n\n\n\nYaotian’s Wheat"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#redundant-encoding",
    "href": "posts/L4-Marks-Channels/index.html#redundant-encoding",
    "title": "LECTURE 4",
    "section": "REDUNDANT ENCODING",
    "text": "REDUNDANT ENCODING\n\n\nUses multiple channels for the same attribute.\n\nSends a stronger message\nUses up channels\nBonus points if you can identify BOTH channels in this figure!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#wheat-growth-by-area",
    "href": "posts/L4-Marks-Channels/index.html#wheat-growth-by-area",
    "title": "LECTURE 4",
    "section": "WHEAT GROWTH BY AREA*",
    "text": "WHEAT GROWTH BY AREA*\nBoth of Yaotian’s plots contain Redundant Encoding. Is this approach equally valuable for both plots?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#choosing-channels",
    "href": "posts/L4-Marks-Channels/index.html#choosing-channels",
    "title": "LECTURE 4",
    "section": "CHOOSING CHANNELS",
    "text": "CHOOSING CHANNELS\n\nEXPRESSIVENESS\n\nThe visual encoding should express all of, and only, the information in the dataset attributes.\n\nEFFECTIVENESS\n\nChannels differ in accuracy of perception.\nThe importance of the attribute should match the salience of the channel (its noticability)."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#expressiveness",
    "href": "posts/L4-Marks-Channels/index.html#expressiveness",
    "title": "LECTURE 4",
    "section": "EXPRESSIVENESS",
    "text": "EXPRESSIVENESS\nThe advantages and disadvantages of jitter plots. How might this relate to the idea of expressiveness?\n\nHeidi’s Cytoswine DataEXPRESSIVENESS: The visual encoding should express all of, and only, the information in the dataset attributes."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channel-effectiveness-rankings",
    "href": "posts/L4-Marks-Channels/index.html#channel-effectiveness-rankings",
    "title": "LECTURE 4",
    "section": "CHANNEL EFFECTIVENESS RANKINGS",
    "text": "CHANNEL EFFECTIVENESS RANKINGS\n\nNote that spatial position ranks high for both types of channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#grouping",
    "href": "posts/L4-Marks-Channels/index.html#grouping",
    "title": "LECTURE 4",
    "section": "GROUPING",
    "text": "GROUPING\n\n\n\nContainment\nConnection\nProximity\n\nSame spatial region.\n\nSimilarity\n\nSame values as other channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#summary-so-far",
    "href": "posts/L4-Marks-Channels/index.html#summary-so-far",
    "title": "LECTURE 4",
    "section": "SUMMARY SO FAR",
    "text": "SUMMARY SO FAR"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "href": "posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "title": "LECTURE 4",
    "section": "CHANNEL EFFECTIVENESS",
    "text": "CHANNEL EFFECTIVENESS\n\nAccuracy: how precisely can we tell the difference between encoded items?\nDiscriminability: how many unique steps can we perceive?\nSeparability: is our ability to use this channel affected by another one?\nPopout: can things jump out using this channel?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy-theory",
    "href": "posts/L4-Marks-Channels/index.html#accuracy-theory",
    "title": "LECTURE 4",
    "section": "ACCURACY (THEORY)",
    "text": "ACCURACY (THEORY)\nSteven’s Psychophisical Power Law: \\(S=I^N\\)\n\n\n\n\n\n\n\n\n\n\n\n\nLENGTH (N=1)\nELECTRIC SHOCK (N=3.5)\nSATURATION (N=1.7)\nAREA (N=0.7)\nBRIGHTNESS (N=0.5)"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "href": "posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "title": "LECTURE 4",
    "section": "ACCURACY (EXPERIMENTAL)",
    "text": "ACCURACY (EXPERIMENTAL)\n\n\n\n\n[Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods]"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy",
    "href": "posts/L4-Marks-Channels/index.html#accuracy",
    "title": "LECTURE 4",
    "section": "ACCURACY?",
    "text": "ACCURACY?\nDepends on the task. Which genre has the most game titles? Which genres are the top 4 in terms of game titles? Are there more Puzzle games than Strategy games?\n\nGeraline’s Video Games"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#discriminability",
    "href": "posts/L4-Marks-Channels/index.html#discriminability",
    "title": "LECTURE 4",
    "section": "DISCRIMINABILITY",
    "text": "DISCRIMINABILITY\nHow many usable steps are in the channel? Are the differences between items perceptible to the human as intended?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#discriminability-and-colors",
    "href": "posts/L4-Marks-Channels/index.html#discriminability-and-colors",
    "title": "LECTURE 4",
    "section": "DISCRIMINABILITY and COLORS",
    "text": "DISCRIMINABILITY and COLORS\n\nGeraline’s Gaming Platforms"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "href": "posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "title": "LECTURE 4",
    "section": "SEPARABILITY VS INTEGRALITY",
    "text": "SEPARABILITY VS INTEGRALITY\nSeparable channels are orthogonal and independent. Integral channels are inextricably combined. Attempts to encode different information with integral channels creates Interference.\n\n\nFigure 5.10. Pairs of visual channels fall along a continuum from fully separable to intrinsically integral. Color and location are separable channels well suited to encode different data attributes for two different groupings that can be selectively attended to. However, size interacts with hue, which is harder to perceive for small objects. The horizontal size and and vertical size channels are automatically fused into an integrated perception of area, yielding three groups. Attempts to code separate information along the red and green axes of the RGB color space fail, because we simply perceive four different hues."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#separability",
    "href": "posts/L4-Marks-Channels/index.html#separability",
    "title": "LECTURE 4",
    "section": "SEPARABILITY",
    "text": "SEPARABILITY\n\nRedundancy may be desirable, but area interferes with hue, with larger shapes having more visual salience."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout",
    "href": "posts/L4-Marks-Channels/index.html#popout",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\nVISUAL POPOUT is often called preattentive processing or tunable detection.\n\n\nfind the red dot! How long does it take?\nPopout results from our low-level visual system performing massively parallel processing on certain visual channels, eliminating the need for the viewer to consciously direct attention to items one by one (serial search).\n\nFigure 5.11. Visual popout. (a) The red circle pops out from a small set of blue circles. (b) The red circle pops out from a large set of blue circles just as quickly. (c) The red circle also pops out from a small set of square shapes, although a bit slower than with color. (d) The red circle also pops out of a large set of red squares. (e) The red circle does not take long to find from a small set of mixed shapes and colors. (f) The red circle does not pop out from a large set of red squares and blue circles, and it can only be found by searching one by one through all the objects."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout-1",
    "href": "posts/L4-Marks-Channels/index.html#popout-1",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\n\n\nMany channels are compatible with preattentive processing and facilitate popout:\n\ntilt\nsize\nshape\nproximity\nshadow direction\n\nBut not all!\n\nExample: parallel line pairs do not pop out from tilted pairs."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout-goes-the-weevil",
    "href": "posts/L4-Marks-Channels/index.html#popout-goes-the-weevil",
    "title": "LECTURE 4",
    "section": "POPOUT GOES THE WEEVIL?",
    "text": "POPOUT GOES THE WEEVIL?\n\n\n\n\n\n\n\n\n\nLucas Weevils before chemicals\n\n\n\n\n\n\n\nLucas Weevils after chemicals"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE VS ABSOLUTE JUDGEMENTS",
    "text": "RELATIVE VS ABSOLUTE JUDGEMENTS\nThe human perceptual system is fundamentally based on relative judgements, not absolute ones. This is why accuracy increases with common frame/scale and alignment.\nWeber’s Law: The detectable difference in stimulus intensity \\(I\\) as a fixed percentage \\(K\\) of the object magnitude: \\(dI/I=K\\) .\n\nThe filled rectangles differ in length by 1:9, and it is therefore difficult to detect the difference without aligment. The white rectangles differ in length by 1:2, it is easier to see this difference even when the objects are unaligned."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE JUDGEMENTS",
    "text": "RELATIVE JUDGEMENTS\n\nHeidi’s Microbiomes"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE LUMINANCE JUDGEMENTS",
    "text": "RELATIVE LUMINANCE JUDGEMENTS\nHuman perception of luminance is completely contextual, and is based on contrast with surrounding colors."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE COLOR JUDGEMENTS",
    "text": "RELATIVE COLOR JUDGEMENTS\nOur visual system evolved to provide color constancy so that the same surface is identifiable across a broad set of illumination conditions, even though a physical light meter would yield very different readings. While the visual system works very well in natural environments, many of its mechanisms work against simple approaches to visually encoding information with color.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.15 shows two colorful cubes. In Figure 5.15(a) corresponding squares both appear to be red. In Figure 5.15(b), masks show that the tile color in the image apparently illuminated by a yellowish light source is actually orange, and for the bluish light the tiles are actually purple.\n\n\n\n\nHOME"
  },
  {
    "objectID": "posts/T7-NHLAwards/Ballot.html",
    "href": "posts/T7-NHLAwards/Ballot.html",
    "title": "BarrieBallots",
    "section": "",
    "text": "Code\nIndividual.Skater &lt;- read.csv(\"SkaterIndividualstats.csv\")\nOnIce.Skater &lt;- read.csv(\"SkaterOnicestats.csv\")\nGoalie &lt;- read.csv(\"Goalies.csv\")\nIndividual.Skater.Rookie &lt;- read.csv(\"RookieSkaterindividual.csv\")\nOnIce.Skater.Rookie &lt;- read.csv(\"RookieSkaterOnIce.csv\")\nRookie.Goalie &lt;- read.csv(\"RookieGoalies.csv\")"
  },
  {
    "objectID": "posts/T7-NHLAwards/Ballot.html#vezina-ballot",
    "href": "posts/T7-NHLAwards/Ballot.html#vezina-ballot",
    "title": "BarrieBallots",
    "section": "VEZINA BALLOT",
    "text": "VEZINA BALLOT\n\n\nCode\nScale.Goalies &lt;- Goalie %&gt;%\n  filter(GP&gt;25)%&gt;%\n  mutate(scale.SV. = scale(SV.))%&gt;%\n  mutate(scale.GAA = -scale(GAA))%&gt;%\n  mutate(scale.GSAA = scale(GSAA))%&gt;%\n  mutate(scale.GSAx = scale(xG.Against-Goals.Against))%&gt;%\n  mutate(scale.Vezina = scale.SV. + scale.GAA + scale.GSAA + scale.GSAx)%&gt;%\n  \n  mutate(rank.SV. = rank(scale.SV.))%&gt;%\n  mutate(rank.GAA = rank(scale.GAA))%&gt;%\n  mutate(rank.GSAA = rank(scale.GSAA))%&gt;%\n  mutate(rank.GSAx = rank(scale.GSAx))\n\nScale.Vez.plot &lt;- Scale.Goalies %&gt;%\n  filter(GP &gt; 25) %&gt;%\n  \n  # Pivot to long format for stacking\n  pivot_longer(\n    cols = c(scale.GAA, scale.GSAA, scale.SV., scale.GSAx,\n             rank.GAA, rank.GSAA, rank.SV., rank.GSAx),\n    names_to = \"component\",\n    values_to = \"value\"\n  )\n\n\nggplot(Scale.Vez.plot%&gt;%\n         filter(str_detect(component, \"scale\")), aes(x = reorder(Player, scale.Vezina), y = value, fill = component)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"scale.GAA\" = \"#1F77B4\", \n               \"scale.GSAA\" = \"#D62728\",\n               \"scale.SV.\" = \"#FF7F0E\",\n               \"scale.GSAx\" = \"black\")\n  ) +\n  labs(x = \"Player Name\",\n       y = \"Vezina Score Components\",\n       fill = \"Component\",\n       caption = \"Source: https://www.naturalstattrick.com/\",\n       title = \"Vezina Score Components (save.resid - goal.resid)\",\n       subtitle = \"2024-2025 season stats as of March 4\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/T7-NHLAwards/Ballot.html#hart-ballot",
    "href": "posts/T7-NHLAwards/Ballot.html#hart-ballot",
    "title": "BarrieBallots",
    "section": "HART BALLOT",
    "text": "HART BALLOT\nAwarded to the “player judged most valuable to his team.” This isn’t necessarily the best overall player, but rather the one who contributes most significantly to his team’s success.\n\n\nCode\nScale.OnIce &lt;- OnIce.Skater %&gt;%\n  filter(GP &gt; 30) %&gt;%\n  select(Player, Position, Team, CF., xGF.) %&gt;%\n  mutate(Position = if_else(Position == \"D\", \"D\", \"F\")) %&gt;%\n  # Extract team after comma, or keep original if no comma exists\n  mutate(Team = case_when(\n    str_detect(Team, \",\") ~ str_trim(str_extract(Team, \",[^,]*$\")), # Extract everything after the last comma\n    TRUE ~ Team\n  )) %&gt;%\n  # Remove the leading comma if it exists\n  mutate(Team = str_replace(Team, \"^,\\\\s*\", \"\")) %&gt;%\n  group_by(Position) %&gt;%\n  mutate(scale.CF. = scale(CF.),\n         scale.xGF. = scale(xGF.)) %&gt;%\n  mutate(scale.Hart = scale.xGF. + scale.CF.)\n\nScale.Individual &lt;- Individual.Skater %&gt;%\n  filter(GP &gt; 30) %&gt;%\n  select(Player, Position, Team, Goals, Total.Assists) %&gt;%\n  mutate(Position = if_else(Position == \"D\", \"D\", \"F\")) %&gt;%\n  # Extract team after comma, or keep original if no comma exists\n  mutate(Team = case_when(\n    str_detect(Team, \",\") ~ str_trim(str_extract(Team, \",[^,]*$\")), # Extract everything after the last comma\n    TRUE ~ Team\n  )) %&gt;%\n  # Remove the leading comma if it exists\n  mutate(Team = str_replace(Team, \"^,\\\\s*\", \"\")) %&gt;%\n  group_by(Position) %&gt;%\n  mutate(scale.Goals = scale(Goals),\n         scale.Assists = scale(Total.Assists)) %&gt;%\n  mutate(scale.Hart.Ind = scale.Goals + scale.Assists)\n\n\nScale.Hart &lt;- Scale.OnIce %&gt;%\n  left_join(Scale.Individual, by = \"Player\") %&gt;%\n  mutate(resid.Goals = resid(lm(scale.Goals ~ Team.x, data = .)),\n         resid.Assists = resid(lm(scale.Assists ~ Team.x, data = .)),\n         resid.CF. = resid(lm(scale.CF. ~ Team.x, data = .)),\n         resid.xGF. = resid(lm(scale.xGF. ~ Team.x, data = .)))\n\n\n\n# Step 1: Data frame preparation\nHart.Long &lt;- Scale.Hart %&gt;%\n  # First calculate Hart score for each player if not already done\n  mutate(Hart = resid.Goals + resid.Assists + resid.CF. + resid.xGF.) %&gt;%\n  pivot_longer(cols = c(scale.CF., scale.xGF., scale.Assists, scale.Goals,\n                        resid.Assists, resid.CF., resid.Goals, resid.xGF.), \n               names_to = \"component\", \n               values_to = \"value\") \n\n\n\n# Split the data into separate components\nlibrary(purrr)\n\n\nhart_filtered &lt;- Hart.Long %&gt;%\n  filter(str_detect(component, \"resid\"), Hart &gt; 6)\n\n# Get unique components\ncomponents &lt;- unique(hart_filtered$component)\n\n# Create a list of plots, one for each component\ncomponent_plots &lt;- map(components, function(comp) {\n  # Filter for just this component\n  comp_data &lt;- hart_filtered %&gt;% filter(component == comp)\n  \n  # Create component-specific plot with its own ordering\n  ggplot(comp_data, \n         aes(x = reorder(Player, value), y = value, fill = Position.x)) +\n    geom_col() +\n    coord_flip() +\n    scale_fill_manual(\n      values = c(\"D\" = \"#1F77B4\", \n                \"F\" = \"#D62728\"),\n      labels = c(\"D\" = \"Defenseman\", \n                \"F\" = \"Forward\")\n    ) +\n    labs(x = NULL, # We'll add a common x-axis label later\n         y = NULL, # We'll add a common y-axis label later\n         fill = \"Position\") +\n    theme_minimal() +\n    theme(\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor = element_blank(),\n      axis.text.y = element_text(size = 8),\n      legend.position = if(comp == components[1]) \"bottom\" else \"none\"\n    ) +\n    ggtitle(comp)\n})\n\n# Combine the plots\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\ncombined_plot &lt;- do.call(grid.arrange, c(component_plots, ncol = 2))\n\n\n\n\n\n\n\n\n\nCode\n# Add overall title and caption\ntitle &lt;- grid::textGrob(\"Top 2 Players Per Team - Hart Score Components\", \n                        gp = grid::gpar(fontsize = 14, fontface = \"bold\"))\nsubtitle &lt;- grid::textGrob(\"2024-2025 season stats as of March 4\", \n                          gp = grid::gpar(fontsize = 10))\ncaption &lt;- grid::textGrob(\"Source: https://www.naturalstattrick.com/\", \n                          gp = grid::gpar(fontsize = 8), \n                          hjust = 1)\n\ngrid.arrange(title, subtitle, combined_plot, caption,\n             heights = c(0.5, 0.3, 10, 0.3),\n             ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Step 2: Create the plot\nggplot(Hart.Long%&gt;%\n         filter(Hart&gt;6, str_detect(component, \"resid\")), \n                   aes(x = reorder(Player, Hart), y = value, fill = component)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"resid.Goals\" = \"#1F77B4\", \n               \"resid.Assists\" = \"#D62728\",\n               \"resid.CF.\" = \"#FF7F0E\",\n               \"resid.xGF.\" = \"black\")\n  ) +\n  \n  labs(x = \"Player Name\",\n       y = \"Hart Score Components\",\n       fill = \"Component\",\n       caption = \"Source: https://www.naturalstattrick.com/\",\n       title = \"Top 1 Players Per Team - Hart Score Components\",\n       subtitle = \"2024-2025 season stats as of March 4\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    # Improve readability of player names\n    axis.text.y = element_text(size = 8)\n  )+\n  facet_wrap(~Position.x)"
  },
  {
    "objectID": "posts/T7-NHLAwards/Ballot.html#james-norris-memorial-trophy",
    "href": "posts/T7-NHLAwards/Ballot.html#james-norris-memorial-trophy",
    "title": "BarrieBallots",
    "section": "James Norris Memorial Trophy",
    "text": "James Norris Memorial Trophy\nAwarded to the defenseman who demonstrates “the greatest all-around ability” at the position.\n\n\nCode\nNorris.OnIce &lt;- OnIce.Skater%&gt;%\n  filter(Position == \"D\", GP &gt; 30)%&gt;%\n  mutate(scale.CF. = scale(CF.),\n         scale.xGF. = scale(xGF.))%&gt;%\n  select(Player, Team, GP, CF., xGF., scale.CF., scale.xGF. )\nNorris.Indiv &lt;- Individual.Skater%&gt;%\n  filter(Position == \"D\", GP &gt; 30)%&gt;%\n  mutate(scale.Goals = scale(Goals),\n         scale.Assists = scale(Total.Assists))%&gt;%\n  select(Player, Team, GP, Goals, Total.Assists, scale.Goals, scale.Assists )\n\nNorris&lt;- Norris.OnIce%&gt;%\n  left_join(Norris.Indiv, by = \"Player\")%&gt;%\n  mutate(scale.Norris = scale.CF. + scale.xGF. + scale.Goals + scale.Assists)\n\nNorris.Plot &lt;- Norris %&gt;%\n  pivot_longer(cols = c(scale.Goals, scale.Assists, scale.CF., scale.xGF.), \n               names_to = \"component\", \n               values_to = \"value\") %&gt;%\n  filter(str_detect(component, \"scale\"))\n\n# Step 2: Create the plot\nggplot(Norris.Plot%&gt;% filter(scale.Norris &gt; 3), \n                   aes(x = reorder(Player, scale.Norris), y = value, fill = component)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"scale.Goals\" = \"#1F77B4\", \n               \"scale.Assists\" = \"#D62728\",\n               \"scale.CF.\" = \"#FF7F0E\",\n               \"scale.xGF.\" = \"black\")\n  ) +\n  \n  labs(x = \"Player Name\",\n       y = \"Norris Score Components\",\n       fill = \"Component\",\n       caption = \"Source: https://www.naturalstattrick.com/\",\n       title = \"Top Players - Norris Score Components\",\n       subtitle = \"2024-2025 season stats as of March 4\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    # Improve readability of player names\n    axis.text.y = element_text(size = 8)\n  )\n\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead."
  },
  {
    "objectID": "posts/T7-NHLAwards/Ballot.html#calder-memorial-trophy",
    "href": "posts/T7-NHLAwards/Ballot.html#calder-memorial-trophy",
    "title": "BarrieBallots",
    "section": "Calder Memorial Trophy",
    "text": "Calder Memorial Trophy\nGiven to the player “adjudged to be the most proficient in his first year of competition.” This is essentially the rookie of the year award.\n\n\nCode\nCalder.OnIce &lt;- OnIce.Skater.Rookie%&gt;%\n  filter(GP &gt; 20)%&gt;%\n  mutate(scale.CF. = scale(CF.),\n         scale.xGF. = scale(xGF.))%&gt;%\n  select(Player, Team, Position, GP, CF., xGF., scale.CF., scale.xGF. )\nCalder.Indiv &lt;- Individual.Skater.Rookie%&gt;%\n  filter(GP &gt; 20)%&gt;%\n  mutate(scale.Goals = scale(Goals),\n         scale.Assists = scale(Total.Assists))%&gt;%\n  select(Player, Team, Position, GP, Goals, Total.Assists, scale.Goals, scale.Assists )\n\nCalder &lt;- Calder.OnIce %&gt;%\n  left_join(Calder.Indiv, by = \"Player\") %&gt;%\n  mutate(scale.Calder = scale.CF. + scale.xGF. + scale.Goals + scale.Assists) %&gt;%\n  mutate(Position.x = if_else(Position.x == \"D\", \"D\", \"F\"))\n\nCalder.Plot &lt;- Calder %&gt;%\n  pivot_longer(cols = c(scale.Goals, scale.Assists, scale.CF., scale.xGF.), \n               names_to = \"component\", \n               values_to = \"value\") %&gt;%\n  filter(str_detect(component, \"scale\"))\n\n# Step 2: Create the plot\nggplot(Calder.Plot%&gt;% filter(scale.Calder &gt; 3), \n                   aes(x = reorder(Player, scale.Calder), y = value, fill = component)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"scale.Goals\" = \"#1F77B4\", \n               \"scale.Assists\" = \"#D62728\",\n               \"scale.CF.\" = \"#FF7F0E\",\n               \"scale.xGF.\" = \"black\")\n  ) +\n  \n  labs(x = \"Player Name\",\n       y = \"Calder Score Components\",\n       fill = \"Component\",\n       caption = \"Source: https://www.naturalstattrick.com/\",\n       title = \"Top Players - Calder Score Components\",\n       subtitle = \"2024-2025 season stats as of March 4\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    # Improve readability of player names\n    axis.text.y = element_text(size = 8)\n  )+\n  facet_wrap(~Position.x)"
  },
  {
    "objectID": "posts/T7-NHLAwards/Ballot.html#frank-j.-selke-trophy",
    "href": "posts/T7-NHLAwards/Ballot.html#frank-j.-selke-trophy",
    "title": "BarrieBallots",
    "section": "Frank J. Selke Trophy",
    "text": "Frank J. Selke Trophy\nAwarded to the forward who best excels in the defensive aspects of the game.\n\n\nCode\nSelke.OnIce &lt;- OnIce.Skater%&gt;%\n  filter(Position != \"D\", TOI &gt; 1000)%&gt;%\n  mutate(scale.CA = scale(CA/TOI),\n         scale.GA = scale(GA/TOI),\n         scale.SA = scale(SA/TOI),\n         scale.xGA = scale(xGA/TOI))%&gt;%\n  select(Player, Team, Position, TOI, GP, CA, xGA, GA, SA, scale.CA, scale.GA, scale.SA, scale.xGA )%&gt;%\n  mutate(Selke = scale.CA + scale.GA + scale.SA + scale.xGA)\n\n\n\n\nSelke.Plot &lt;- Selke.OnIce %&gt;%\n  pivot_longer(cols = c(scale.CA, scale.GA, scale.SA, scale.xGA), \n               names_to = \"component\", \n               values_to = \"value\") %&gt;%\n  filter(str_detect(component, \"scale\"))\n\n\n\n# Step 2: Create the plot\nggplot(Selke.Plot%&gt;% filter(Selke &lt; -2), \n                   aes(x = reorder(Player, Selke), y = value, fill = component)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"scale.CA\" = \"#1F77B4\", \n               \"scale.GA\" = \"#D62728\",\n               \"scale.SA\" = \"#FF7F0E\",\n               \"scale.xGA\" = \"black\")\n  ) +\n  \n  labs(x = \"Player Name\",\n       y = \"Selke Score Components\",\n       fill = \"Component\",\n       caption = \"Source: https://www.naturalstattrick.com/\",\n       title = \"Top Players - Selke Score Components\",\n       subtitle = \"2024-2025 season stats as of March 4\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    # Improve readability of player names\n    axis.text.y = element_text(size = 8)\n  )"
  },
  {
    "objectID": "posts/T7-NHLAwards/Ballot.html#lady-byng-memorial-trophy",
    "href": "posts/T7-NHLAwards/Ballot.html#lady-byng-memorial-trophy",
    "title": "BarrieBallots",
    "section": "Lady Byng Memorial Trophy",
    "text": "Lady Byng Memorial Trophy\nPresented to the player who exhibits “the best type of sportsmanship and gentlemanly conduct combined with a high standard of playing ability.”\n\n\nCode\nByng.OnIce &lt;- OnIce.Skater%&gt;%\n  filter(TOI &gt; 1000)%&gt;%\n  group_by(Position)%&gt;%\n  mutate(scale.CF. = scale(CF.),\n         scale.xGF. = scale(xGF.))%&gt;%\n  select(Player, Team, GP, Position, CF., xGF., scale.CF., scale.xGF. )\nByng.Indiv &lt;- Individual.Skater%&gt;%\n  filter(TOI &gt; 1000)%&gt;%\n  group_by(Position)%&gt;%\n  mutate(scale.Goals = scale(Goals),\n         scale.Assists = scale(Total.Assists),\n         scale.PIM = -scale(PIM))%&gt;%\n  select(Player, Team, GP, Position, PIM, Goals, Total.Assists, scale.Goals, scale.Assists, scale.PIM )\n\nByng&lt;- Byng.OnIce%&gt;%\n  left_join(Byng.Indiv, by = \"Player\")%&gt;%\n  mutate(scale.Byng = (scale.CF. + scale.xGF. + scale.Goals + scale.Assists)/4,\n         Byng = scale.Byng + scale.PIM)\n\n\n\nByng.Plot &lt;- Byng %&gt;%\n  pivot_longer(cols = c(scale.Byng, scale.PIM), \n               names_to = \"component\", \n               values_to = \"value\") %&gt;%\n  filter(str_detect(component, \"scale\"))\n\n\n\n# Step 2: Create the plot\nggplot(Byng%&gt;% filter(Byng &gt; 0.5, scale.PIM &gt; .7)%&gt;%\n         mutate(Position = if_else(Position.x == \"D\", \"D\", \"F\")), \n                   aes(x = reorder(Player,scale.PIM), y = scale.PIM, fill = Position)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(\n    values = c(\"D\" = \"#1F77B4\", \n               \"F\" = \"#D62728\")\n  ) +\n  \n  labs(x = \"Player Name\",\n       y = \"Byng Score Components\",\n       fill = \"Component\",\n       caption = \"Source: https://www.naturalstattrick.com/\",\n       title = \"Top Players - Byng Score Components\",\n       subtitle = \"2024-2025 season stats as of March 4\") +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    # Improve readability of player names\n    axis.text.y = element_text(size = 8)\n  )+\n  facet_wrap(~Position)"
  },
  {
    "objectID": "posts/L1-Intro/index.html#who-am-i",
    "href": "posts/L1-Intro/index.html#who-am-i",
    "title": "LECTURE 1 - INTRO",
    "section": "WHO AM I?",
    "text": "WHO AM I?\nBarrie Robison\nDepartment of Biological Sciences\nInstitute for Interdisicplinary Data Sciences\nPolymorphic Games\nUniversity of Idaho"
  },
  {
    "objectID": "posts/L1-Intro/index.html#visualization",
    "href": "posts/L1-Intro/index.html#visualization",
    "title": "LECTURE 1 - INTRO",
    "section": "VISUALIZATION",
    "text": "VISUALIZATION\nComputers provide visual representations of datasets designed to help people carry out tasks more effectively.\nTamara Munzner\nDepartment of Computer Science\nDr. Munzner’s version of this lecture\nUniversity of British Columbia"
  },
  {
    "objectID": "posts/L1-Intro/index.html#the-human",
    "href": "posts/L1-Intro/index.html#the-human",
    "title": "LECTURE 1 - INTRO",
    "section": "THE HUMAN",
    "text": "THE HUMAN\nWhy have a human in the loop?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\nWe don’t need visualization when a trusted fully automatic solution exists.\nVisualization is suitable when there is a need to augment human capabilities rather than replace people with computational decision-making methods."
  },
  {
    "objectID": "posts/L1-Intro/index.html#the-representation",
    "href": "posts/L1-Intro/index.html#the-representation",
    "title": "LECTURE 1 - INTRO",
    "section": "THE REPRESENTATION",
    "text": "THE REPRESENTATION\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\nEXTERNAL REPRESENTATIONS: Replace cognition with perception."
  },
  {
    "objectID": "posts/L1-Intro/index.html#in-class-exercise",
    "href": "posts/L1-Intro/index.html#in-class-exercise",
    "title": "LECTURE 1 - INTRO",
    "section": "IN CLASS EXERCISE",
    "text": "IN CLASS EXERCISE\nYou have 5 minutes.\nFind an example of a data visualization from your daily life. I may not be related to your research or discipline in any way. Be prepared to share it with the class (zoom screen share or email me a link) and discuss its purpose, efficacy, strengths, and weaknesses."
  },
  {
    "objectID": "posts/L1-Intro/index.html#why-depend-on-vision",
    "href": "posts/L1-Intro/index.html#why-depend-on-vision",
    "title": "LECTURE 1 - INTRO",
    "section": "WHY DEPEND ON VISION?",
    "text": "WHY DEPEND ON VISION?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\nThe human visual system is a high-bandwidth channel to the brain.\nOverview is possible due to background processing, providing the subjective experience of seeing everything simultaneously.\nSignificant processing occurs in parallel and pre-attentively.\nWhat about sound? lower bandwidth and different semantics, overview not supported, subjective experience of sequential stream.\nWhat about touch/haptics? impoverished record/replay capacity, only very low-bandwidth communication thus far.\nWhat about taste, smell? no viable record/replay devices."
  },
  {
    "objectID": "posts/L1-Intro/index.html#why-represent-all-the-data",
    "href": "posts/L1-Intro/index.html#why-represent-all-the-data",
    "title": "LECTURE 1 - INTRO",
    "section": "WHY REPRESENT (ALL THE) DATA?",
    "text": "WHY REPRESENT (ALL THE) DATA?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\nsummaries lose information\ndetails matter\nconfirm expected and find unexpected patterns\nassess validity of statistical model\nANSCOMBE’S QUARTET is a fun example that we shall use to illustrate these points!"
  },
  {
    "objectID": "posts/L1-Intro/index.html#anscombes-quartet",
    "href": "posts/L1-Intro/index.html#anscombes-quartet",
    "title": "LECTURE 1 - INTRO",
    "section": "ANSCOMBE’S QUARTET",
    "text": "ANSCOMBE’S QUARTET\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(datasets)\nlibrary(tidyverse)\nlibrary(dplyr)\ndatasets::anscombe\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\n\nAnscombe’s Quartet\nThe four x-y pairs have identical summary statistics.\n\n\nCode\ntidy_anscombe &lt;- anscombe %&gt;%\n pivot_longer(cols = everything(),\n              names_to = c(\".value\", \"set\"),\n              names_pattern = \"(.)(.)\")\ntidy_anscombe_summary &lt;- tidy_anscombe %&gt;%\n  group_by(set) %&gt;%\n  summarise(across(.cols = everything(),\n                   .fns = lst(min,max,median,mean,sd,var),\n                   .names = \"{col}_{fn}\"))\n#&gt; `summarise()` ungrouping output (override with `.groups` argument)\n\nvars&lt;-c(\"set\", \"x_mean\", \"x_var\",  \"y_mean\", \"y_var\")\nthing&lt;- as.data.frame(tidy_anscombe_summary[vars])\nknitr::kable(thing)\n\n\n\n\n\nset\nx_mean\nx_var\ny_mean\ny_var\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249"
  },
  {
    "objectID": "posts/L1-Intro/index.html#viz-matters",
    "href": "posts/L1-Intro/index.html#viz-matters",
    "title": "LECTURE 1 - INTRO",
    "section": "VIZ MATTERS",
    "text": "VIZ MATTERS\n\n\nCode\nggplot(tidy_anscombe,\n       aes(x = x,\n           y = y)) +\n  geom_point() +\n  geom_point(data = tidy_anscombe_summary, aes(x=x_mean, y = y_mean, color = \"red\", size = 5),\n             show.legend = FALSE)+\n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nLearn more: TIDY ANSCOMBE"
  },
  {
    "objectID": "posts/L1-Intro/index.html#resource-limitations",
    "href": "posts/L1-Intro/index.html#resource-limitations",
    "title": "LECTURE 1 - INTRO",
    "section": "RESOURCE LIMITATIONS",
    "text": "RESOURCE LIMITATIONS\nVisualization designers must take into account three very different kinds of resource limitations:\n\nLimitations of computers.\nLimitations of humans.\nLimitations of displays."
  },
  {
    "objectID": "posts/L1-Intro/index.html#computational-limits",
    "href": "posts/L1-Intro/index.html#computational-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "COMPUTATIONAL LIMITS",
    "text": "COMPUTATIONAL LIMITS\nCPU time\nSystem Memory"
  },
  {
    "objectID": "posts/L1-Intro/index.html#display-limits",
    "href": "posts/L1-Intro/index.html#display-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "DISPLAY LIMITS",
    "text": "DISPLAY LIMITS\nPixels are precious and are the most constrained resource.\n\nInformation Density: ratio of space used to encode information vs unused whitespace.\nThere is a tradeoff between clutter and wasting space.\nDesigner must find the sweet spot between dense and sparse."
  },
  {
    "objectID": "posts/L1-Intro/index.html#human-limits",
    "href": "posts/L1-Intro/index.html#human-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "HUMAN LIMITS",
    "text": "HUMAN LIMITS\n\nTime\nMemory\nAttention\n\n\n\n\n\nHOME"
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html",
    "href": "posts/A1-Lit-Prog/index.html",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "",
    "text": "The idea of Literate Programming is that source code that is executed as part of the program’s purpose is interspersed with documentation that describes the program’s logic. The concept of literate programming was first articulated by David Knuth in 1984. You know… back when music was good? Modern Data Science leans pretty heavily on literate programming, and to be honest, there aren’t very many good arguments as to why you WOULDN’T want to implement this approach in your own work. Bearing this in mind, we will adopt this framework for most of the activities, exercises, and assignments in this course. All of us will benefit by practicing these skills."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#summary",
    "href": "posts/A1-Lit-Prog/index.html#summary",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "",
    "text": "The idea of Literate Programming is that source code that is executed as part of the program’s purpose is interspersed with documentation that describes the program’s logic. The concept of literate programming was first articulated by David Knuth in 1984. You know… back when music was good? Modern Data Science leans pretty heavily on literate programming, and to be honest, there aren’t very many good arguments as to why you WOULDN’T want to implement this approach in your own work. Bearing this in mind, we will adopt this framework for most of the activities, exercises, and assignments in this course. All of us will benefit by practicing these skills."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "href": "posts/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LITERATE PROGRAMMING PUBLISHING SYSTEMS",
    "text": "LITERATE PROGRAMMING PUBLISHING SYSTEMS\nI’m trying to keep this course as technology agnostic as I can. The idea is that you should be practicing and building competencies in the languages and algorithms that are most useful to you. Who am I to tell you to use R instead of Python? If you have skills in a particular language I encourage you to keep using that during this course. That being said, I am going to work the examples using R and R Studio, and I will (mostly) use Quarto as the literate programming framework.\nIf all of this is new to you, no problem. Just follow along in R and Quarto and start your skill building journey with those languages.\nIf you are a Python person, great! Quarto can accommodate that language as well. If you have another preference for literate programming, such as sticking with R Markdown until the Quarto bugs are fixed, that is great. Find the framework and tools that work for you, and practice, practice, practice!\n\nQuarto\nAn open source publishing system that allows you to create websites, documents, blogs, books, publications, presentations, and more while using R, Python, Julia, or Observable. Quarto is intended to be the more functional successor of R Markdown. I intend to use Quarto for most of my work in this course.\n\n\nR Markdown\nAnother publishing system for creating all the things … websites, slides, manuscripts, dashboards, etc. While most people (including me!) instinctively think of R and Python within R Markdown, the list of supported language engines is pretty extensive.\n\nnames(knitr::knit_engines$get())\n\n [1] \"awk\"       \"bash\"      \"coffee\"    \"gawk\"      \"groovy\"    \"haskell\"  \n [7] \"lein\"      \"mysql\"     \"node\"      \"octave\"    \"perl\"      \"php\"      \n[13] \"psql\"      \"Rscript\"   \"ruby\"      \"sas\"       \"scala\"     \"sed\"      \n[19] \"sh\"        \"stata\"     \"zsh\"       \"asis\"      \"asy\"       \"block\"    \n[25] \"block2\"    \"bslib\"     \"c\"         \"cat\"       \"cc\"        \"comment\"  \n[31] \"css\"       \"ditaa\"     \"dot\"       \"embed\"     \"eviews\"    \"exec\"     \n[37] \"fortran\"   \"fortran95\" \"go\"        \"highlight\" \"js\"        \"julia\"    \n[43] \"python\"    \"R\"         \"Rcpp\"      \"sass\"      \"scss\"      \"sql\"      \n[49] \"stan\"      \"targets\"   \"tikz\"      \"verbatim\"  \"ojs\"       \"mermaid\""
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#languages-and-toolsets",
    "href": "posts/A1-Lit-Prog/index.html#languages-and-toolsets",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LANGUAGES AND TOOLSETS",
    "text": "LANGUAGES AND TOOLSETS\nThere are quite a few, but the five that seemed to keep coming up as I prepped this course are:\n\nR\nA very powerful open source framework for statistical computing and graphics. R has a lot of base functionality, and its capabilities are increased by 100 fold with packages created by R users. Packages are the core units of R code. I’m going to use R for the vast majority of demonstrations in this course.\n\n\nPython\nPython is an open source general purpose programming language. It wasn’t developed just for statistical computing or data science, and people use this language for tons of different applications. There is no denying it has become a very powerful language for data science and data visualization.\n\n\nTableau\nTableau is proprietary software that is very powerful for creating beautiful and functional data visualizations. It can integrate with all sorts of data sources and is used a lot for analytics, especially in the business world. The downsides (that occur to me at least) are that it costs money, it is not open source, and is more of a one-trick-pony than the programming languages on this list.\n\n\nJavascript\nJavascript has been around for about 25 years, and is (I think) the world’s most popular programming language. Along with HTML and CSS, Javascript drives pretty much the entire internet. I mention Javascript here because it has the D3 library, which can create super cool interactive data visualizaitons. In my experience, the learning curve with Javascript and D3 was pretty steep. I bought a book about it once, but just haven’t been able to allocate the amount of time necessary to really start using it. Check out the gallery of examples. Amazing!\n\n\nObservable / D3\nObservable is a set of extensions to Javascript that features something called reactive runtime. This means that the code blocks are executed and compiled as they are written, and changes are implemented instantaneously. Observable is pretty great for data exploration, and is well supported by Quarto. In addition, you can use the Observable JS libraries in Quarto to access D3. We’ll use some of these tools in this course, especially when we start considering interactivity."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#assignment",
    "href": "posts/A1-Lit-Prog/index.html#assignment",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nAfter that long introduction, I suppose you are wondering what I want you to actually DO.\nWell, I want you to set up your publishing system and preferred language on your computer. Then I want you to recreate the classic figure from Anscombe’s Quartet.\nNow, you might be asking…\n“How am I supposed to do that? You haven’t taught me how to do anything yet!”\nHere is the dirty little secret of modern education.\nThe Internet Exists.\nWhile I could use up an entire 90 minute lecture telling you how to:\n\nDownload and install R, R-Studio, and Quarto (included by default with R-Studio).\nCreate a Quarto document that will publish in the .html format\nInstall the R packages you will need\nTidy up the Anscombe’s Quartet data\nCalculate the summary statistics for each x y pair\nMake a nice little plot…\n\nI’m not going to do that.\nInstead, I want you to use the resources I point towards, or other resources that make more sense to you, to figure out how to do those things."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#resources",
    "href": "posts/A1-Lit-Prog/index.html#resources",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "RESOURCES",
    "text": "RESOURCES\nTidyverse and Anscombe’s Quartet\nHandy cheat-sheets for many different R packages\nTutorial 1 - Literate Programming\nTutorial 2 - Literate Programming and Anscombe’s Quartet\nTutorial 3 - Python"
  },
  {
    "objectID": "posts/A9-NetworksinObservable/index.html",
    "href": "posts/A9-NetworksinObservable/index.html",
    "title": "NETWORKS IN OBSERVABLE",
    "section": "",
    "text": "The QUARTO documentation on Observable can be found here.\nThe preamble of that document summarizes things nicely:\n\nQuarto includes native support for Observable JS, a set of enhancements to vanilla JavaScript created by Mike Bostock (also the author of D3). Observable JS is distinguished by its reactive runtime, which is especially well suited for interactive data exploration and analysis.\nThe creators of Observable JS (Observable, Inc.) run a hosted service at https://observablehq.com/ where you can create and publish notebooks. Additionally, you can use Observable JS (“OJS”) in standalone documents and websites via its core libraries. Quarto uses these libraries along with a compiler that is run at render time to enable the use of OJS within Quarto documents.\nOJS works in any Quarto document (plain markdown as well as Jupyter and Knitr documents). Just include your code in an {ojs} executable code block.\n\n\n\nI’m going to start by trying to replicate this observable notebook:\n\n\nCode\nviewof graph = {\n  const form = html`&lt;form style=\"font: 12px var(--sans-serif); display: flex; height: 33px; align-items: center;\"&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"1\" style=\"margin-right: 0.5em;\" checked&gt; Graph 1\n  &lt;/label&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"2\" style=\"margin-right: 0.5em;\"&gt; Graph 2\n  &lt;/label&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"3\" style=\"margin-right: 0.5em;\"&gt; Graph 3\n  &lt;/label&gt;\n&lt;/form&gt;`;\n  const graphs = {1: graph1, 2: graph2, 3: graph3};\n  const timeout = setInterval(() =&gt; {\n    form.value = graphs[form.radio.value = (+form.radio.value) % 3 + 1];\n    form.dispatchEvent(new CustomEvent(\"input\"));\n  }, 2000);\n  form.onchange = () =&gt; form.dispatchEvent(new CustomEvent(\"input\")); // Safari\n  form.oninput = event =&gt; { \n    if (event.isTrusted) clearInterval(timeout), form.onchange = null;\n    form.value = graphs[form.radio.value];\n  };\n  form.value = graphs[form.radio.value];\n  invalidation.then(() =&gt; clearInterval(timeout));\n  return form;\n}\n\n\n\n\nchart2 = {\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height]);\n\n  const simulation = d3.forceSimulation()\n      .force(\"charge\", d3.forceManyBody().strength(-1000))\n      .force(\"link\", d3.forceLink().id(d =&gt; d.id).distance(200))\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  let link = svg.append(\"g\")\n      .attr(\"stroke\", \"#000\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"line\");\n\n  let node = svg.append(\"g\")\n      .attr(\"stroke\", \"#fff\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"circle\");\n\n  function ticked() {\n    node.attr(\"cx\", d =&gt; d.x)\n        .attr(\"cy\", d =&gt; d.y)\n\n    link.attr(\"x1\", d =&gt; d.source.x)\n        .attr(\"y1\", d =&gt; d.source.y)\n        .attr(\"x2\", d =&gt; d.target.x)\n        .attr(\"y2\", d =&gt; d.target.y);\n  }\n\n  // Terminate the force layout when this cell re-runs.\n  invalidation.then(() =&gt; simulation.stop());\n\n  return Object.assign(svg.node(), {\n    update({nodes, links}) {\n\n      // Make a shallow copy to protect against mutation, while\n      // recycling old nodes to preserve position and velocity.\n      const old = new Map(node.data().map(d =&gt; [d.id, d]));\n      nodes = nodes.map(d =&gt; Object.assign(old.get(d.id) || {}, d));\n      links = links.map(d =&gt; Object.assign({}, d));\n\n      simulation.nodes(nodes);\n      simulation.force(\"link\").links(links);\n      simulation.alpha(1).restart();\n\n      node = node\n        .data(nodes, d =&gt; d.id)\n        .join(enter =&gt; enter.append(\"circle\")\n          .attr(\"r\", 8)\n          .attr(\"fill\", d =&gt; color(d.id)));\n\n      link = link\n        .data(links, d =&gt; `${d.source.id}\\t${d.target.id}`)\n        .join(\"line\");\n    }\n  });\n}\n\n\nupdate = chart2.update(graph)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows us how Dr. Bostock creates the basic architecture of an EDGE LIST called links along with NODE LABELS called nodes. Have a close look at the structure. He is setting this up in a heirarchy very similar to a JSON file, which we will examine in EXAMPLE 2.\n\n\nCode\ngraph1 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: []\n})\n\n\ngraph2 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: [\n    {source: \"a\", target: \"b\"},\n    {source: \"b\", target: \"c\"},\n    {source: \"c\", target: \"a\"}\n  ]\n})\n\n\ngraph3 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"}\n  ],\n  links: [\n    {source: \"a\", target: \"b\"}\n  ]\n})\n\n\ncolor = d3.scaleOrdinal(d3.schemeTableau10)\n\nheight = 400\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the graph3 object so that it contains eight nodes called “Life Sciences”, “Physical Sciences”, “Heidi”, “Robyn”, “Konrad”, “Geraline”, “Lucas”, and “Yaotian”. Change the links object to reflect our shared understanding of those links.\n\n\n\nI honestly cannot believe this works! I mean… Hey! Look at this cool interactive network!\n\n\nCode\nchart = ForceGraph(miserables, {\n  nodeId: d =&gt; d.id,\n  nodeGroup: d =&gt; d.group,\n  nodeTitle: d =&gt; `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l =&gt; Math.sqrt(l.value),\n  width,\n  height: 600,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\n\n\n\n\n\n\n\nThe first line of code in the chunk below defines the data object from a .json file called miserables.json. Have a look at this file within RStudio. Does the overall structure look familiar?\nCould we possibly replace the stupid data file about a stupid musical with something of our own design???\n\n\nCode\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d =&gt; d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) =&gt; source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) =&gt; target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) =&gt; N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) =&gt; ({id: N[i]}));\n  links = d3.map(links, (_, i) =&gt; ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) =&gt; N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) =&gt; W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) =&gt; L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) =&gt; color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) =&gt; T[i]);\n  if (invalidation != null) invalidation.then(() =&gt; simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d =&gt; d.source.x)\n      .attr(\"y1\", d =&gt; d.source.y)\n      .attr(\"x2\", d =&gt; d.target.x)\n      .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if we replaced the datafile by making our own json file??\n\n\nCode\nlibrary(jsonlite)\n\n# create data frames for nodes and links\nnodes &lt;- data.frame(\n  id = c(\"Barrie\", \"Ronald\", \"Cody\", \"Erick\", \"Jiyin\", \"Cthulhu\"),\n  group = c(1, 1, 1 , 2, 2, 3)\n)\n\nlinks &lt;- data.frame(\n  source = c(\"Barrie\", \"Ronald\", \"Cody\", \"Barrie\", \"Erick\", \"Jiyin\", \"Ronald\"),\n  target = c(\"Cthulhu\", \"Erick\", \"Jiyin\", \"Erick\", \"Cthulhu\", \"Ronald\", \"Cody\"),\n  value = c(1, 8, 10, 6, 1, 1, 1)\n)\n\n# convert data frames to JSON objects\nnodes_json &lt;- toJSON(list(nodes = nodes), pretty = TRUE)\nlinks_json &lt;- toJSON(list(links = links), pretty = TRUE)\n\n# merge JSON objects into one\njson &lt;- paste0( nodes_json, links_json)\n\n# write JSON object to file\nwrite(json, file = \"network_graph2.json\")\n\n\nOh god…. now go back and point the stuff to the stuff…\nAnyway…. here is where I want to go:\nAMAZING"
  },
  {
    "objectID": "posts/A9-NetworksinObservable/index.html#example-1-basic-force-directed-graph",
    "href": "posts/A9-NetworksinObservable/index.html#example-1-basic-force-directed-graph",
    "title": "NETWORKS IN OBSERVABLE",
    "section": "",
    "text": "I’m going to start by trying to replicate this observable notebook:\n\n\nCode\nviewof graph = {\n  const form = html`&lt;form style=\"font: 12px var(--sans-serif); display: flex; height: 33px; align-items: center;\"&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"1\" style=\"margin-right: 0.5em;\" checked&gt; Graph 1\n  &lt;/label&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"2\" style=\"margin-right: 0.5em;\"&gt; Graph 2\n  &lt;/label&gt;\n  &lt;label style=\"margin-right: 1em; display: inline-flex; align-items: center;\"&gt;\n    &lt;input type=\"radio\" name=\"radio\" value=\"3\" style=\"margin-right: 0.5em;\"&gt; Graph 3\n  &lt;/label&gt;\n&lt;/form&gt;`;\n  const graphs = {1: graph1, 2: graph2, 3: graph3};\n  const timeout = setInterval(() =&gt; {\n    form.value = graphs[form.radio.value = (+form.radio.value) % 3 + 1];\n    form.dispatchEvent(new CustomEvent(\"input\"));\n  }, 2000);\n  form.onchange = () =&gt; form.dispatchEvent(new CustomEvent(\"input\")); // Safari\n  form.oninput = event =&gt; { \n    if (event.isTrusted) clearInterval(timeout), form.onchange = null;\n    form.value = graphs[form.radio.value];\n  };\n  form.value = graphs[form.radio.value];\n  invalidation.then(() =&gt; clearInterval(timeout));\n  return form;\n}\n\n\n\n\nchart2 = {\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height]);\n\n  const simulation = d3.forceSimulation()\n      .force(\"charge\", d3.forceManyBody().strength(-1000))\n      .force(\"link\", d3.forceLink().id(d =&gt; d.id).distance(200))\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  let link = svg.append(\"g\")\n      .attr(\"stroke\", \"#000\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"line\");\n\n  let node = svg.append(\"g\")\n      .attr(\"stroke\", \"#fff\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"circle\");\n\n  function ticked() {\n    node.attr(\"cx\", d =&gt; d.x)\n        .attr(\"cy\", d =&gt; d.y)\n\n    link.attr(\"x1\", d =&gt; d.source.x)\n        .attr(\"y1\", d =&gt; d.source.y)\n        .attr(\"x2\", d =&gt; d.target.x)\n        .attr(\"y2\", d =&gt; d.target.y);\n  }\n\n  // Terminate the force layout when this cell re-runs.\n  invalidation.then(() =&gt; simulation.stop());\n\n  return Object.assign(svg.node(), {\n    update({nodes, links}) {\n\n      // Make a shallow copy to protect against mutation, while\n      // recycling old nodes to preserve position and velocity.\n      const old = new Map(node.data().map(d =&gt; [d.id, d]));\n      nodes = nodes.map(d =&gt; Object.assign(old.get(d.id) || {}, d));\n      links = links.map(d =&gt; Object.assign({}, d));\n\n      simulation.nodes(nodes);\n      simulation.force(\"link\").links(links);\n      simulation.alpha(1).restart();\n\n      node = node\n        .data(nodes, d =&gt; d.id)\n        .join(enter =&gt; enter.append(\"circle\")\n          .attr(\"r\", 8)\n          .attr(\"fill\", d =&gt; color(d.id)));\n\n      link = link\n        .data(links, d =&gt; `${d.source.id}\\t${d.target.id}`)\n        .join(\"line\");\n    }\n  });\n}\n\n\nupdate = chart2.update(graph)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows us how Dr. Bostock creates the basic architecture of an EDGE LIST called links along with NODE LABELS called nodes. Have a close look at the structure. He is setting this up in a heirarchy very similar to a JSON file, which we will examine in EXAMPLE 2.\n\n\nCode\ngraph1 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: []\n})\n\n\ngraph2 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: [\n    {source: \"a\", target: \"b\"},\n    {source: \"b\", target: \"c\"},\n    {source: \"c\", target: \"a\"}\n  ]\n})\n\n\ngraph3 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"}\n  ],\n  links: [\n    {source: \"a\", target: \"b\"}\n  ]\n})\n\n\ncolor = d3.scaleOrdinal(d3.schemeTableau10)\n\nheight = 400"
  },
  {
    "objectID": "posts/A9-NetworksinObservable/index.html#task-1",
    "href": "posts/A9-NetworksinObservable/index.html#task-1",
    "title": "NETWORKS IN OBSERVABLE",
    "section": "",
    "text": "Change the graph3 object so that it contains eight nodes called “Life Sciences”, “Physical Sciences”, “Heidi”, “Robyn”, “Konrad”, “Geraline”, “Lucas”, and “Yaotian”. Change the links object to reflect our shared understanding of those links."
  },
  {
    "objectID": "posts/A9-NetworksinObservable/index.html#example-2-interactive-force-directed-graph",
    "href": "posts/A9-NetworksinObservable/index.html#example-2-interactive-force-directed-graph",
    "title": "NETWORKS IN OBSERVABLE",
    "section": "",
    "text": "I honestly cannot believe this works! I mean… Hey! Look at this cool interactive network!\n\n\nCode\nchart = ForceGraph(miserables, {\n  nodeId: d =&gt; d.id,\n  nodeGroup: d =&gt; d.group,\n  nodeTitle: d =&gt; `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l =&gt; Math.sqrt(l.value),\n  width,\n  height: 600,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\n\n\n\n\n\n\n\nThe first line of code in the chunk below defines the data object from a .json file called miserables.json. Have a look at this file within RStudio. Does the overall structure look familiar?\nCould we possibly replace the stupid data file about a stupid musical with something of our own design???\n\n\nCode\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d =&gt; d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) =&gt; source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) =&gt; target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) =&gt; N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) =&gt; ({id: N[i]}));\n  links = d3.map(links, (_, i) =&gt; ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) =&gt; N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) =&gt; W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) =&gt; L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) =&gt; color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) =&gt; T[i]);\n  if (invalidation != null) invalidation.then(() =&gt; simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d =&gt; d.source.x)\n      .attr(\"y1\", d =&gt; d.source.y)\n      .attr(\"x2\", d =&gt; d.target.x)\n      .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\""
  },
  {
    "objectID": "posts/A9-NetworksinObservable/index.html#task-2",
    "href": "posts/A9-NetworksinObservable/index.html#task-2",
    "title": "NETWORKS IN OBSERVABLE",
    "section": "",
    "text": "What if we replaced the datafile by making our own json file??\n\n\nCode\nlibrary(jsonlite)\n\n# create data frames for nodes and links\nnodes &lt;- data.frame(\n  id = c(\"Barrie\", \"Ronald\", \"Cody\", \"Erick\", \"Jiyin\", \"Cthulhu\"),\n  group = c(1, 1, 1 , 2, 2, 3)\n)\n\nlinks &lt;- data.frame(\n  source = c(\"Barrie\", \"Ronald\", \"Cody\", \"Barrie\", \"Erick\", \"Jiyin\", \"Ronald\"),\n  target = c(\"Cthulhu\", \"Erick\", \"Jiyin\", \"Erick\", \"Cthulhu\", \"Ronald\", \"Cody\"),\n  value = c(1, 8, 10, 6, 1, 1, 1)\n)\n\n# convert data frames to JSON objects\nnodes_json &lt;- toJSON(list(nodes = nodes), pretty = TRUE)\nlinks_json &lt;- toJSON(list(links = links), pretty = TRUE)\n\n# merge JSON objects into one\njson &lt;- paste0( nodes_json, links_json)\n\n# write JSON object to file\nwrite(json, file = \"network_graph2.json\")\n\n\nOh god…. now go back and point the stuff to the stuff…\nAnyway…. here is where I want to go:\nAMAZING"
  },
  {
    "objectID": "posts/AIisfun/AIisstuff.html",
    "href": "posts/AIisfun/AIisstuff.html",
    "title": "AI Code Test",
    "section": "",
    "text": "This post is a brief example of how AI tools such as GPT-4o or Claude can accelerate your programming.\n\n\nOur example in class was this prompt:\n\nI want to use the tidyverse to create a parralel axis plot of microbiome data. I have 10 microbial species that will be the axes and a want to plot the abundance for 58 different samples. the samples are equally divided between normal baby poop and baby poop from sick babies.\n\nI put that prompt into AnythingLLM (pointed at the OpenAI API) and after a brief interaction:\n\nAda says you suck because of this error Error in [.data.frame(data, , columns) : undefined columns selected\n\nWe got this response and this code:\nThe error you’re encountering is due to the columns argument in ggparcoord, which specifies the numeric indices of the columns to be plotted. We need to adjust those indices based on the actual column positions in your dataset. Let’s refine the code.\nMake sure the column indices in the columns argument match the positions of your microbial species in the dataset and that groupColumn correctly refers to the column with the grouping variable (Condition).\nHere’s the corrected code:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nCode\n# Example microbiome data, replace this with your actual dataset\nset.seed(123)  # For reproducibility\ndata &lt;- tibble(\n  SampleID = 1:58,\n  Condition = rep(c(\"Normal\", \"Sick\"), each = 29),\n  Species1 = runif(58, 5, 15),\n  Species2 = runif(58, 10, 20),\n  Species3 = runif(58, 0, 5),\n  Species4 = runif(58, 3, 8),\n  Species5 = runif(58, 1, 6),\n  Species6 = runif(58, 2, 7),\n  Species7 = runif(58, 4, 9),\n  Species8 = runif(58, 5, 10),\n  Species9 = runif(58, 6, 12),\n  Species10 = runif(58, 0, 8)\n)\n\n# Create parallel coordinates plot\nggparcoord(data, columns = 3:12, groupColumn = 2, scale = \"globalminmax\") +\n  labs(title = \"Parallel Coordinates Plot of Microbial Abundance\",\n       x = \"Microbial Species\",\n       y = \"Abundance\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Normal\" = \"blue\", \"Sick\" = \"red\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nIn this code, the columns argument is set to 3:12 to refer to the species columns, assuming these are from the 3rd to the 12th columns in your dataset. Adjust these indices if your data structure differs. The groupColumn is set to 2, assuming the Condition column is the second column. Please ensure your actual dataset aligns with these assumptions.\n\n\n\nThen we decided to change programming languages entirely!\n\nI’ve changed my mind. I want to use Python instead.\n\nI’ll spare you the brief iterations, but we ended up with this:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Example microbiome data\nnp.random.seed(123)  # For reproducibility\ndata = pd.DataFrame({\n    'SampleID': range(1, 59),\n    'Condition': ['Normal'] * 29 + ['Sick'] * 29,\n    'Species1': np.random.uniform(5, 15, 58),\n    'Species2': np.random.uniform(10, 20, 58),\n    'Species3': np.random.uniform(0, 5, 58),\n    'Species4': np.random.uniform(3, 8, 58),\n    'Species5': np.random.uniform(1, 6, 58),\n    'Species6': np.random.uniform(2, 7, 58),\n    'Species7': np.random.uniform(4, 9, 58),\n    'Species8': np.random.uniform(5, 10, 58),\n    'Species9': np.random.uniform(6, 12, 58),\n    'Species10': np.random.uniform(0, 8, 58)\n})\n\n# Define colors for each condition\ncolor_map = {'Normal': 'rgba(0, 0, 255, 0.3)', 'Sick': 'rgba(255, 0, 0, 0.3)'}\ncolors = [color_map[condition] for condition in data['Condition']]\n\n# Get the column names for species\nspecies_cols = [col for col in data.columns if col.startswith('Species')]\n\n# Create the parallel coordinates plot using graph_objects\ndimensions = [\n    dict(range=[data[col].min(), data[col].max()],\n         label=f'Species {i+1}',\n         values=data[col])\n    for i, col in enumerate(species_cols)\n]\n\nfig = go.Figure(data=\n    go.Parcoords(\n        line=dict(color=data['Condition'].astype('category').cat.codes, \n                 colorscale=[[0, 'blue'], [1, 'red']],\n                 showscale=True,\n                 colorbar=dict(title='Condition'),\n                 ),\n        dimensions=dimensions\n    )\n)\n\nfig.update_layout(\n    title='Parallel Coordinates Plot of Microbial Abundance',\n    font=dict(size=12)\n)\n\n\n                        \n                                            \n\n\nCode\n\n# fig.show()\n\n\nWhat do you think?"
  },
  {
    "objectID": "posts/AIisfun/AIisstuff.html#a-zero-shot-example",
    "href": "posts/AIisfun/AIisstuff.html#a-zero-shot-example",
    "title": "AI Code Test",
    "section": "",
    "text": "Our example in class was this prompt:\n\nI want to use the tidyverse to create a parralel axis plot of microbiome data. I have 10 microbial species that will be the axes and a want to plot the abundance for 58 different samples. the samples are equally divided between normal baby poop and baby poop from sick babies.\n\nI put that prompt into AnythingLLM (pointed at the OpenAI API) and after a brief interaction:\n\nAda says you suck because of this error Error in [.data.frame(data, , columns) : undefined columns selected\n\nWe got this response and this code:\nThe error you’re encountering is due to the columns argument in ggparcoord, which specifies the numeric indices of the columns to be plotted. We need to adjust those indices based on the actual column positions in your dataset. Let’s refine the code.\nMake sure the column indices in the columns argument match the positions of your microbial species in the dataset and that groupColumn correctly refers to the column with the grouping variable (Condition).\nHere’s the corrected code:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nCode\n# Example microbiome data, replace this with your actual dataset\nset.seed(123)  # For reproducibility\ndata &lt;- tibble(\n  SampleID = 1:58,\n  Condition = rep(c(\"Normal\", \"Sick\"), each = 29),\n  Species1 = runif(58, 5, 15),\n  Species2 = runif(58, 10, 20),\n  Species3 = runif(58, 0, 5),\n  Species4 = runif(58, 3, 8),\n  Species5 = runif(58, 1, 6),\n  Species6 = runif(58, 2, 7),\n  Species7 = runif(58, 4, 9),\n  Species8 = runif(58, 5, 10),\n  Species9 = runif(58, 6, 12),\n  Species10 = runif(58, 0, 8)\n)\n\n# Create parallel coordinates plot\nggparcoord(data, columns = 3:12, groupColumn = 2, scale = \"globalminmax\") +\n  labs(title = \"Parallel Coordinates Plot of Microbial Abundance\",\n       x = \"Microbial Species\",\n       y = \"Abundance\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Normal\" = \"blue\", \"Sick\" = \"red\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nIn this code, the columns argument is set to 3:12 to refer to the species columns, assuming these are from the 3rd to the 12th columns in your dataset. Adjust these indices if your data structure differs. The groupColumn is set to 2, assuming the Condition column is the second column. Please ensure your actual dataset aligns with these assumptions."
  },
  {
    "objectID": "posts/AIisfun/AIisstuff.html#what-about-python",
    "href": "posts/AIisfun/AIisstuff.html#what-about-python",
    "title": "AI Code Test",
    "section": "",
    "text": "Then we decided to change programming languages entirely!\n\nI’ve changed my mind. I want to use Python instead.\n\nI’ll spare you the brief iterations, but we ended up with this:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Example microbiome data\nnp.random.seed(123)  # For reproducibility\ndata = pd.DataFrame({\n    'SampleID': range(1, 59),\n    'Condition': ['Normal'] * 29 + ['Sick'] * 29,\n    'Species1': np.random.uniform(5, 15, 58),\n    'Species2': np.random.uniform(10, 20, 58),\n    'Species3': np.random.uniform(0, 5, 58),\n    'Species4': np.random.uniform(3, 8, 58),\n    'Species5': np.random.uniform(1, 6, 58),\n    'Species6': np.random.uniform(2, 7, 58),\n    'Species7': np.random.uniform(4, 9, 58),\n    'Species8': np.random.uniform(5, 10, 58),\n    'Species9': np.random.uniform(6, 12, 58),\n    'Species10': np.random.uniform(0, 8, 58)\n})\n\n# Define colors for each condition\ncolor_map = {'Normal': 'rgba(0, 0, 255, 0.3)', 'Sick': 'rgba(255, 0, 0, 0.3)'}\ncolors = [color_map[condition] for condition in data['Condition']]\n\n# Get the column names for species\nspecies_cols = [col for col in data.columns if col.startswith('Species')]\n\n# Create the parallel coordinates plot using graph_objects\ndimensions = [\n    dict(range=[data[col].min(), data[col].max()],\n         label=f'Species {i+1}',\n         values=data[col])\n    for i, col in enumerate(species_cols)\n]\n\nfig = go.Figure(data=\n    go.Parcoords(\n        line=dict(color=data['Condition'].astype('category').cat.codes, \n                 colorscale=[[0, 'blue'], [1, 'red']],\n                 showscale=True,\n                 colorbar=dict(title='Condition'),\n                 ),\n        dimensions=dimensions\n    )\n)\n\nfig.update_layout(\n    title='Parallel Coordinates Plot of Microbial Abundance',\n    font=dict(size=12)\n)\n\n\n                        \n                                            \n\n\nCode\n\n# fig.show()\n\n\nWhat do you think?"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html",
    "href": "posts/A3-PrototypeVizPortfolio/index.html",
    "title": "ASSIGNMENT 3",
    "section": "",
    "text": "Enough with the theory and conceptual mumbo jumbo! Let’s get down to making a visualization and posting it somewhere for all the world to see! The basic idea of this assignment is to set up a repository that will serve as an experimental portfolio, and then create your first novel visualization element inside the portfolio. Its gonna be fun! Or at least educational…\nRemember that time that Barrie was dumb?"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#summary",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#summary",
    "title": "ASSIGNMENT 3",
    "section": "",
    "text": "Enough with the theory and conceptual mumbo jumbo! Let’s get down to making a visualization and posting it somewhere for all the world to see! The basic idea of this assignment is to set up a repository that will serve as an experimental portfolio, and then create your first novel visualization element inside the portfolio. Its gonna be fun! Or at least educational…\nRemember that time that Barrie was dumb?"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#assignment",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#assignment",
    "title": "ASSIGNMENT 3",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThis assignment has two parts. The first part is technical. We’ll set up a Quarto Blog project as a new repository in your GitHub account. Then you’ll be a Blogger! Prestigious! The second part should be more fun. We are going to create your first Blog post as a visualization that explores an ACTION - TARGET pair relevant to your data set from Assignment 2."
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "title": "ASSIGNMENT 3",
    "section": "PART 1 TECHNICAL SORCERY",
    "text": "PART 1 TECHNICAL SORCERY\n\n1A - Create your BLOG project.\nHopefully by now you have created / dusted off / logged in to your GitHub account. Go ahead and log in to your account on the web and leave it open in a tab in your browser. There is a really great video about the next few steps (also linked below in RESORUCES) from Posit, but I’m giving you the condensed version here.\n\nFire up RStudio.\nGo to File-&gt;New Project and then select NEW DIRECTORY.\nNow select QUARTO BLOG.\nYou are going to create a the project in a new working directory. It is best practice to put this in a senstible directory structure on your local hard drive where your other GitHub repositories also live. Here is what mine looks like:\n\n 4. I suggest making the directory name something informative, like BCB504Portfolio, but hey… If you want to call your repository HasturBoxerShorts I won’t stop you. 5. Most of you will select Knitr as your Engine, but Cody “Mr. Hacker McPythonPants” might select Jupyter. 6. Check Create a git repository. The other boxes are optional and we can talk more about them later. 7. Click CREATE PROJECT.\nNow you’ve got a BLOG template all set up! Ha Ha! Onward to Internet Fame!\n\n\n1B - Make the BLOG about you.\nWe won’t spend a ton of time here, because this will be an ongoing process. You’ll go and watch all those cool videos and tutorials this weekend to figure this out. But lets do a couple things.\n\nMaybe you should modify the about.qmd file so that your name is in there somewhere.\nMaybe you should modify the index.qmd file with a better title in the YAML header.\nMaybe you should navigate to the posts folder, open the Welcome to my blog folder, open index.qmd from that directory, and add a sentence or two.\n\n\n\n1C - Customize your first post.\n\nNavigate to the posts folder, open the post with code folder, and open index.qmd.\nReplace ALL of the content of index.qmd with the most recent version of your .qmd file from ASSIGNMENT 2. Keep the file name index.qmd. Save that file!\nMove your data files to the post with code folder.\nRender the index.qmd file from this folder. Hopefully it worked!\n\n\n\n1D - Render the BLOG as a website.\n\nIMPORTANT Open your _quarto.yml file and add output-dir: docs under project:\n\n\nThe indentations matter here.\n\nSave all the files you’ve modified.\nGo to the BUILD tab in the (probably) top right section of RStudio.\nClick RENDER WEBSITE.\nClick through your new Blog and see how it works!\n\n\n\n1E - Push to GitHub.\nThere are quite a few ways to do this part. I’m going to use GitHub Desktop, but those video will show you other ways.\n\nGo to GitHub Desktop.\nType some text in the summary box.\nClick COMMIT TO MASTER.\nClick PUSH ORIGIN.\nGo to your GitHub in your browser. You should see your new repository! Yay!\n\n\n\n1F - Make it a website with GitHub pages.\n\nIn your browser, click on your repository.\nGo to SETTINGS.\nSelect PAGES.\nSet the SOURCE option to Deploy from a branch.\nSet the BRANCH to master and the directory to docs\nDeploy that stuff and wait. Then visit your site!"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "title": "ASSIGNMENT 3",
    "section": "PART 2 ELDRITCH VISUALIZATION RITUAL",
    "text": "PART 2 ELDRITCH VISUALIZATION RITUAL\n\n2A Define your ACTION - TARGET pair(s)\nIn [LECTURE 3] we discussed the concept of Task Abstraction in which you define the viz task that you want to help the user accomplish. This was represented as sets of ACTIONS that the user would perform (e.g. Discover, Present, Browse, Identify) on TARGETS related to the data set (e.g. Trends, Attributes, etc.).\nThink about one or two visualizations you wish to construct with your data, and try to define them in terms of ACTION - TARGET pairs. While you are at it, why don’t you update the index.qmd file of your BLOG POST with a new seciton at the bottom titled TASK ABSTRACTION, and put a sentence describing your visualizations and the ACTION - TARGET pairs they represent?\n\n\n2B Construct your Visualization\nLet’s get to work! Using whatever tools you can, code up your visualization in that new section of your BLOG post. You can check out how I approached this part in TUTORIAL 4."
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#resources",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#resources",
    "title": "ASSIGNMENT 3",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nQuarto’s BLOG Documentation\nA YouTube Video from Posit on Building a BLOG with Quarto"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html",
    "href": "posts/T9-FattyAcids/index.html",
    "title": "BCB 520 - Fatty Acids",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#preamble",
    "href": "posts/T9-FattyAcids/index.html#preamble",
    "title": "BCB 520 - Fatty Acids",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#data",
    "href": "posts/T9-FattyAcids/index.html#data",
    "title": "BCB 520 - Fatty Acids",
    "section": "Data",
    "text": "Data\nCreate a synthetic data set."
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#dataset-description",
    "href": "posts/T9-FattyAcids/index.html#dataset-description",
    "title": "BCB 520 - Fatty Acids",
    "section": "Dataset Description",
    "text": "Dataset Description\nThis synthetic dataset mimics data collection from a study of 21 women who provided diet and breast milk samples at 6 timepoints postpartum. The data includes:\n\n21 women (mother IDs 1-21)\n6 timepoints per woman (approximately weekly sampling starting in the first week postpartum)\n40 fatty acids measured in diet (percentage of total fatty acids)\n37 fatty acids measured in breast milk (percentage of total fatty acids)\n\nThe dataset includes common fatty acid types: - Saturated fatty acids (e.g., C16:0 palmitic acid, C18:0 stearic acid) - Monounsaturated fatty acids (e.g., C18:1n9 oleic acid) - Polyunsaturated fatty acids (PUFAs) - Omega-6 (e.g., C18:2n6 linoleic acid, C20:4n6 arachidonic acid) - Omega-3 (e.g., C18:3n3 alpha-linolenic acid, C22:6n3 docosahexaenoic acid/DHA) - Trans fatty acids (e.g., C18:1t)\nThe dataset incorporates biologically plausible relationships: - Diet influences milk fatty acid composition (especially for essential fatty acids) - Individual women have their own baseline patterns - Some fatty acids show time-dependent changes in milk (e.g., medium-chain FAs often increase)\n\n\nCode\n# Generate synthetic data for fatty acid profiles in diet and breast milk\n\nlibrary(tidyverse)\n\n# Define constants\nNUM_WOMEN &lt;- 21\nNUM_TIME_POINTS &lt;- 6\nNUM_DIET_FAS &lt;- 40\nNUM_MILK_FAS &lt;- 37\n\n# Generate fatty acid names\ngenerate_fatty_acid_names &lt;- function(count) {\n  # Common fatty acid naming patterns\n  fatty_acids &lt;- c(\n    # Saturated FAs\n    \"C4:0\", \"C6:0\", \"C8:0\", \"C10:0\", \"C12:0\", \"C14:0\", \"C16:0\", \"C18:0\", \"C20:0\", \"C22:0\", \"C24:0\",\n    # Monounsaturated FAs\n    \"C14:1\", \"C16:1\", \"C18:1n9\", \"C18:1n7\", \"C20:1n9\", \"C22:1n9\", \"C24:1n9\",\n    # Polyunsaturated FAs - Omega-6\n    \"C18:2n6\", \"C18:3n6\", \"C20:2n6\", \"C20:3n6\", \"C20:4n6\", \"C22:2n6\", \"C22:4n6\", \"C22:5n6\",\n    # Polyunsaturated FAs - Omega-3\n    \"C18:3n3\", \"C18:4n3\", \"C20:3n3\", \"C20:4n3\", \"C20:5n3\", \"C22:5n3\", \"C22:6n3\",\n    # Trans FAs\n    \"C16:1t\", \"C18:1t\", \"C18:2t\"\n  )\n  \n  # For any additional fatty acids needed beyond the common ones\n  if (count &gt; length(fatty_acids)) {\n    additional_names &lt;- character(count - length(fatty_acids))\n    for (i in 1:length(additional_names)) {\n      # Generate additional names in realistic pattern\n      carbon_chain &lt;- sample(4:24, 1)  # C4 to C24\n      double_bonds &lt;- sample(0:5, 1) \n      name &lt;- paste0(\"C\", carbon_chain, \":\", double_bonds)\n      if (double_bonds &gt; 0) {\n        position &lt;- sample(1:10, 1)\n        name &lt;- paste0(name, \"n\", position)\n      }\n      additional_names[i] &lt;- name\n    }\n    fatty_acids &lt;- c(fatty_acids, additional_names)\n  }\n  \n  return(fatty_acids[1:count])\n}\n\n# Generate the synthetic dataset\ngenerate_synthetic_data &lt;- function() {\n  # Generate names for diet and milk fatty acids\n  diet_fa_names &lt;- generate_fatty_acid_names(NUM_DIET_FAS)\n  milk_fa_names &lt;- diet_fa_names[1:NUM_MILK_FAS]  # Most milk FAs will be the same as diet FAs\n  \n  # Create empty dataframes with appropriate columns\n  diet_columns &lt;- c(\"woman_id\", \"timepoint\", \"days_postpartum\", diet_fa_names)\n  milk_columns &lt;- c(\"woman_id\", \"timepoint\", \"days_postpartum\", milk_fa_names)\n  \n  diet_data &lt;- data.frame(matrix(NA, nrow = 0, ncol = length(diet_columns)))\n  names(diet_data) &lt;- diet_columns\n  \n  milk_data &lt;- data.frame(matrix(NA, nrow = 0, ncol = length(milk_columns)))\n  names(milk_data) &lt;- milk_columns\n  \n  # Generate data for each woman\n  for (woman in 1:NUM_WOMEN) {\n    # Individual baseline characteristics that will influence their FA profiles\n    diet_pattern &lt;- runif(1)    # 0 = low fat, 1 = high fat\n    omega3_intake &lt;- runif(1)   # 0 = low, 1 = high\n    omega6_intake &lt;- runif(1)   # 0 = low, 1 = high\n    \n    # For each time point\n    for (timepoint in 1:NUM_TIME_POINTS) {\n      # Days postpartum (approximately weekly)\n      days_postpartum &lt;- (timepoint - 1) * 7 + sample(0:2, 1)\n      \n      # Create new rows for diet and milk\n      diet_row &lt;- data.frame(woman_id = woman, \n                            timepoint = timepoint,\n                            days_postpartum = days_postpartum)\n      \n      milk_row &lt;- data.frame(woman_id = woman, \n                            timepoint = timepoint,\n                            days_postpartum = days_postpartum)\n      \n      # Major fatty acids (higher abundance)\n      major_fas &lt;- c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\")\n      # Medium abundance fatty acids\n      medium_fas &lt;- c(\"C12:0\", \"C14:0\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\")\n      \n      # Generate diet fatty acid values\n      for (fa in diet_fa_names) {\n        if (fa %in% major_fas) {\n          # Major FAs (10-30%)\n          diet_row[[fa]] &lt;- runif(1, 10, 30)\n        } else if (fa %in% medium_fas) {\n          # Medium FAs (1-10%)\n          diet_row[[fa]] &lt;- runif(1, 1, 10)\n        } else {\n          # Minor FAs (&lt;1%)\n          diet_row[[fa]] &lt;- runif(1, 0.1, 1)\n        }\n        \n        # Apply individual characteristics\n        if (grepl(\"n3$\", fa)) {\n          diet_row[[fa]] &lt;- diet_row[[fa]] * (0.5 + omega3_intake)\n        } else if (grepl(\"n6$\", fa)) {\n          diet_row[[fa]] &lt;- diet_row[[fa]] * (0.5 + omega6_intake)\n        }\n        \n        # Round to 2 decimal places\n        diet_row[[fa]] &lt;- round(diet_row[[fa]], 2)\n      }\n      \n      # Generate milk fatty acid values based on diet\n      for (fa in milk_fa_names) {\n        if (fa %in% major_fas) {\n          # Major FAs (10-30%)\n          base_value &lt;- runif(1, 10, 30)\n        } else if (fa %in% medium_fas) {\n          # Medium FAs (1-10%)\n          base_value &lt;- runif(1, 1, 10)\n        } else {\n          # Minor FAs (&lt;1%)\n          base_value &lt;- runif(1, 0.1, 1)\n        }\n        \n        # Milk FA reflects diet FA but with biological processing\n        if (fa %in% diet_fa_names) {\n          # For essential FAs that transfer more directly\n          if (grepl(\"C18:2n6|C18:3n3|C20:5n3|C22:6n3\", fa)) {\n            # 60-80% correlation with diet for essential FAs\n            milk_row[[fa]] &lt;- diet_row[[fa]] * (0.6 + runif(1, 0, 0.2)) + \n                             base_value * (0.2 + runif(1, 0, 0.2))\n          } else {\n            # 30-50% correlation for other FAs\n            milk_row[[fa]] &lt;- diet_row[[fa]] * (0.3 + runif(1, 0, 0.2)) + \n                             base_value * (0.5 + runif(1, 0, 0.2))\n          }\n        } else {\n          milk_row[[fa]] &lt;- base_value\n        }\n        \n        # Time-dependent changes\n        if (grepl(\"C12:0|C14:0\", fa)) {\n          # Medium chain FAs often increase slightly in milk over time\n          milk_row[[fa]] &lt;- milk_row[[fa]] * (1 + (timepoint - 1) * 0.05)\n        } else if (fa == \"C22:6n3\") { # DHA\n          # DHA often decreases slightly if not supplemented\n          milk_row[[fa]] &lt;- milk_row[[fa]] * (1 - (timepoint - 1) * 0.03)\n        }\n        \n        # Round to 2 decimal places\n        milk_row[[fa]] &lt;- round(milk_row[[fa]], 2)\n      }\n      \n      # Add rows to dataframes\n      diet_data &lt;- rbind(diet_data, diet_row)\n      milk_data &lt;- rbind(milk_data, milk_row)\n    }\n  }\n  \n  return(list(diet_data = diet_data, milk_data = milk_data))\n}\n\n# Generate the data\nset.seed(123)  # For reproducibility\nsynthetic_data &lt;- generate_synthetic_data()\n\n# Write to CSV files\nwrite.csv(synthetic_data$diet_data, \"diet_fatty_acids.csv\", row.names = FALSE)\nwrite.csv(synthetic_data$milk_data, \"milk_fatty_acids.csv\", row.names = FALSE)\n\n# Preview the data\nhead(synthetic_data$diet_data[, 1:10])  # Show first 10 columns\nhead(synthetic_data$milk_data[, 1:10])  # Show first 10 columns\n\n# Verify data integrity\ncat(\"Diet data dimensions:\", dim(synthetic_data$diet_data), \"\\n\")\ncat(\"Milk data dimensions:\", dim(synthetic_data$milk_data), \"\\n\")\n\n# Check if values are plausible\ncat(\"Diet data range:\", range(as.matrix(synthetic_data$diet_data[, -(1:3)])), \"\\n\")\ncat(\"Milk data range:\", range(as.matrix(synthetic_data$milk_data[, -(1:3)])), \"\\n\")\n\n# Verify that key fatty acids are present\ncat(\"Key FAs in diet:\", \n    c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\") %in% colnames(synthetic_data$diet_data), \n    \"\\n\")\ncat(\"Key FAs in milk:\", \n    c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\") %in% colnames(synthetic_data$milk_data), \n    \"\\n\")\n\n# Calculate some basic descriptive statistics\ndiet_means &lt;- colMeans(synthetic_data$diet_data[, -(1:3)])\nmilk_means &lt;- colMeans(synthetic_data$milk_data[, -(1:3)])\n\ncat(\"Top 5 most abundant fatty acids in diet:\\n\")\nprint(sort(diet_means, decreasing = TRUE)[1:5])\n\ncat(\"Top 5 most abundant fatty acids in milk:\\n\")\nprint(sort(milk_means, decreasing = TRUE)[1:5])\n\n# Check correlation between diet and milk for key fatty acids\nkey_fas &lt;- c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\")\n\ncat(\"Diet-milk correlations for key fatty acids:\\n\")\ncorrelations &lt;- sapply(key_fas, function(fa) {\n  cor(synthetic_data$diet_data[[fa]], synthetic_data$milk_data[[fa]])\n})\nnames(correlations) &lt;- key_fas\nprint(correlations)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#data-files",
    "href": "posts/T9-FattyAcids/index.html#data-files",
    "title": "BCB 520 - Fatty Acids",
    "section": "Data Files",
    "text": "Data Files\nTwo CSV files are provided: - diet_fatty_acids.csv: Diet fatty acid profiles - milk_fatty_acids.csv: Breast milk fatty acid profiles\nEach file contains columns for: - woman_id: Unique ID for each woman (1-21) - timepoint: Sampling timepoint (1-6) - days_postpartum: Days since birth (varies slightly around weekly intervals) - Individual columns for each fatty acid (values in percentage of total fatty acids)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#analysis-approaches-in-r",
    "href": "posts/T9-FattyAcids/index.html#analysis-approaches-in-r",
    "title": "BCB 520 - Fatty Acids",
    "section": "Analysis Approaches in R",
    "text": "Analysis Approaches in R\nBelow are several R code examples demonstrating different analysis and visualization approaches for this dataset.\n\nSetup and Data Import\n\n\nCode\n# Import libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(vegan)    # For multivariate analyses\nlibrary(pheatmap) # For heatmaps\nlibrary(corrplot) # For correlation plots\n\n# Import the data\ndiet_data &lt;- read.csv(\"diet_fatty_acids.csv\")\nmilk_data &lt;- read.csv(\"milk_fatty_acids.csv\")\n\n# Convert to long format for some analyses\ndiet_long &lt;- diet_data %&gt;%\n  pivot_longer(cols = -c(woman_id, timepoint, days_postpartum),\n               names_to = \"fatty_acid\", \n               values_to = \"abundance\")\n\nmilk_long &lt;- milk_data %&gt;%\n  pivot_longer(cols = -c(woman_id, timepoint, days_postpartum),\n               names_to = \"fatty_acid\", \n               values_to = \"abundance\")\n\n\n\n\nAnalysis 1: Time Series Analysis\nTrack changes in specific fatty acids (e.g., DHA) over time:\n\n\nCode\nplot_fa_trajectory &lt;- function(data_long, target_fa) {\n  require(ggplot2)\n  \n  fa_data &lt;- data_long %&gt;% \n    filter(fatty_acid == target_fa)\n  \n  p &lt;- ggplot(fa_data, aes(x = days_postpartum, y = abundance, \n                       group = woman_id, color = factor(woman_id))) +\n    geom_line() +\n    geom_point() +\n    labs(title = paste0(target_fa, \" Abundance Over Time\"),\n         x = \"Days Postpartum\",\n         y = \"Abundance (% of total FAs)\",\n         color = \"Woman ID\") +\n    theme_minimal()\n  \n  return(p)\n}\n\n# DHA in milk over time \nmilk_dha &lt;- milk_long %&gt;% \n  filter(fatty_acid == \"C18.2n6\")\n\nggplot(milk_dha, aes(x = days_postpartum, y = abundance, \n                     group = woman_id, color = factor(woman_id))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Fatty Acid Classes in Breast Milk Over Time\",\n       x = \"Timepoint\",\n       y = \"Total Abundance (%)\",\n       fill = \"Fatty Acid Class\") +\n  theme_minimal()\n\n\n\n\n# Calculate omega-6/omega-3 ratio\nomega_ratio &lt;- milk_class_totals %&gt;%\n  filter(fa_class %in% c(\"Omega-3\", \"Omega-6\")) %&gt;%\n  pivot_wider(id_cols = c(woman_id, timepoint, days_postpartum),\n              names_from = fa_class,\n              values_from = total_abundance) %&gt;%\n  mutate(omega6_omega3_ratio = `Omega-6` / `Omega-3`)\n\nggplot(omega_ratio, aes(x = days_postpartum, y = omega6_omega3_ratio, \n                       group = woman_id, color = factor(woman_id))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Omega-6/Omega-3 Ratio in Breast Milk Over Time\",\n       x = \"Days Postpartum\",\n       y = \"Omega-6/Omega-3 Ratio\",\n       color = \"Woman ID\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#additional-visualization-ideas",
    "href": "posts/T9-FattyAcids/index.html#additional-visualization-ideas",
    "title": "BCB 520 - Fatty Acids",
    "section": "Additional Visualization Ideas",
    "text": "Additional Visualization Ideas\n\nInteractive Visualizations (using plotly)\n\n\nCode\nlibrary(plotly)\n\n# Interactive time series\ndha_plot &lt;- ggplot(milk_dha, aes(x = days_postpartum, y = abundance, \n                              group = woman_id, color = factor(woman_id))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"DHA in Breast Milk Over Time\",\n       x = \"Days Postpartum\",\n       y = \"Abundance (% of total FAs)\",\n       color = \"Woman ID\") +\n  theme_minimal()\n\nggplotly(dha_plot)\n\n# Interactive PCA plot\npca_plot &lt;- ggplot(pca_scores, aes(x = PC1, y = PC2, \n                                 color = factor(woman_id), \n                                 shape = factor(timepoint),\n                                 text = paste(\"Woman:\", woman_id, \n                                              \"\\nTimepoint:\", timepoint))) +\n  geom_point(size = 3) +\n  labs(title = \"PCA of Breast Milk Fatty Acid Profiles\",\n       color = \"Woman ID\",\n       shape = \"Timepoint\") +\n  theme_minimal()\n\nggplotly(pca_plot)\n\n\n\n\nRadar/Spider Plots (for comparing profiles)\nlibrary(fmsb)\n\n# Prepare data for radar plot (example for comparing diet clusters)\nradar_data &lt;- milk_by_cluster %&gt;%\n  pivot_wider(id_cols = diet_cluster, names_from = fatty_acid, values_from = mean_abundance) %&gt;%\n  as.data.frame()\n\nrownames(radar_data) &lt;- paste(\"Cluster\", radar_data$diet_cluster)\nradar_data &lt;- radar_data[,-1]  # Remove cluster column\n\n# Add max and min rows required by fmsb\nradar_data &lt;- rbind(\n  rep(max(radar_data), ncol(radar_data)),  # Max values\n  rep(min(radar_data), ncol(radar_data)),  # Min values\n  radar_data\n)\n\n# Plot\npar(mar = c(1, 1, 2, 1))\nradarchart(\n  radar_data,\n  pcol = rainbow(nrow(radar_data) - 2),\n  pfcol = scales::alpha(rainbow(nrow(radar_data) - 2), 0.3),\n  plwd = 2,\n  cglcol = \"grey\",\n  title = \"Comparison of Milk FA Profiles by Diet Cluster\"\n)\nlegend(\n  \"topright\",\n  legend = paste(\"Cluster\", 1:(nrow(radar_data) - 2)),\n  col = rainbow(nrow(radar_data) - 2),\n  lwd = 2,\n  pch = 16,\n  bty = \"n\"\n)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#key-research-questions-to-explore",
    "href": "posts/T9-FattyAcids/index.html#key-research-questions-to-explore",
    "title": "BCB 520 - Fatty Acids",
    "section": "Key Research Questions to Explore",
    "text": "Key Research Questions to Explore\n\nDiet-Milk Relationship\n\nWhich fatty acids show the strongest correlation between diet and milk?\nDo some women transfer specific fatty acids more efficiently than others?\nIs there a time lag between changes in diet and changes in milk composition?\n\nTemporal Changes\n\nHow do milk fatty acid profiles change over the first weeks postpartum?\nAre changes consistent across women or individual-specific?\nDo specific fatty acid classes show different temporal patterns?\n\nPattern Identification\n\nCan women be grouped based on their milk fatty acid profiles?\nDo dietary patterns predict milk composition patterns?\nIs the Omega-6/Omega-3 ratio stable or variable over time?\n\nIndividual Variability\n\nWhat is the extent of between-woman vs. within-woman variability?\nAre there fatty acids that show particularly high or low variability?\nCan maternal characteristics explain individual differences?"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#conclusion",
    "href": "posts/T9-FattyAcids/index.html#conclusion",
    "title": "BCB 520 - Fatty Acids",
    "section": "Conclusion",
    "text": "Conclusion\nThis synthetic dataset allows exploration of multiple analytical approaches for studying the relationship between maternal diet and breast milk fatty acid composition. The analyses range from simple visualization techniques to more complex multivariate and mixed-effect modeling approaches, providing a comprehensive toolkit for the student to apply to their real research data. = “DHA (C22:6n3) in Breast Milk Over Time”, x = “Days Postpartum”, y = “Abundance (% of total FAs)”, color = “Woman ID”) + theme_minimal() + theme(legend.position = “none”) # Omit legend if too many women\n\n### Analysis 2: Diet-Milk Correlations\n\nExamine correlations between dietary and milk fatty acid abundances:\n\n```r\n# Join diet and milk data for correlation analysis\njoined_data &lt;- diet_long %&gt;%\n  inner_join(milk_long, \n             by = c(\"woman_id\", \"timepoint\", \"days_postpartum\", \"fatty_acid\"),\n             suffix = c(\"_diet\", \"_milk\"))\n\n# Calculate correlations for each fatty acid\ncorrelations &lt;- joined_data %&gt;%\n  group_by(fatty_acid) %&gt;%\n  summarize(correlation = cor(abundance_diet, abundance_milk, method = \"spearman\"),\n            p_value = cor.test(abundance_diet, abundance_milk, method = \"spearman\")$p.value)\n\n# Plot the correlations\nggplot(correlations, aes(x = reorder(fatty_acid, correlation), y = correlation)) +\n  geom_bar(stat = \"identity\", aes(fill = p_value &lt; 0.05)) +\n  coord_flip() +\n  labs(title = \"Correlation Between Diet and Milk Fatty Acid Abundance\",\n       x = \"Fatty Acid\",\n       y = \"Spearman Correlation\",\n       fill = \"P &lt; 0.05\") +\n  theme_minimal()\n\n# Correlation matrix of selected important FAs between diet and milk\nimportant_fas &lt;- c(\"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C20:5n3\", \"C22:6n3\")\nimportant_diet &lt;- diet_data %&gt;% \n  select(woman_id, timepoint, all_of(important_fas)) %&gt;%\n  pivot_longer(cols = all_of(important_fas), \n               names_to = \"fatty_acid\", \n               values_to = \"diet_abundance\") %&gt;%\n  mutate(diet_milk = paste0(\"diet_\", fatty_acid))\n\nimportant_milk &lt;- milk_data %&gt;%\n  select(woman_id, timepoint, all_of(important_fas)) %&gt;%\n  pivot_longer(cols = all_of(important_fas), \n               names_to = \"fatty_acid\", \n               values_to = \"milk_abundance\") %&gt;%\n  mutate(diet_milk = paste0(\"milk_\", fatty_acid))\n\ncombined &lt;- important_diet %&gt;%\n  select(woman_id, timepoint, diet_milk, diet_abundance) %&gt;%\n  bind_rows(important_milk %&gt;% \n              select(woman_id, timepoint, diet_milk, milk_abundance = milk_abundance)) %&gt;%\n  pivot_wider(id_cols = c(woman_id, timepoint), \n              names_from = diet_milk, \n              values_from = c(diet_abundance, milk_abundance))\n\ncor_matrix &lt;- cor(combined %&gt;% select(-woman_id, -timepoint), use = \"pairwise.complete.obs\")\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", order = \"hclust\",\n         tl.col = \"black\", tl.srt = 45)\n\nAnalysis 3: Multivariate Analysis (PCA)\nPrincipal Component Analysis to identify patterns in milk fatty acid composition:\n# Prepare data for PCA (wide format, just the fatty acid abundances)\nmilk_wide &lt;- milk_data %&gt;%\n  select(-days_postpartum)\n\n# Extract just the FA columns for PCA\nmilk_fa_cols &lt;- milk_wide %&gt;% select(-woman_id, -timepoint)\n\n# Run PCA\npca_result &lt;- prcomp(milk_fa_cols, scale. = TRUE)\n\n# Extract scores\npca_scores &lt;- as.data.frame(pca_result$x)\npca_scores$woman_id &lt;- milk_wide$woman_id\npca_scores$timepoint &lt;- milk_wide$timepoint\n\n# Plot PCA\nggplot(pca_scores, aes(x = PC1, y = PC2, \n                       color = factor(woman_id), \n                       shape = factor(timepoint))) +\n  geom_point(size = 3) +\n  labs(title = \"PCA of Breast Milk Fatty Acid Profiles\",\n       color = \"Woman ID\",\n       shape = \"Timepoint\") +\n  theme_minimal()\n\n# Loadings plot to see which fatty acids drive the variation\nloadings &lt;- as.data.frame(pca_result$rotation)\nloadings$fatty_acid &lt;- rownames(loadings)\n\nggplot(loadings, aes(x = PC1, y = PC2, label = fatty_acid)) +\n  geom_point() +\n  geom_text(vjust = -0.5) +\n  labs(title = \"PCA Loadings: Fatty Acids\") +\n  theme_minimal()\n\n\nAnalysis 4: Heatmap Visualization\nCreate heatmaps to visualize patterns across women and fatty acids:\n# Prepare data for heatmap (woman at one timepoint, e.g., timepoint 3)\nmilk_wide_t3 &lt;- milk_data %&gt;%\n  filter(timepoint == 3) %&gt;%\n  select(-timepoint, -days_postpartum) %&gt;%\n  column_to_rownames(\"woman_id\")\n\n# Create heatmap\npheatmap(milk_wide_t3,\n         scale = \"column\",  # Scale by fatty acid\n         clustering_distance_rows = \"euclidean\",\n         clustering_distance_cols = \"euclidean\",\n         main = \"Breast Milk Fatty Acid Profiles at Timepoint 3\")\n\n# Compare across timepoints for a subset of important fatty acids\nimportant_fas &lt;- c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \n                   \"C20:4n6\", \"C20:5n3\", \"C22:6n3\")\n\n# Time series heatmap\nmilk_long_subset &lt;- milk_long %&gt;%\n  filter(fatty_acid %in% important_fas) %&gt;%\n  unite(\"woman_time\", c(woman_id, timepoint), sep = \"_T\")\n\nmilk_heatmap_data &lt;- milk_long_subset %&gt;%\n  pivot_wider(names_from = fatty_acid, values_from = abundance) %&gt;%\n  column_to_rownames(\"woman_time\")\n\npheatmap(as.matrix(milk_heatmap_data),\n         scale = \"column\",\n         cluster_rows = TRUE,\n         cluster_cols = TRUE,\n         main = \"Breast Milk Fatty Acid Profiles Across Women and Time\")\n\n\nAnalysis 5: Diet-Milk Transfer Efficiency\nCalculate and visualize the transfer efficiency from diet to milk:\n# Calculate ratio of milk to diet for each fatty acid\njoined_data &lt;- diet_long %&gt;%\n  inner_join(milk_long, \n             by = c(\"woman_id\", \"timepoint\", \"days_postpartum\", \"fatty_acid\"),\n             suffix = c(\"_diet\", \"_milk\"))\n\n# Calculate transfer ratio\njoined_data$transfer_ratio &lt;- joined_data$abundance_milk / joined_data$abundance_diet\n\n# Calculate mean transfer ratio for each fatty acid\ntransfer_summary &lt;- joined_data %&gt;%\n  group_by(fatty_acid) %&gt;%\n  summarize(mean_ratio = mean(transfer_ratio, na.rm = TRUE),\n            sd_ratio = sd(transfer_ratio, na.rm = TRUE))\n\n# Plot mean transfer ratios\nggplot(transfer_summary, aes(x = reorder(fatty_acid, mean_ratio), y = mean_ratio)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_ratio - sd_ratio, ymax = mean_ratio + sd_ratio), width = 0.2) +\n  coord_flip() +\n  labs(title = \"Fatty Acid Transfer Efficiency (Milk/Diet Ratio)\",\n       x = \"Fatty Acid\",\n       y = \"Mean Transfer Ratio (Milk/Diet)\") +\n  theme_minimal()\n\n# Does transfer efficiency change over time?\ntransfer_time &lt;- joined_data %&gt;%\n  filter(fatty_acid %in% important_fas) %&gt;%\n  group_by(fatty_acid, timepoint) %&gt;%\n  summarize(mean_ratio = mean(transfer_ratio, na.rm = TRUE),\n            sd_ratio = sd(transfer_ratio, na.rm = TRUE))\n\nggplot(transfer_time, aes(x = timepoint, y = mean_ratio, color = fatty_acid, group = fatty_acid)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean_ratio - sd_ratio, ymax = mean_ratio + sd_ratio), width = 0.2) +\n  labs(title = \"Fatty Acid Transfer Efficiency Over Time\",\n       x = \"Timepoint\",\n       y = \"Mean Transfer Ratio (Milk/Diet)\",\n       color = \"Fatty Acid\") +\n  theme_minimal()\n\n\nAnalysis 6: Mixed-Effects Models\nExamine fatty acid changes over time while accounting for individual differences:\nlibrary(lme4)\nlibrary(lmerTest)\n\n# Example for DHA\ndha_data &lt;- milk_long %&gt;% \n  filter(fatty_acid == \"C22:6n3\")\n\n# Linear mixed model with random intercept by woman\ndha_model &lt;- lmer(abundance ~ days_postpartum + (1|woman_id), data = dha_data)\nsummary(dha_model)\n\n# Visualize individual trajectories with overall trend\nggplot(dha_data, aes(x = days_postpartum, y = abundance, group = woman_id)) +\n  geom_line(alpha = 0.3) +\n  geom_smooth(aes(group = 1), method = \"lm\", color = \"red\") +\n  labs(title = \"DHA in Breast Milk Over Time with Overall Trend\",\n       x = \"Days Postpartum\",\n       y = \"DHA Abundance (%)\") +\n  theme_minimal()\n\n\nAnalysis 7: Fatty Acid Group Analysis\nAnalyze by fatty acid groups (saturated, monounsaturated, omega-3, omega-6):\n# Create a function to classify fatty acids\nclassify_fa &lt;- function(fa_name) {\n  if (grepl(\"C[0-9]+:0$\", fa_name)) {\n    return(\"Saturated\")\n  } else if (grepl(\"C[0-9]+:1\", fa_name)) {\n    if (grepl(\"t$\", fa_name)) {\n      return(\"Trans\")\n    } else {\n      return(\"Monounsaturated\")\n    }\n  } else if (grepl(\"n3$\", fa_name)) {\n    return(\"Omega-3\")\n  } else if (grepl(\"n6$\", fa_name)) {\n    return(\"Omega-6\")\n  } else if (grepl(\"t$\", fa_name)) {\n    return(\"Trans\")\n  } else {\n    return(\"Other\")\n  }\n}\n\n# Add classifications to the data\nmilk_long$fa_class &lt;- sapply(milk_long$fatty_acid, classify_fa)\ndiet_long$fa_class &lt;- sapply(diet_long$fatty_acid, classify_fa)\n\n# Calculate totals by class\nmilk_class_totals &lt;- milk_long %&gt;%\n  group_by(woman_id, timepoint, days_postpartum, fa_class) %&gt;%\n  summarize(total_abundance = sum(abundance))\n\ndiet_class_totals &lt;- diet_long %&gt;%\n  group_by(woman_id, timepoint, days_postpartum, fa_class) %&gt;%\n  summarize(total_abundance = sum(abundance))\n\n# Plot class distributions\nggplot(milk_class_totals, aes(x = factor(timepoint), y = total_abundance, fill = fa_class)) +\n  geom_boxplot() +\n  facet_wrap(~fa_class, scales = \"free_y\") +\n  labs(title\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[BCB520]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar-title\"}\n[BCB520]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar-title\"}\n[Home]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Home\"}\n[/index.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/index.html\"}\n[SYLLABUS 2025]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:SYLLABUS 2025\"}\n[/syllabus.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/syllabus.html\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[BCB 520 - Fatty Acids – BCB520]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-metatitle\"}\n[BCB 520 - Fatty Acids – BCB520]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercardtitle\"}\n[BCB 520 - Fatty Acids – BCB520]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardtitle\"}\n[BCB520]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-metasitename\"}\n[Sharon's data]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercarddesc\"}\n[Sharon's data]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardddesc\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"BCB 520 - Fatty Acids\"\nsubtitle: \"Synthetic Data!\"\nformat:\n  html:\n    toc: false\n    echo: true\nauthor: \"Barrie Robison\"\ndate: \"2025-03-14\"\ncategories: [Portfolio, DataViz, Observable, Animation]\nimage: \"Zomsimmesh.png\"\ndescription: \"Sharon's data\"\ncode-fold: true\ncode-tools: true\neval: false\n\n---\n\n## PREAMBLE\n\nquarto-executable-code-5450563D\n\n```r\n#| output: false\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#data-1",
    "href": "posts/T9-FattyAcids/index.html#data-1",
    "title": "BCB 520 - Fatty Acids",
    "section": "Data",
    "text": "Data\nCreate a synthetic data set."
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#dataset-description-1",
    "href": "posts/T9-FattyAcids/index.html#dataset-description-1",
    "title": "BCB 520 - Fatty Acids",
    "section": "Dataset Description",
    "text": "Dataset Description\nThis synthetic dataset mimics data collection from a study of 21 women who provided diet and breast milk samples at 6 timepoints postpartum. The data includes:\n\n21 women (mother IDs 1-21)\n6 timepoints per woman (approximately weekly sampling starting in the first week postpartum)\n40 fatty acids measured in diet (percentage of total fatty acids)\n37 fatty acids measured in breast milk (percentage of total fatty acids)\n\nThe dataset includes common fatty acid types: - Saturated fatty acids (e.g., C16:0 palmitic acid, C18:0 stearic acid) - Monounsaturated fatty acids (e.g., C18:1n9 oleic acid) - Polyunsaturated fatty acids (PUFAs) - Omega-6 (e.g., C18:2n6 linoleic acid, C20:4n6 arachidonic acid) - Omega-3 (e.g., C18:3n3 alpha-linolenic acid, C22:6n3 docosahexaenoic acid/DHA) - Trans fatty acids (e.g., C18:1t)\nThe dataset incorporates biologically plausible relationships: - Diet influences milk fatty acid composition (especially for essential fatty acids) - Individual women have their own baseline patterns - Some fatty acids show time-dependent changes in milk (e.g., medium-chain FAs often increase)\nquarto-executable-code-5450563D\n#| eval: false\n# Generate synthetic data for fatty acid profiles in diet and breast milk\n\nlibrary(tidyverse)\n\n# Define constants\nNUM_WOMEN &lt;- 21\nNUM_TIME_POINTS &lt;- 6\nNUM_DIET_FAS &lt;- 40\nNUM_MILK_FAS &lt;- 37\n\n# Generate fatty acid names\ngenerate_fatty_acid_names &lt;- function(count) {\n  # Common fatty acid naming patterns\n  fatty_acids &lt;- c(\n    # Saturated FAs\n    \"C4:0\", \"C6:0\", \"C8:0\", \"C10:0\", \"C12:0\", \"C14:0\", \"C16:0\", \"C18:0\", \"C20:0\", \"C22:0\", \"C24:0\",\n    # Monounsaturated FAs\n    \"C14:1\", \"C16:1\", \"C18:1n9\", \"C18:1n7\", \"C20:1n9\", \"C22:1n9\", \"C24:1n9\",\n    # Polyunsaturated FAs - Omega-6\n    \"C18:2n6\", \"C18:3n6\", \"C20:2n6\", \"C20:3n6\", \"C20:4n6\", \"C22:2n6\", \"C22:4n6\", \"C22:5n6\",\n    # Polyunsaturated FAs - Omega-3\n    \"C18:3n3\", \"C18:4n3\", \"C20:3n3\", \"C20:4n3\", \"C20:5n3\", \"C22:5n3\", \"C22:6n3\",\n    # Trans FAs\n    \"C16:1t\", \"C18:1t\", \"C18:2t\"\n  )\n  \n  # For any additional fatty acids needed beyond the common ones\n  if (count &gt; length(fatty_acids)) {\n    additional_names &lt;- character(count - length(fatty_acids))\n    for (i in 1:length(additional_names)) {\n      # Generate additional names in realistic pattern\n      carbon_chain &lt;- sample(4:24, 1)  # C4 to C24\n      double_bonds &lt;- sample(0:5, 1) \n      name &lt;- paste0(\"C\", carbon_chain, \":\", double_bonds)\n      if (double_bonds &gt; 0) {\n        position &lt;- sample(1:10, 1)\n        name &lt;- paste0(name, \"n\", position)\n      }\n      additional_names[i] &lt;- name\n    }\n    fatty_acids &lt;- c(fatty_acids, additional_names)\n  }\n  \n  return(fatty_acids[1:count])\n}\n\n# Generate the synthetic dataset\ngenerate_synthetic_data &lt;- function() {\n  # Generate names for diet and milk fatty acids\n  diet_fa_names &lt;- generate_fatty_acid_names(NUM_DIET_FAS)\n  milk_fa_names &lt;- diet_fa_names[1:NUM_MILK_FAS]  # Most milk FAs will be the same as diet FAs\n  \n  # Create empty dataframes with appropriate columns\n  diet_columns &lt;- c(\"woman_id\", \"timepoint\", \"days_postpartum\", diet_fa_names)\n  milk_columns &lt;- c(\"woman_id\", \"timepoint\", \"days_postpartum\", milk_fa_names)\n  \n  diet_data &lt;- data.frame(matrix(NA, nrow = 0, ncol = length(diet_columns)))\n  names(diet_data) &lt;- diet_columns\n  \n  milk_data &lt;- data.frame(matrix(NA, nrow = 0, ncol = length(milk_columns)))\n  names(milk_data) &lt;- milk_columns\n  \n  # Generate data for each woman\n  for (woman in 1:NUM_WOMEN) {\n    # Individual baseline characteristics that will influence their FA profiles\n    diet_pattern &lt;- runif(1)    # 0 = low fat, 1 = high fat\n    omega3_intake &lt;- runif(1)   # 0 = low, 1 = high\n    omega6_intake &lt;- runif(1)   # 0 = low, 1 = high\n    \n    # For each time point\n    for (timepoint in 1:NUM_TIME_POINTS) {\n      # Days postpartum (approximately weekly)\n      days_postpartum &lt;- (timepoint - 1) * 7 + sample(0:2, 1)\n      \n      # Create new rows for diet and milk\n      diet_row &lt;- data.frame(woman_id = woman, \n                            timepoint = timepoint,\n                            days_postpartum = days_postpartum)\n      \n      milk_row &lt;- data.frame(woman_id = woman, \n                            timepoint = timepoint,\n                            days_postpartum = days_postpartum)\n      \n      # Major fatty acids (higher abundance)\n      major_fas &lt;- c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\")\n      # Medium abundance fatty acids\n      medium_fas &lt;- c(\"C12:0\", \"C14:0\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\")\n      \n      # Generate diet fatty acid values\n      for (fa in diet_fa_names) {\n        if (fa %in% major_fas) {\n          # Major FAs (10-30%)\n          diet_row[[fa]] &lt;- runif(1, 10, 30)\n        } else if (fa %in% medium_fas) {\n          # Medium FAs (1-10%)\n          diet_row[[fa]] &lt;- runif(1, 1, 10)\n        } else {\n          # Minor FAs (&lt;1%)\n          diet_row[[fa]] &lt;- runif(1, 0.1, 1)\n        }\n        \n        # Apply individual characteristics\n        if (grepl(\"n3$\", fa)) {\n          diet_row[[fa]] &lt;- diet_row[[fa]] * (0.5 + omega3_intake)\n        } else if (grepl(\"n6$\", fa)) {\n          diet_row[[fa]] &lt;- diet_row[[fa]] * (0.5 + omega6_intake)\n        }\n        \n        # Round to 2 decimal places\n        diet_row[[fa]] &lt;- round(diet_row[[fa]], 2)\n      }\n      \n      # Generate milk fatty acid values based on diet\n      for (fa in milk_fa_names) {\n        if (fa %in% major_fas) {\n          # Major FAs (10-30%)\n          base_value &lt;- runif(1, 10, 30)\n        } else if (fa %in% medium_fas) {\n          # Medium FAs (1-10%)\n          base_value &lt;- runif(1, 1, 10)\n        } else {\n          # Minor FAs (&lt;1%)\n          base_value &lt;- runif(1, 0.1, 1)\n        }\n        \n        # Milk FA reflects diet FA but with biological processing\n        if (fa %in% diet_fa_names) {\n          # For essential FAs that transfer more directly\n          if (grepl(\"C18:2n6|C18:3n3|C20:5n3|C22:6n3\", fa)) {\n            # 60-80% correlation with diet for essential FAs\n            milk_row[[fa]] &lt;- diet_row[[fa]] * (0.6 + runif(1, 0, 0.2)) + \n                             base_value * (0.2 + runif(1, 0, 0.2))\n          } else {\n            # 30-50% correlation for other FAs\n            milk_row[[fa]] &lt;- diet_row[[fa]] * (0.3 + runif(1, 0, 0.2)) + \n                             base_value * (0.5 + runif(1, 0, 0.2))\n          }\n        } else {\n          milk_row[[fa]] &lt;- base_value\n        }\n        \n        # Time-dependent changes\n        if (grepl(\"C12:0|C14:0\", fa)) {\n          # Medium chain FAs often increase slightly in milk over time\n          milk_row[[fa]] &lt;- milk_row[[fa]] * (1 + (timepoint - 1) * 0.05)\n        } else if (fa == \"C22:6n3\") { # DHA\n          # DHA often decreases slightly if not supplemented\n          milk_row[[fa]] &lt;- milk_row[[fa]] * (1 - (timepoint - 1) * 0.03)\n        }\n        \n        # Round to 2 decimal places\n        milk_row[[fa]] &lt;- round(milk_row[[fa]], 2)\n      }\n      \n      # Add rows to dataframes\n      diet_data &lt;- rbind(diet_data, diet_row)\n      milk_data &lt;- rbind(milk_data, milk_row)\n    }\n  }\n  \n  return(list(diet_data = diet_data, milk_data = milk_data))\n}\n\n# Generate the data\nset.seed(123)  # For reproducibility\nsynthetic_data &lt;- generate_synthetic_data()\n\n# Write to CSV files\nwrite.csv(synthetic_data$diet_data, \"diet_fatty_acids.csv\", row.names = FALSE)\nwrite.csv(synthetic_data$milk_data, \"milk_fatty_acids.csv\", row.names = FALSE)\n\n# Preview the data\nhead(synthetic_data$diet_data[, 1:10])  # Show first 10 columns\nhead(synthetic_data$milk_data[, 1:10])  # Show first 10 columns\n\n# Verify data integrity\ncat(\"Diet data dimensions:\", dim(synthetic_data$diet_data), \"\\n\")\ncat(\"Milk data dimensions:\", dim(synthetic_data$milk_data), \"\\n\")\n\n# Check if values are plausible\ncat(\"Diet data range:\", range(as.matrix(synthetic_data$diet_data[, -(1:3)])), \"\\n\")\ncat(\"Milk data range:\", range(as.matrix(synthetic_data$milk_data[, -(1:3)])), \"\\n\")\n\n# Verify that key fatty acids are present\ncat(\"Key FAs in diet:\", \n    c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\") %in% colnames(synthetic_data$diet_data), \n    \"\\n\")\ncat(\"Key FAs in milk:\", \n    c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\") %in% colnames(synthetic_data$milk_data), \n    \"\\n\")\n\n# Calculate some basic descriptive statistics\ndiet_means &lt;- colMeans(synthetic_data$diet_data[, -(1:3)])\nmilk_means &lt;- colMeans(synthetic_data$milk_data[, -(1:3)])\n\ncat(\"Top 5 most abundant fatty acids in diet:\\n\")\nprint(sort(diet_means, decreasing = TRUE)[1:5])\n\ncat(\"Top 5 most abundant fatty acids in milk:\\n\")\nprint(sort(milk_means, decreasing = TRUE)[1:5])\n\n# Check correlation between diet and milk for key fatty acids\nkey_fas &lt;- c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C22:6n3\")\n\ncat(\"Diet-milk correlations for key fatty acids:\\n\")\ncorrelations &lt;- sapply(key_fas, function(fa) {\n  cor(synthetic_data$diet_data[[fa]], synthetic_data$milk_data[[fa]])\n})\nnames(correlations) &lt;- key_fas\nprint(correlations)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#data-files-1",
    "href": "posts/T9-FattyAcids/index.html#data-files-1",
    "title": "BCB 520 - Fatty Acids",
    "section": "Data Files",
    "text": "Data Files\nTwo CSV files are provided: - diet_fatty_acids.csv: Diet fatty acid profiles - milk_fatty_acids.csv: Breast milk fatty acid profiles\nEach file contains columns for: - woman_id: Unique ID for each woman (1-21) - timepoint: Sampling timepoint (1-6) - days_postpartum: Days since birth (varies slightly around weekly intervals) - Individual columns for each fatty acid (values in percentage of total fatty acids)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#analysis-approaches-in-r-1",
    "href": "posts/T9-FattyAcids/index.html#analysis-approaches-in-r-1",
    "title": "BCB 520 - Fatty Acids",
    "section": "Analysis Approaches in R",
    "text": "Analysis Approaches in R\nBelow are several R code examples demonstrating different analysis and visualization approaches for this dataset.\n\nSetup and Data Import\nquarto-executable-code-5450563D\n# Import libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(vegan)    # For multivariate analyses\nlibrary(pheatmap) # For heatmaps\nlibrary(corrplot) # For correlation plots\n\n# Import the data\ndiet_data &lt;- read.csv(\"diet_fatty_acids.csv\")\nmilk_data &lt;- read.csv(\"milk_fatty_acids.csv\")\n\n# Convert to long format for some analyses\ndiet_long &lt;- diet_data %&gt;%\n  pivot_longer(cols = -c(woman_id, timepoint, days_postpartum),\n               names_to = \"fatty_acid\", \n               values_to = \"abundance\")\n\nmilk_long &lt;- milk_data %&gt;%\n  pivot_longer(cols = -c(woman_id, timepoint, days_postpartum),\n               names_to = \"fatty_acid\", \n               values_to = \"abundance\")\n\n\nAnalysis 1: Time Series Analysis\nTrack changes in specific fatty acids (e.g., DHA) over time:\nquarto-executable-code-5450563D\nplot_fa_trajectory &lt;- function(data_long, target_fa) {\n  require(ggplot2)\n  \n  fa_data &lt;- data_long %&gt;% \n    filter(fatty_acid == target_fa)\n  \n  p &lt;- ggplot(fa_data, aes(x = days_postpartum, y = abundance, \n                       group = woman_id, color = factor(woman_id))) +\n    geom_line() +\n    geom_point() +\n    labs(title = paste0(target_fa, \" Abundance Over Time\"),\n         x = \"Days Postpartum\",\n         y = \"Abundance (% of total FAs)\",\n         color = \"Woman ID\") +\n    theme_minimal()\n  \n  return(p)\n}\n\n# DHA in milk over time \nmilk_dha &lt;- milk_long %&gt;% \n  filter(fatty_acid == \"C18.2n6\")\n\nggplot(milk_dha, aes(x = days_postpartum, y = abundance, \n                     group = woman_id, color = factor(woman_id))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Fatty Acid Classes in Breast Milk Over Time\",\n       x = \"Timepoint\",\n       y = \"Total Abundance (%)\",\n       fill = \"Fatty Acid Class\") +\n  theme_minimal()\n\n\n\n\n# Calculate omega-6/omega-3 ratio\nomega_ratio &lt;- milk_class_totals %&gt;%\n  filter(fa_class %in% c(\"Omega-3\", \"Omega-6\")) %&gt;%\n  pivot_wider(id_cols = c(woman_id, timepoint, days_postpartum),\n              names_from = fa_class,\n              values_from = total_abundance) %&gt;%\n  mutate(omega6_omega3_ratio = `Omega-6` / `Omega-3`)\n\nggplot(omega_ratio, aes(x = days_postpartum, y = omega6_omega3_ratio, \n                       group = woman_id, color = factor(woman_id))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Omega-6/Omega-3 Ratio in Breast Milk Over Time\",\n       x = \"Days Postpartum\",\n       y = \"Omega-6/Omega-3 Ratio\",\n       color = \"Woman ID\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#additional-visualization-ideas-1",
    "href": "posts/T9-FattyAcids/index.html#additional-visualization-ideas-1",
    "title": "BCB 520 - Fatty Acids",
    "section": "Additional Visualization Ideas",
    "text": "Additional Visualization Ideas\n\nInteractive Visualizations (using plotly)\nquarto-executable-code-5450563D\nlibrary(plotly)\n\n# Interactive time series\ndha_plot &lt;- ggplot(milk_dha, aes(x = days_postpartum, y = abundance, \n                              group = woman_id, color = factor(woman_id))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"DHA in Breast Milk Over Time\",\n       x = \"Days Postpartum\",\n       y = \"Abundance (% of total FAs)\",\n       color = \"Woman ID\") +\n  theme_minimal()\n\nggplotly(dha_plot)\n\n# Interactive PCA plot\npca_plot &lt;- ggplot(pca_scores, aes(x = PC1, y = PC2, \n                                 color = factor(woman_id), \n                                 shape = factor(timepoint),\n                                 text = paste(\"Woman:\", woman_id, \n                                              \"\\nTimepoint:\", timepoint))) +\n  geom_point(size = 3) +\n  labs(title = \"PCA of Breast Milk Fatty Acid Profiles\",\n       color = \"Woman ID\",\n       shape = \"Timepoint\") +\n  theme_minimal()\n\nggplotly(pca_plot)\n\n\nRadar/Spider Plots (for comparing profiles)\nlibrary(fmsb)\n\n# Prepare data for radar plot (example for comparing diet clusters)\nradar_data &lt;- milk_by_cluster %&gt;%\n  pivot_wider(id_cols = diet_cluster, names_from = fatty_acid, values_from = mean_abundance) %&gt;%\n  as.data.frame()\n\nrownames(radar_data) &lt;- paste(\"Cluster\", radar_data$diet_cluster)\nradar_data &lt;- radar_data[,-1]  # Remove cluster column\n\n# Add max and min rows required by fmsb\nradar_data &lt;- rbind(\n  rep(max(radar_data), ncol(radar_data)),  # Max values\n  rep(min(radar_data), ncol(radar_data)),  # Min values\n  radar_data\n)\n\n# Plot\npar(mar = c(1, 1, 2, 1))\nradarchart(\n  radar_data,\n  pcol = rainbow(nrow(radar_data) - 2),\n  pfcol = scales::alpha(rainbow(nrow(radar_data) - 2), 0.3),\n  plwd = 2,\n  cglcol = \"grey\",\n  title = \"Comparison of Milk FA Profiles by Diet Cluster\"\n)\nlegend(\n  \"topright\",\n  legend = paste(\"Cluster\", 1:(nrow(radar_data) - 2)),\n  col = rainbow(nrow(radar_data) - 2),\n  lwd = 2,\n  pch = 16,\n  bty = \"n\"\n)"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#key-research-questions-to-explore-1",
    "href": "posts/T9-FattyAcids/index.html#key-research-questions-to-explore-1",
    "title": "BCB 520 - Fatty Acids",
    "section": "Key Research Questions to Explore",
    "text": "Key Research Questions to Explore\n\nDiet-Milk Relationship\n\nWhich fatty acids show the strongest correlation between diet and milk?\nDo some women transfer specific fatty acids more efficiently than others?\nIs there a time lag between changes in diet and changes in milk composition?\n\nTemporal Changes\n\nHow do milk fatty acid profiles change over the first weeks postpartum?\nAre changes consistent across women or individual-specific?\nDo specific fatty acid classes show different temporal patterns?\n\nPattern Identification\n\nCan women be grouped based on their milk fatty acid profiles?\nDo dietary patterns predict milk composition patterns?\nIs the Omega-6/Omega-3 ratio stable or variable over time?\n\nIndividual Variability\n\nWhat is the extent of between-woman vs. within-woman variability?\nAre there fatty acids that show particularly high or low variability?\nCan maternal characteristics explain individual differences?"
  },
  {
    "objectID": "posts/T9-FattyAcids/index.html#conclusion-1",
    "href": "posts/T9-FattyAcids/index.html#conclusion-1",
    "title": "BCB 520 - Fatty Acids",
    "section": "Conclusion",
    "text": "Conclusion\nThis synthetic dataset allows exploration of multiple analytical approaches for studying the relationship between maternal diet and breast milk fatty acid composition. The analyses range from simple visualization techniques to more complex multivariate and mixed-effect modeling approaches, providing a comprehensive toolkit for the student to apply to their real research data. = “DHA (C22:6n3) in Breast Milk Over Time”, x = “Days Postpartum”, y = “Abundance (% of total FAs)”, color = “Woman ID”) + theme_minimal() + theme(legend.position = “none”) # Omit legend if too many women\n\n### Analysis 2: Diet-Milk Correlations\n\nExamine correlations between dietary and milk fatty acid abundances:\n\n```r\n# Join diet and milk data for correlation analysis\njoined_data &lt;- diet_long %&gt;%\n  inner_join(milk_long, \n             by = c(\"woman_id\", \"timepoint\", \"days_postpartum\", \"fatty_acid\"),\n             suffix = c(\"_diet\", \"_milk\"))\n\n# Calculate correlations for each fatty acid\ncorrelations &lt;- joined_data %&gt;%\n  group_by(fatty_acid) %&gt;%\n  summarize(correlation = cor(abundance_diet, abundance_milk, method = \"spearman\"),\n            p_value = cor.test(abundance_diet, abundance_milk, method = \"spearman\")$p.value)\n\n# Plot the correlations\nggplot(correlations, aes(x = reorder(fatty_acid, correlation), y = correlation)) +\n  geom_bar(stat = \"identity\", aes(fill = p_value &lt; 0.05)) +\n  coord_flip() +\n  labs(title = \"Correlation Between Diet and Milk Fatty Acid Abundance\",\n       x = \"Fatty Acid\",\n       y = \"Spearman Correlation\",\n       fill = \"P &lt; 0.05\") +\n  theme_minimal()\n\n# Correlation matrix of selected important FAs between diet and milk\nimportant_fas &lt;- c(\"C18:2n6\", \"C18:3n3\", \"C20:4n6\", \"C20:5n3\", \"C22:6n3\")\nimportant_diet &lt;- diet_data %&gt;% \n  select(woman_id, timepoint, all_of(important_fas)) %&gt;%\n  pivot_longer(cols = all_of(important_fas), \n               names_to = \"fatty_acid\", \n               values_to = \"diet_abundance\") %&gt;%\n  mutate(diet_milk = paste0(\"diet_\", fatty_acid))\n\nimportant_milk &lt;- milk_data %&gt;%\n  select(woman_id, timepoint, all_of(important_fas)) %&gt;%\n  pivot_longer(cols = all_of(important_fas), \n               names_to = \"fatty_acid\", \n               values_to = \"milk_abundance\") %&gt;%\n  mutate(diet_milk = paste0(\"milk_\", fatty_acid))\n\ncombined &lt;- important_diet %&gt;%\n  select(woman_id, timepoint, diet_milk, diet_abundance) %&gt;%\n  bind_rows(important_milk %&gt;% \n              select(woman_id, timepoint, diet_milk, milk_abundance = milk_abundance)) %&gt;%\n  pivot_wider(id_cols = c(woman_id, timepoint), \n              names_from = diet_milk, \n              values_from = c(diet_abundance, milk_abundance))\n\ncor_matrix &lt;- cor(combined %&gt;% select(-woman_id, -timepoint), use = \"pairwise.complete.obs\")\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", order = \"hclust\",\n         tl.col = \"black\", tl.srt = 45)\n\nAnalysis 3: Multivariate Analysis (PCA)\nPrincipal Component Analysis to identify patterns in milk fatty acid composition:\n# Prepare data for PCA (wide format, just the fatty acid abundances)\nmilk_wide &lt;- milk_data %&gt;%\n  select(-days_postpartum)\n\n# Extract just the FA columns for PCA\nmilk_fa_cols &lt;- milk_wide %&gt;% select(-woman_id, -timepoint)\n\n# Run PCA\npca_result &lt;- prcomp(milk_fa_cols, scale. = TRUE)\n\n# Extract scores\npca_scores &lt;- as.data.frame(pca_result$x)\npca_scores$woman_id &lt;- milk_wide$woman_id\npca_scores$timepoint &lt;- milk_wide$timepoint\n\n# Plot PCA\nggplot(pca_scores, aes(x = PC1, y = PC2, \n                       color = factor(woman_id), \n                       shape = factor(timepoint))) +\n  geom_point(size = 3) +\n  labs(title = \"PCA of Breast Milk Fatty Acid Profiles\",\n       color = \"Woman ID\",\n       shape = \"Timepoint\") +\n  theme_minimal()\n\n# Loadings plot to see which fatty acids drive the variation\nloadings &lt;- as.data.frame(pca_result$rotation)\nloadings$fatty_acid &lt;- rownames(loadings)\n\nggplot(loadings, aes(x = PC1, y = PC2, label = fatty_acid)) +\n  geom_point() +\n  geom_text(vjust = -0.5) +\n  labs(title = \"PCA Loadings: Fatty Acids\") +\n  theme_minimal()\n\n\nAnalysis 4: Heatmap Visualization\nCreate heatmaps to visualize patterns across women and fatty acids:\n# Prepare data for heatmap (woman at one timepoint, e.g., timepoint 3)\nmilk_wide_t3 &lt;- milk_data %&gt;%\n  filter(timepoint == 3) %&gt;%\n  select(-timepoint, -days_postpartum) %&gt;%\n  column_to_rownames(\"woman_id\")\n\n# Create heatmap\npheatmap(milk_wide_t3,\n         scale = \"column\",  # Scale by fatty acid\n         clustering_distance_rows = \"euclidean\",\n         clustering_distance_cols = \"euclidean\",\n         main = \"Breast Milk Fatty Acid Profiles at Timepoint 3\")\n\n# Compare across timepoints for a subset of important fatty acids\nimportant_fas &lt;- c(\"C16:0\", \"C18:0\", \"C18:1n9\", \"C18:2n6\", \"C18:3n3\", \n                   \"C20:4n6\", \"C20:5n3\", \"C22:6n3\")\n\n# Time series heatmap\nmilk_long_subset &lt;- milk_long %&gt;%\n  filter(fatty_acid %in% important_fas) %&gt;%\n  unite(\"woman_time\", c(woman_id, timepoint), sep = \"_T\")\n\nmilk_heatmap_data &lt;- milk_long_subset %&gt;%\n  pivot_wider(names_from = fatty_acid, values_from = abundance) %&gt;%\n  column_to_rownames(\"woman_time\")\n\npheatmap(as.matrix(milk_heatmap_data),\n         scale = \"column\",\n         cluster_rows = TRUE,\n         cluster_cols = TRUE,\n         main = \"Breast Milk Fatty Acid Profiles Across Women and Time\")\n\n\nAnalysis 5: Diet-Milk Transfer Efficiency\nCalculate and visualize the transfer efficiency from diet to milk:\n# Calculate ratio of milk to diet for each fatty acid\njoined_data &lt;- diet_long %&gt;%\n  inner_join(milk_long, \n             by = c(\"woman_id\", \"timepoint\", \"days_postpartum\", \"fatty_acid\"),\n             suffix = c(\"_diet\", \"_milk\"))\n\n# Calculate transfer ratio\njoined_data$transfer_ratio &lt;- joined_data$abundance_milk / joined_data$abundance_diet\n\n# Calculate mean transfer ratio for each fatty acid\ntransfer_summary &lt;- joined_data %&gt;%\n  group_by(fatty_acid) %&gt;%\n  summarize(mean_ratio = mean(transfer_ratio, na.rm = TRUE),\n            sd_ratio = sd(transfer_ratio, na.rm = TRUE))\n\n# Plot mean transfer ratios\nggplot(transfer_summary, aes(x = reorder(fatty_acid, mean_ratio), y = mean_ratio)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_ratio - sd_ratio, ymax = mean_ratio + sd_ratio), width = 0.2) +\n  coord_flip() +\n  labs(title = \"Fatty Acid Transfer Efficiency (Milk/Diet Ratio)\",\n       x = \"Fatty Acid\",\n       y = \"Mean Transfer Ratio (Milk/Diet)\") +\n  theme_minimal()\n\n# Does transfer efficiency change over time?\ntransfer_time &lt;- joined_data %&gt;%\n  filter(fatty_acid %in% important_fas) %&gt;%\n  group_by(fatty_acid, timepoint) %&gt;%\n  summarize(mean_ratio = mean(transfer_ratio, na.rm = TRUE),\n            sd_ratio = sd(transfer_ratio, na.rm = TRUE))\n\nggplot(transfer_time, aes(x = timepoint, y = mean_ratio, color = fatty_acid, group = fatty_acid)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean_ratio - sd_ratio, ymax = mean_ratio + sd_ratio), width = 0.2) +\n  labs(title = \"Fatty Acid Transfer Efficiency Over Time\",\n       x = \"Timepoint\",\n       y = \"Mean Transfer Ratio (Milk/Diet)\",\n       color = \"Fatty Acid\") +\n  theme_minimal()\n\n\nAnalysis 6: Mixed-Effects Models\nExamine fatty acid changes over time while accounting for individual differences:\nlibrary(lme4)\nlibrary(lmerTest)\n\n# Example for DHA\ndha_data &lt;- milk_long %&gt;% \n  filter(fatty_acid == \"C22:6n3\")\n\n# Linear mixed model with random intercept by woman\ndha_model &lt;- lmer(abundance ~ days_postpartum + (1|woman_id), data = dha_data)\nsummary(dha_model)\n\n# Visualize individual trajectories with overall trend\nggplot(dha_data, aes(x = days_postpartum, y = abundance, group = woman_id)) +\n  geom_line(alpha = 0.3) +\n  geom_smooth(aes(group = 1), method = \"lm\", color = \"red\") +\n  labs(title = \"DHA in Breast Milk Over Time with Overall Trend\",\n       x = \"Days Postpartum\",\n       y = \"DHA Abundance (%)\") +\n  theme_minimal()\n\n\nAnalysis 7: Fatty Acid Group Analysis\nAnalyze by fatty acid groups (saturated, monounsaturated, omega-3, omega-6):\n# Create a function to classify fatty acids\nclassify_fa &lt;- function(fa_name) {\n  if (grepl(\"C[0-9]+:0$\", fa_name)) {\n    return(\"Saturated\")\n  } else if (grepl(\"C[0-9]+:1\", fa_name)) {\n    if (grepl(\"t$\", fa_name)) {\n      return(\"Trans\")\n    } else {\n      return(\"Monounsaturated\")\n    }\n  } else if (grepl(\"n3$\", fa_name)) {\n    return(\"Omega-3\")\n  } else if (grepl(\"n6$\", fa_name)) {\n    return(\"Omega-6\")\n  } else if (grepl(\"t$\", fa_name)) {\n    return(\"Trans\")\n  } else {\n    return(\"Other\")\n  }\n}\n\n# Add classifications to the data\nmilk_long$fa_class &lt;- sapply(milk_long$fatty_acid, classify_fa)\ndiet_long$fa_class &lt;- sapply(diet_long$fatty_acid, classify_fa)\n\n# Calculate totals by class\nmilk_class_totals &lt;- milk_long %&gt;%\n  group_by(woman_id, timepoint, days_postpartum, fa_class) %&gt;%\n  summarize(total_abundance = sum(abundance))\n\ndiet_class_totals &lt;- diet_long %&gt;%\n  group_by(woman_id, timepoint, days_postpartum, fa_class) %&gt;%\n  summarize(total_abundance = sum(abundance))\n\n# Plot class distributions\nggplot(milk_class_totals, aes(x = factor(timepoint), y = total_abundance, fill = fa_class)) +\n  geom_boxplot() +\n  facet_wrap(~fa_class, scales = \"free_y\") +\n  labs(title\n\nquarto-executable-code-5450563D\n\n```r\n``````````````````` :::"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#tldr-version",
    "href": "posts/L5-ThumbRules/index.html#tldr-version",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "TLDR VERSION",
    "text": "TLDR VERSION\nThese are guidelines and considerations, not really absolute rules:\n\nWhen to use 3D? when to use 2D?\nWhen to use eyes instead of memory?\nWhen does immersion help?\nWhen to use overviews?\nHow long is too long?\nWhich comes first, form or function?"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#unjustified-use-of-3d",
    "href": "posts/L5-ThumbRules/index.html#unjustified-use-of-3d",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "UNJUSTIFIED USE OF 3D",
    "text": "UNJUSTIFIED USE OF 3D\nCommon in news and some infographics.\n\n\n\nCheck this out on WTFViz\n\n\nCheck this out on WTFViz"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#depth-vs-planar-position",
    "href": "posts/L5-ThumbRules/index.html#depth-vs-planar-position",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "DEPTH VS PLANAR POSITION",
    "text": "DEPTH VS PLANAR POSITION\nPosition channels are very powerful… when they are PLANAR SPATIAL POSITION, not depth!"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#the-dangers-of-depth",
    "href": "posts/L5-ThumbRules/index.html#the-dangers-of-depth",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "THE DANGERS OF DEPTH",
    "text": "THE DANGERS OF DEPTH"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#the-actual-dangers-of-depth",
    "href": "posts/L5-ThumbRules/index.html#the-actual-dangers-of-depth",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "THE ACTUAL DANGERS OF DEPTH",
    "text": "THE ACTUAL DANGERS OF DEPTH\nWe don’t really live in 3D: we see in 2.05D.\n\nWe acquire more info on an image plane quickly using eye movements\nWe acquire more info for depth much more slowly using head/body motion"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#occlusion",
    "href": "posts/L5-ThumbRules/index.html#occlusion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "OCCLUSION",
    "text": "OCCLUSION\n“Occlusion occurs when one 3D graphic partially blocks another. It is the result of mimicking space in the natural world–where objects have differing X, Y, and Z coordinates. In data visualization, occlusion obscures important data and creates false hierarchies wherein unobstructed graphics appear most important.” … From This BLOG post by M. Bowers.\n\nInteraction can resolve occlusion, but at cost of time and cognitive load."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#perspective-distortion",
    "href": "posts/L5-ThumbRules/index.html#perspective-distortion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "PERSPECTIVE DISTORTION",
    "text": "PERSPECTIVE DISTORTION\nPerspective in 3D visualizations interferes with all size channel encodings. The power of 2D planar positioning is lost!\n“Distortion occurs when 3D graphics recede into or project out from the picture plane through foreshortening. In drawing, foreshortening makes objects seem as though they inhabit three-dimensional space, but in data visualization, it creates more false hierarchies. Foreground graphics appear larger, background graphics smaller, and the relationship between data series is needlessly skewed.”…… From This BLOG post by M. Bowers."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-bar-charts---probably-not",
    "href": "posts/L5-ThumbRules/index.html#d-bar-charts---probably-not",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D BAR CHARTS - Probably not…",
    "text": "3D BAR CHARTS - Probably not…\n3D bars are very difficult to justify!\nPerspective distortion and occlusion make faceting into 2D the better choice in most situations."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#tilted-text",
    "href": "posts/L5-ThumbRules/index.html#tilted-text",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "TILTED TEXT",
    "text": "TILTED TEXT\nSkewed perspective in 3D visualizations does not interact well with fonts.\n\nExploring and Reducing the Effects of Orientation on Text Readability in Volumetric Displays.Grossman et al. CHI 2007"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#example-3d-extruded-time-series",
    "href": "posts/L5-ThumbRules/index.html#example-3d-extruded-time-series",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "EXAMPLE: 3D EXTRUDED TIME SERIES",
    "text": "EXAMPLE: 3D EXTRUDED TIME SERIES\nWhat information can we decode from this visualization?\n\nCluster and Calendar based Visualization of Time Series Data. van Wijk and van Selow, Proc. InfoVis 99."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#planar-alternative",
    "href": "posts/L5-ThumbRules/index.html#planar-alternative",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "PLANAR ALTERNATIVE",
    "text": "PLANAR ALTERNATIVE\nThis version of the extruded time series visualization uses derived data by computing a cluster hierarchy of power usage patterns. It then juxtaposes multiple views: a calendar and superimposed 2D curves differentiated with a color channel."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-shape-perception",
    "href": "posts/L5-ThumbRules/index.html#d-shape-perception",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D: SHAPE PERCEPTION",
    "text": "3D: SHAPE PERCEPTION\nThe benefits of 3D visualization outweigh its costs when the task is shape perception for 3D spatial data. Interactive navigation often supports synthesis across many viewpoints.\n\nImage-Based Streamline Generation and Rendering. Li and Shen. IEEE Trans. Visualization and Computer Graphics (TVCG) 13:3 (2007), 630–640."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-constrained-navigation",
    "href": "posts/L5-ThumbRules/index.html#d-constrained-navigation",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D: CONSTRAINED NAVIGATION",
    "text": "3D: CONSTRAINED NAVIGATION\n\nNYTimes Subscribers can access here"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-summary",
    "href": "posts/L5-ThumbRules/index.html#d-summary",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D SUMMARY",
    "text": "3D SUMMARY\n\n3D legitimate for true 3D spatial data.\n3D needs very careful justification for abstract data\nThe benefits outweigh costs when topological structure/context important for the task."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#eyes-beat-memory",
    "href": "posts/L5-ThumbRules/index.html#eyes-beat-memory",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "EYES BEAT MEMORY",
    "text": "EYES BEAT MEMORY\nExternal Cognition vs. Internal Memory\nIt is easy to compare by moving eyes between side-by-side views.\nIt is much more difficult to compare a visible item to your memory of what you saw.\n\nImplications for animation:\ngreat for choreographed storytelling\ngreat for transitions between two states\npoor for many states with changes everywhere\nconsider small multiples instead"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#resolution-beats-immersion",
    "href": "posts/L5-ThumbRules/index.html#resolution-beats-immersion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "RESOLUTION BEATS IMMERSION",
    "text": "RESOLUTION BEATS IMMERSION\nImmersion is typically not helpful for abstract data because we do not need a sense of presence or stereoscopic 3D. A desktop view is also usually better for workflow integration.\nResolution in VR is a critical constraint, as pixels are the scarcest resource.\nFirst wave: virtual reality for abstract data! This is difficult to justify.\nSecond wave: AR/MR (augmented/mixed reality) has more promise."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#overview",
    "href": "posts/L5-ThumbRules/index.html#overview",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "OVERVIEW",
    "text": "OVERVIEW\n“Overview first, zoom and filter, details on demand”\nThe Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations. Shneiderman. Proc. IEEE Visual Languages, pp. 336–343, 1996.\n\nThis seems simple enough, until you realize that “Overview” is really a microcosm of the full vis design problem space."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#breaking-conventions",
    "href": "posts/L5-ThumbRules/index.html#breaking-conventions",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "BREAKING CONVENTIONS",
    "text": "BREAKING CONVENTIONS\nThe inverted y axis is evocative of blood dripping down on Poe. Also, this is a cool example of “Overview first, zoom and filter, details on demand”\n\nhttps://public.tableau.com/profile/ben.jones#!/vizhome/EdgarAllanPoeViz/EdgarAllanPoeViz"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#responsiveness",
    "href": "posts/L5-ThumbRules/index.html#responsiveness",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "RESPONSIVENESS",
    "text": "RESPONSIVENESS\nResponsiveness (visual feedback to the user) has 3 rough categories:\n\n0.1 seconds: perceptual processing\n\nsubsecond response for mouseover highlighting\n\n1 second: immediate response\n\nfast response after mouseclick, button press\nFitts’ Law limits on motor control\n\n10 seconds: brief tasks\n\nbounded response after dialog box\nmental model of heavyweight operation (file load)\n\nShow hourglass for multi-second operations (check for cancel/undo)\nShow progress bar for long operations (process in background thread)\nConsider rendering speed when item count is large (guaranteed frame rate)"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#function-then-form",
    "href": "posts/L5-ThumbRules/index.html#function-then-form",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "FUNCTION, THEN FORM",
    "text": "FUNCTION, THEN FORM\nIt is dangerous to start with aesthetics because it is usually impossible to add function retroactively.\nStart with focus on functionality because you can improve and refine aesthetics later.\nAesthetics do matter! They are another level of function. Consider working with a graphic designer on important visualizations."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#form-basic-graphic-design",
    "href": "posts/L5-ThumbRules/index.html#form-basic-graphic-design",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "FORM: BASIC GRAPHIC DESIGN",
    "text": "FORM: BASIC GRAPHIC DESIGN\n\n\nProximity:\nDO group related items together\nAVOID equal whitespace between unrelated items\nAlignment:\nDO find/make a strong line and stick to it\nAVOID automatic centering\nRepetition:\nDO unify by pushing existing consistencies\nContrast:\nIf not identical, then very different\nAVOID not quite the same\n\n\n\nThe Non-Designer’s Design Book, 4th ed. Robin Williams, Peachpit Press, 2015. fast read, very practical to work through whole thing"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#labelling",
    "href": "posts/L5-ThumbRules/index.html#labelling",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "LABELLING",
    "text": "LABELLING\nMake visualizations as self-documenting as possible!\nMeaningful & useful title, labels, legends.\nAxes and panes/subwindows should have labels and axes should have good mix/max boundary tick marks.\nEverything that’s plotted should have a legend and its own header/labels if not redundant with main title.\nUse reasonable numerical format and avoid scientific notation in most cases.\n\nhttps://xkcd.com/833/"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#summary---rules-of-thumb",
    "href": "posts/L5-ThumbRules/index.html#summary---rules-of-thumb",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "SUMMARY - RULES OF THUMB",
    "text": "SUMMARY - RULES OF THUMB\n\nNo unjustified 3D\n\nPower of the plane\nDisparity of depth\nOcclusion hides information\nPerspective distortion dangers\nTilted text isn’t legible\n\nEyes beat memory\nResolution over immersion\nOverview first, zoom and filter, details on demand\nResponsiveness is required\nFunction first, form next\n\n\n\n\n\nHOME"
  },
  {
    "objectID": "posts/A5-TabularData/DataDictionary.html",
    "href": "posts/A5-TabularData/DataDictionary.html",
    "title": "DataDictionary",
    "section": "",
    "text": "All player statistics are presented within the scope of the selected filters.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPlayer\nPlayer name.\n\n\nTeam\nTeam or teams that the player has played for. Not displayed when filtering for specific teams.\n\n\nPosition\nPosition or positions that the player has been listed as playing by the NHL.\n\n\nGP\nGames played.\n\n\nTOI\nTotal amount of time played.\n\n\nTOI/GP\nAmount of time played per game. Not displayed when filtering for Counts.\n\n\n\n\n\nCorsi: Any shot attempt (goals, shots on net, misses and blocks) outside of the shootout. Referred to as SAT by the NHL.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCF\nCount of Corsi for that player’s team while that player is on the ice.\n\n\nCA\nCount of Corsi against that player’s team while that player is on the ice.\n\n\nCF%\nPercentage of total Corsi while that player is on the ice that are for that player’s team. CF*100/(CF+CA)\n\n\n\n\n\n\nFenwick: Any unblocked shot attempt (goals, shots on net and misses) outside of the shootout. Referred to as USAT by the NHL.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFF\nCount of Fenwick for that player’s team while that player is on the ice.\n\n\nFA\nCount of Fenwick against that player’s team while that player is on the ice.\n\n\nFF%\nPercentage of total Fenwick while that player is on the ice that are for that player’s team. FF*100/(FF+FA)\n\n\n\n\n\n\nShots: Any shot attempt on net (goals and shots on net) outside of the shootout.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nSF\nCount of Shots for that player’s team while that player is on the ice.\n\n\nSA\nCount of Shots against that player’s team while that player is on the ice.\n\n\nSF%\nPercentage of total Shots while that player is on the ice that are for that player’s team. SF*100/(SF+SA)\n\n\n\n\n\n\nGoals: Any goal, outside of the shootout.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGF\nCount of Goals for that player’s team while that player is on the ice.\n\n\nGA\nCount of Goals against that player’s team while that player is on the ice.\n\n\nGF%\nPercentage of total Goals while that player is on the ice that are for that player’s team. GF*100/(GF+GA)\n\n\n\n\n\n\nA scoring chance, as originally defined by War-on-Ice.\nEach shot attempt (Corsi) taken in the offensive zone is assigned a value based on the area of the zone in which it was recorded. Attempts made from the attacking team’s neutral or defensive zones are excluded.\n\nAttempts from the yellow areas are assigned a value of 1\nAttempts from the red areas are assigned a value of 2\nAttempts in the green area are assigned a value of 3\n\nAdjustments: - Add 1 to this value if the attempt is considered a rush shot or a rebound - A rebound is any attempt made within 3 seconds of another blocked, missed or saved attempt without a stoppage in play in between - A rush shot is any attempt within 4 seconds of any event in the neutral or defensive zone without a stoppage in play in between - Decrease this value by 1 if it was a blocked shot - Any attempt with a score of 2 or higher is considered a scoring chance\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nSCF\nCount of Scoring Chances for that player’s team while that player is on the ice.\n\n\nSCA\nCount of Scoring Chances against that player’s team while that player is on the ice.\n\n\nSCF%\nPercentage of total Scoring Chances while that player is on the ice that are for that player’s team. SCF*100/(SCF+SCA)\n\n\n\n\n\n\nA scoring chance with a score of 3 or higher.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nHDCF\nCount of High Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nHDCA\nCount of High Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nHDCF%\nPercentage of total High Danger Scoring Chances while that player is on the ice that are for that player’s team. HDCF*100/(HDCF+HDCA)\n\n\n\n\n\n\nGoals generated from High Danger Scoring Chances\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nHDGF\nCount of Goals off of High Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nHDGA\nCount of Goals off of High Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nHDGF%\nPercentage of High Danger Goals while that player is on the ice that are for that player’s team. HDGF*100/(HDGF+HDGA)\n\n\n\n\n\n\nA scoring chance with a score of exactly 2.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nMDCF\nCount of Medium Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nMDCA\nCount of Medium Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nMDCF%\nPercentage of total Medium Danger Scoring Chances while that player is on the ice that are for that player’s team. MDCF*100/(MDCF+MDCA)\n\n\n\n\n\n\nGoals generated from Medium Danger Scoring Chances\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nMDGF\nCount of Goals off of Medium Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nMDGA\nCount of Goals off of Medium Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nMDGF%\nPercentage of Medium Danger Goals while that player is on the ice that are for that player’s team. MDGF*100/(MDGF+MDGA)\n\n\n\n\n\n\nA scoring chance with a score of 1 or less. Does not include any attempts from the attacking team’s neutral or defensive zone.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLDCF\nCount of Low Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nLDCA\nCount of Low Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nLDCF%\nPercentage of total Low Danger Scoring Chances while that player is on the ice that are for that player’s team. LDCF*100/(LDCF+LDCA)\n\n\n\n\n\n\nGoals generated from Low Danger Scoring Chances\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLDGF\nCount of Goals off of Low Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nLDGA\nCount of Goals off of Low Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nLDGF%\nPercentage of Low Danger Goals while that player is on the ice that are for that player’s team. LDGF*100/(LDGF+LDGA)\n\n\n\n\n\n\nExpected goals (xG) is a statistical measure that evaluates shot quality by assigning a goal probability to each shot attempt.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nxGF\nExpected Goals For. The sum of the probability values of all shot attempts for that player’s team while that player is on the ice. Represents the number of goals the team should have scored based on shot quality.\n\n\nxGA\nExpected Goals Against. The sum of the probability values of all shot attempts against that player’s team while that player is on the ice. Represents the number of goals the team should have conceded based on shot quality.\n\n\nxGF%\nExpected Goals Percentage. The percentage of the total expected goals while that player is on the ice that are for that player’s team. xGF*100/(xGF+xGA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nSH%\nPercentage of Shots for that player’s team while that player is on the ice that were Goals. GF*100/SF\n\n\nSV%\nPercentage of Shots against that player’s team while that player is on the ice that were not Goals. GA*100/SA\n\n\nPDO\nShooting percentage plus save percentage. (GF/SF)+(GA/SA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nOff. Zone Starts\nNumber of shifts for the player that started with an offensive zone faceoff.\n\n\nNeu. Zone Starts\nNumber of shifts for the player that started with a neutral zone faceoff.\n\n\nDef. Zone Starts\nNumber of shifts for the player that started with a defensive zone faceoff.\n\n\nOn The Fly Starts\nNumber of shifts for the player that started during play (without a faceoff).\n\n\nOff. Zone Start %\nPercentage of starts for the player that were Offensive Zone Starts, excluding Neutral Zone and On The Fly Starts. Off. Zone Starts*100/(Off. Zone Starts+Def. Zone Starts)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nOff. Zone Faceoffs\nNumber of faceoffs in the offensive zone for which the player was on the ice.\n\n\nNeu. Zone Faceoffs\nNumber of faceoffs in the neutral zone for which the player was on the ice.\n\n\nDef. Zone Faceoffs\nNumber of faceoffs in the defensive zone for which the player was on the ice.\n\n\nOff. Zone Faceoff %\nPercentage of faceoffs in the offensive zone for which the player was on the ice, excluding neutral zone faceoffs. Off. Zone Faceoffs*100/(Off. Zone Faceoffs+Def. Zone Faceoffs)\n\n\n\n\n\n\n\nAll player statistics are presented within the scope of the selected filters.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPlayer\nPlayer name.\n\n\nTeam\nTeam or teams that the player has played for. Not displayed when filtering for specific teams.\n\n\nPosition\nPosition or positions that the player has been listed as playing by the NHL.\n\n\nGP\nGames played.\n\n\nTOI\nTotal amount of time played.\n\n\nTOI/GP\nAmount of time played per game. Not displayed when filtering for Counts.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGoals\nGoals scored by the player, outside of the shootout.\n\n\nAssists\nAny assist by the player.\n\n\nFirst Assists\nPrimary assists by the player.\n\n\nSecond Assists\nSecondary assists by the player.\n\n\nTotal Points\nGoals scored and assists by the player, outside of the shootout.\n\n\nIPP\nIndividual Point Percentage, the percentage of goals for that player’s team while that player is on the ice that the player earned a point on. Total Points/Goals For\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nShots\nAny shot attempt on net (goals and shots on net) by the player, outside of the shootout.\n\n\nSH%\nPercentage of Shots by the player that were Goals. Goals*100/Shots\n\n\niCF\nAny shot attempt (goals, shots on net, misses and blocks) by the player, outside of the shootout.\n\n\niFF\nAny unblocked shot attempt (goals, shots on net and misses) by the player, outside of the shootout.\n\n\niSCF\nAny scoring chance by the player, outside of the shootout.\n\n\niHDCF\nAny high danger scoring chance by the player, outside of the shootout.\n\n\nRush Attempts\nAny rush shot attempt (goals, shots on net, misses and blocks) by the player, outside of the shootout.\n\n\nRebounds Created\nAny shot attempt (shots on net, misses and blocks) that results in a rebound shot attempt.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPIM\nPenalty minutes assessed to the player.\n\n\nTotal Penalties\nTotal number of penalties taken by the player\n\n\nMinor\nNumber of minor penalties taken by the player.\n\n\nMajor\nNumber of major penalties taken by the player.\n\n\nMisconduct\nNumber of 10 minute or game misconducts taken by the player.\n\n\nPenalties Drawn\nNumber of penalties committed against the player.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGiveaways\nNumber of unforced turnovers made by the player.\n\n\nTakeaways\nNumber of times the player takes the puck away from the opposition.\n\n\nHits\nNumber of hits made by the player.\n\n\nHits Taken\nNumber of hits taken by the player.\n\n\nShots Blocked\nNumber of opposition shot attempts blocked by the player.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFaceoffs Won\nFaceoffs won by the player\n\n\nFaceoffs Lost\nFaceoffs lost by the player\n\n\nFaceoff %\nPercentage of total faceoffs taken by that player that were won Faceoffs Won*100/(Faceoffs Won+Faceoffs Lost)\n\n\n\n\n\n\n\nAll player statistics are presented within the scope of the selected filters.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPlayer\nPlayer name.\n\n\nTeam\nTeam or teams that the player has played for. Not displayed when filtering for specific teams.\n\n\nGP\nGames played.\n\n\nTOI\nTotal amount of time played.\n\n\nTOI/GP\nAmount of time played per game. Not displayed when filtering for Counts.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nShots Against\nShots on Goal (Shots and Goals) faced by the goalie.\n\n\nSaves\nSaves made by the goalie.\n\n\nGoals Against\nGoals allowed by the goalie.\n\n\nSV%\nPercentage of Shots Against that were Saves. (Saves/Shots Against)\n\n\nGAA\nGoals allowed per 60 minutes of play. Goals Against*60/TOI\n\n\nGSAA\nGoals Saved Above Average, the difference between the goalie’s Goals Against and a Goals Against with the same Shots Against and the average SV%. Average SV%*Shots Against - Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nHD Shots Against\nShots on Goal (Shots and Goals) that are High Danger Chances faced by the goalie.\n\n\nHD Saves\nSaves on HD Shots Against made by the goalie.\n\n\nHD Goals Against\nGoals from HD Shots Against allowed by the goalie.\n\n\nHDSV%\nPercentage of HD Shots Against that were Saves. (HD Saves/HD Shots Against)\n\n\nHDGAA\nHD Goals allowed per 60 minutes of play. HD Goals Against*60/TOI\n\n\nHDGSAA\nHD Goals Saved Above Average, the difference between the goalie’s HD Goals Against and a HD Goals Against with the same HD Shots Against and the average HDSV%. Average HDSV%*HD Shots Against - HD Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nMD Shots Against\nShots on Goal (Shots and Goals) that are Medium Danger Chances faced by the goalie.\n\n\nMD Saves\nSaves on MD Shots Against made by the goalie.\n\n\nMD Goals Against\nGoals from MD Shots Against allowed by the goalie.\n\n\nMDSV%\nPercentage of MD Shots Against that were Saves. (MD Saves/MD Shots Against)\n\n\nMDGAA\nMD Goals allowed per 60 minutes of play. MD Goals Against*60/TOI\n\n\nMDGSAA\nMD Goals Saved Above Average, the difference between the goalie’s MD Goals Against and a MD Goals Against with the same MD Shots Against and the average MDSV%. Average MDSV%*MD Shots Against - MD Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLD Shots Against\nShots on Goal (Shots and Goals) that are Low Danger Chances faced by the goalie.\n\n\nLD Saves\nSaves on LD Shots Against made by the goalie.\n\n\nLD Goals Against\nGoals from LD Shots Against allowed by the goalie.\n\n\nLDSV%\nPercentage of LD Shots Against that were Saves. (LD Saves/LD Shots Against)\n\n\nLDGAA\nLD Goals allowed per 60 minutes of play. LD Goals Against*60/TOI\n\n\nLDGSAA\nLD Goals Saved Above Average, the difference between the goalie’s LD Goals Against and a LD Goals Against with the same LD Shots Against and the average LDSV%. Average LDSV%*LD Shots Against - LD Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRush Attempts Against\nNumber of Shot Attempts (including missed and block shots) that the goalie faced that are considered Rush Attempts\n\n\nRebound Attempts Against\nNumber of Shot Attempts (including missed and block shots) that the goalie faced that are considered Rebound Attempts\n\n\nAvg. Shot Distance\nAverage distance (in feet) of shots on net faced by the goalie\n\n\nAvg. Goal Distance\nAverage distance (in feet) of goals allowed by the goalie"
  },
  {
    "objectID": "posts/A5-TabularData/DataDictionary.html#player-on-ice-statistics",
    "href": "posts/A5-TabularData/DataDictionary.html#player-on-ice-statistics",
    "title": "DataDictionary",
    "section": "",
    "text": "All player statistics are presented within the scope of the selected filters.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPlayer\nPlayer name.\n\n\nTeam\nTeam or teams that the player has played for. Not displayed when filtering for specific teams.\n\n\nPosition\nPosition or positions that the player has been listed as playing by the NHL.\n\n\nGP\nGames played.\n\n\nTOI\nTotal amount of time played.\n\n\nTOI/GP\nAmount of time played per game. Not displayed when filtering for Counts.\n\n\n\n\n\nCorsi: Any shot attempt (goals, shots on net, misses and blocks) outside of the shootout. Referred to as SAT by the NHL.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCF\nCount of Corsi for that player’s team while that player is on the ice.\n\n\nCA\nCount of Corsi against that player’s team while that player is on the ice.\n\n\nCF%\nPercentage of total Corsi while that player is on the ice that are for that player’s team. CF*100/(CF+CA)\n\n\n\n\n\n\nFenwick: Any unblocked shot attempt (goals, shots on net and misses) outside of the shootout. Referred to as USAT by the NHL.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFF\nCount of Fenwick for that player’s team while that player is on the ice.\n\n\nFA\nCount of Fenwick against that player’s team while that player is on the ice.\n\n\nFF%\nPercentage of total Fenwick while that player is on the ice that are for that player’s team. FF*100/(FF+FA)\n\n\n\n\n\n\nShots: Any shot attempt on net (goals and shots on net) outside of the shootout.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nSF\nCount of Shots for that player’s team while that player is on the ice.\n\n\nSA\nCount of Shots against that player’s team while that player is on the ice.\n\n\nSF%\nPercentage of total Shots while that player is on the ice that are for that player’s team. SF*100/(SF+SA)\n\n\n\n\n\n\nGoals: Any goal, outside of the shootout.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGF\nCount of Goals for that player’s team while that player is on the ice.\n\n\nGA\nCount of Goals against that player’s team while that player is on the ice.\n\n\nGF%\nPercentage of total Goals while that player is on the ice that are for that player’s team. GF*100/(GF+GA)\n\n\n\n\n\n\nA scoring chance, as originally defined by War-on-Ice.\nEach shot attempt (Corsi) taken in the offensive zone is assigned a value based on the area of the zone in which it was recorded. Attempts made from the attacking team’s neutral or defensive zones are excluded.\n\nAttempts from the yellow areas are assigned a value of 1\nAttempts from the red areas are assigned a value of 2\nAttempts in the green area are assigned a value of 3\n\nAdjustments: - Add 1 to this value if the attempt is considered a rush shot or a rebound - A rebound is any attempt made within 3 seconds of another blocked, missed or saved attempt without a stoppage in play in between - A rush shot is any attempt within 4 seconds of any event in the neutral or defensive zone without a stoppage in play in between - Decrease this value by 1 if it was a blocked shot - Any attempt with a score of 2 or higher is considered a scoring chance\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nSCF\nCount of Scoring Chances for that player’s team while that player is on the ice.\n\n\nSCA\nCount of Scoring Chances against that player’s team while that player is on the ice.\n\n\nSCF%\nPercentage of total Scoring Chances while that player is on the ice that are for that player’s team. SCF*100/(SCF+SCA)\n\n\n\n\n\n\nA scoring chance with a score of 3 or higher.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nHDCF\nCount of High Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nHDCA\nCount of High Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nHDCF%\nPercentage of total High Danger Scoring Chances while that player is on the ice that are for that player’s team. HDCF*100/(HDCF+HDCA)\n\n\n\n\n\n\nGoals generated from High Danger Scoring Chances\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nHDGF\nCount of Goals off of High Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nHDGA\nCount of Goals off of High Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nHDGF%\nPercentage of High Danger Goals while that player is on the ice that are for that player’s team. HDGF*100/(HDGF+HDGA)\n\n\n\n\n\n\nA scoring chance with a score of exactly 2.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nMDCF\nCount of Medium Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nMDCA\nCount of Medium Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nMDCF%\nPercentage of total Medium Danger Scoring Chances while that player is on the ice that are for that player’s team. MDCF*100/(MDCF+MDCA)\n\n\n\n\n\n\nGoals generated from Medium Danger Scoring Chances\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nMDGF\nCount of Goals off of Medium Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nMDGA\nCount of Goals off of Medium Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nMDGF%\nPercentage of Medium Danger Goals while that player is on the ice that are for that player’s team. MDGF*100/(MDGF+MDGA)\n\n\n\n\n\n\nA scoring chance with a score of 1 or less. Does not include any attempts from the attacking team’s neutral or defensive zone.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLDCF\nCount of Low Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nLDCA\nCount of Low Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nLDCF%\nPercentage of total Low Danger Scoring Chances while that player is on the ice that are for that player’s team. LDCF*100/(LDCF+LDCA)\n\n\n\n\n\n\nGoals generated from Low Danger Scoring Chances\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLDGF\nCount of Goals off of Low Danger Scoring Chances for that player’s team while that player is on the ice.\n\n\nLDGA\nCount of Goals off of Low Danger Scoring Chances against that player’s team while that player is on the ice.\n\n\nLDGF%\nPercentage of Low Danger Goals while that player is on the ice that are for that player’s team. LDGF*100/(LDGF+LDGA)\n\n\n\n\n\n\nExpected goals (xG) is a statistical measure that evaluates shot quality by assigning a goal probability to each shot attempt.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nxGF\nExpected Goals For. The sum of the probability values of all shot attempts for that player’s team while that player is on the ice. Represents the number of goals the team should have scored based on shot quality.\n\n\nxGA\nExpected Goals Against. The sum of the probability values of all shot attempts against that player’s team while that player is on the ice. Represents the number of goals the team should have conceded based on shot quality.\n\n\nxGF%\nExpected Goals Percentage. The percentage of the total expected goals while that player is on the ice that are for that player’s team. xGF*100/(xGF+xGA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nSH%\nPercentage of Shots for that player’s team while that player is on the ice that were Goals. GF*100/SF\n\n\nSV%\nPercentage of Shots against that player’s team while that player is on the ice that were not Goals. GA*100/SA\n\n\nPDO\nShooting percentage plus save percentage. (GF/SF)+(GA/SA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nOff. Zone Starts\nNumber of shifts for the player that started with an offensive zone faceoff.\n\n\nNeu. Zone Starts\nNumber of shifts for the player that started with a neutral zone faceoff.\n\n\nDef. Zone Starts\nNumber of shifts for the player that started with a defensive zone faceoff.\n\n\nOn The Fly Starts\nNumber of shifts for the player that started during play (without a faceoff).\n\n\nOff. Zone Start %\nPercentage of starts for the player that were Offensive Zone Starts, excluding Neutral Zone and On The Fly Starts. Off. Zone Starts*100/(Off. Zone Starts+Def. Zone Starts)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nOff. Zone Faceoffs\nNumber of faceoffs in the offensive zone for which the player was on the ice.\n\n\nNeu. Zone Faceoffs\nNumber of faceoffs in the neutral zone for which the player was on the ice.\n\n\nDef. Zone Faceoffs\nNumber of faceoffs in the defensive zone for which the player was on the ice.\n\n\nOff. Zone Faceoff %\nPercentage of faceoffs in the offensive zone for which the player was on the ice, excluding neutral zone faceoffs. Off. Zone Faceoffs*100/(Off. Zone Faceoffs+Def. Zone Faceoffs)"
  },
  {
    "objectID": "posts/A5-TabularData/DataDictionary.html#player-individual-statistics",
    "href": "posts/A5-TabularData/DataDictionary.html#player-individual-statistics",
    "title": "DataDictionary",
    "section": "",
    "text": "All player statistics are presented within the scope of the selected filters.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPlayer\nPlayer name.\n\n\nTeam\nTeam or teams that the player has played for. Not displayed when filtering for specific teams.\n\n\nPosition\nPosition or positions that the player has been listed as playing by the NHL.\n\n\nGP\nGames played.\n\n\nTOI\nTotal amount of time played.\n\n\nTOI/GP\nAmount of time played per game. Not displayed when filtering for Counts.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGoals\nGoals scored by the player, outside of the shootout.\n\n\nAssists\nAny assist by the player.\n\n\nFirst Assists\nPrimary assists by the player.\n\n\nSecond Assists\nSecondary assists by the player.\n\n\nTotal Points\nGoals scored and assists by the player, outside of the shootout.\n\n\nIPP\nIndividual Point Percentage, the percentage of goals for that player’s team while that player is on the ice that the player earned a point on. Total Points/Goals For\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nShots\nAny shot attempt on net (goals and shots on net) by the player, outside of the shootout.\n\n\nSH%\nPercentage of Shots by the player that were Goals. Goals*100/Shots\n\n\niCF\nAny shot attempt (goals, shots on net, misses and blocks) by the player, outside of the shootout.\n\n\niFF\nAny unblocked shot attempt (goals, shots on net and misses) by the player, outside of the shootout.\n\n\niSCF\nAny scoring chance by the player, outside of the shootout.\n\n\niHDCF\nAny high danger scoring chance by the player, outside of the shootout.\n\n\nRush Attempts\nAny rush shot attempt (goals, shots on net, misses and blocks) by the player, outside of the shootout.\n\n\nRebounds Created\nAny shot attempt (shots on net, misses and blocks) that results in a rebound shot attempt.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPIM\nPenalty minutes assessed to the player.\n\n\nTotal Penalties\nTotal number of penalties taken by the player\n\n\nMinor\nNumber of minor penalties taken by the player.\n\n\nMajor\nNumber of major penalties taken by the player.\n\n\nMisconduct\nNumber of 10 minute or game misconducts taken by the player.\n\n\nPenalties Drawn\nNumber of penalties committed against the player.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGiveaways\nNumber of unforced turnovers made by the player.\n\n\nTakeaways\nNumber of times the player takes the puck away from the opposition.\n\n\nHits\nNumber of hits made by the player.\n\n\nHits Taken\nNumber of hits taken by the player.\n\n\nShots Blocked\nNumber of opposition shot attempts blocked by the player.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFaceoffs Won\nFaceoffs won by the player\n\n\nFaceoffs Lost\nFaceoffs lost by the player\n\n\nFaceoff %\nPercentage of total faceoffs taken by that player that were won Faceoffs Won*100/(Faceoffs Won+Faceoffs Lost)"
  },
  {
    "objectID": "posts/A5-TabularData/DataDictionary.html#goalie-statistics",
    "href": "posts/A5-TabularData/DataDictionary.html#goalie-statistics",
    "title": "DataDictionary",
    "section": "",
    "text": "All player statistics are presented within the scope of the selected filters.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nPlayer\nPlayer name.\n\n\nTeam\nTeam or teams that the player has played for. Not displayed when filtering for specific teams.\n\n\nGP\nGames played.\n\n\nTOI\nTotal amount of time played.\n\n\nTOI/GP\nAmount of time played per game. Not displayed when filtering for Counts.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nShots Against\nShots on Goal (Shots and Goals) faced by the goalie.\n\n\nSaves\nSaves made by the goalie.\n\n\nGoals Against\nGoals allowed by the goalie.\n\n\nSV%\nPercentage of Shots Against that were Saves. (Saves/Shots Against)\n\n\nGAA\nGoals allowed per 60 minutes of play. Goals Against*60/TOI\n\n\nGSAA\nGoals Saved Above Average, the difference between the goalie’s Goals Against and a Goals Against with the same Shots Against and the average SV%. Average SV%*Shots Against - Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nHD Shots Against\nShots on Goal (Shots and Goals) that are High Danger Chances faced by the goalie.\n\n\nHD Saves\nSaves on HD Shots Against made by the goalie.\n\n\nHD Goals Against\nGoals from HD Shots Against allowed by the goalie.\n\n\nHDSV%\nPercentage of HD Shots Against that were Saves. (HD Saves/HD Shots Against)\n\n\nHDGAA\nHD Goals allowed per 60 minutes of play. HD Goals Against*60/TOI\n\n\nHDGSAA\nHD Goals Saved Above Average, the difference between the goalie’s HD Goals Against and a HD Goals Against with the same HD Shots Against and the average HDSV%. Average HDSV%*HD Shots Against - HD Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nMD Shots Against\nShots on Goal (Shots and Goals) that are Medium Danger Chances faced by the goalie.\n\n\nMD Saves\nSaves on MD Shots Against made by the goalie.\n\n\nMD Goals Against\nGoals from MD Shots Against allowed by the goalie.\n\n\nMDSV%\nPercentage of MD Shots Against that were Saves. (MD Saves/MD Shots Against)\n\n\nMDGAA\nMD Goals allowed per 60 minutes of play. MD Goals Against*60/TOI\n\n\nMDGSAA\nMD Goals Saved Above Average, the difference between the goalie’s MD Goals Against and a MD Goals Against with the same MD Shots Against and the average MDSV%. Average MDSV%*MD Shots Against - MD Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLD Shots Against\nShots on Goal (Shots and Goals) that are Low Danger Chances faced by the goalie.\n\n\nLD Saves\nSaves on LD Shots Against made by the goalie.\n\n\nLD Goals Against\nGoals from LD Shots Against allowed by the goalie.\n\n\nLDSV%\nPercentage of LD Shots Against that were Saves. (LD Saves/LD Shots Against)\n\n\nLDGAA\nLD Goals allowed per 60 minutes of play. LD Goals Against*60/TOI\n\n\nLDGSAA\nLD Goals Saved Above Average, the difference between the goalie’s LD Goals Against and a LD Goals Against with the same LD Shots Against and the average LDSV%. Average LDSV%*LD Shots Against - LD Goals Against\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nRush Attempts Against\nNumber of Shot Attempts (including missed and block shots) that the goalie faced that are considered Rush Attempts\n\n\nRebound Attempts Against\nNumber of Shot Attempts (including missed and block shots) that the goalie faced that are considered Rebound Attempts\n\n\nAvg. Shot Distance\nAverage distance (in feet) of shots on net faced by the goalie\n\n\nAvg. Goal Distance\nAverage distance (in feet) of goals allowed by the goalie"
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html",
    "href": "posts/A4-MarksChannels/index.html",
    "title": "ASSIGNMENT 4",
    "section": "",
    "text": "It’s one thing to “learn” about the theory of marks and channels for data visualization. It’s an entirely different thing to LEARN these concepts by IMPLEMENTING them in your own visualizations. In this assignment, you will do exactly that. Create some simple visualizations that highlight some of the key concepts from this section of the class."
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#summary",
    "href": "posts/A4-MarksChannels/index.html#summary",
    "title": "ASSIGNMENT 4",
    "section": "",
    "text": "It’s one thing to “learn” about the theory of marks and channels for data visualization. It’s an entirely different thing to LEARN these concepts by IMPLEMENTING them in your own visualizations. In this assignment, you will do exactly that. Create some simple visualizations that highlight some of the key concepts from this section of the class."
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#a-new-portfolio-post",
    "href": "posts/A4-MarksChannels/index.html#a-new-portfolio-post",
    "title": "ASSIGNMENT 4",
    "section": "A NEW PORTFOLIO POST",
    "text": "A NEW PORTFOLIO POST\nThis assignment will appear as a new post in your nascent data science portfolio. To create a new post, you need to navigate to your posts directory in your RStudio Project. I do this using the Files tab in the lower right quadrant of RStudio.\n\n\nMake a new directory within posts called MarksChannels .\nClick on that new directory to enter it and then use that create a new blank file button and choose Quarto Document.\nName the new file index.qmd .\nAdd YAML text to the top of the file. In particular, add the following:\n\ntitle: “ASSIGNMENT 4”\nsubtitle: “Marks and Channels”\nauthor: “YOUR NAME”\ndate: “2024-02-08”\ncategories: [Assignment, DataViz]\nimage: “some fun image you put in that new directory.png”\ncode-fold: true\ncode-tools: true\ndescription: “A clever description that describes the stuff”"
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#make-your-portfolio-less-terrible",
    "href": "posts/A4-MarksChannels/index.html#make-your-portfolio-less-terrible",
    "title": "ASSIGNMENT 4",
    "section": "MAKE YOUR PORTFOLIO LESS TERRIBLE",
    "text": "MAKE YOUR PORTFOLIO LESS TERRIBLE\nWhile we are at it, let’s improve the overall look and structure of your portfolio. Navigate to the about.qmd file in your portfolio root directory and open it in RStudio. Use the information found here to customize your About page.\nAT MINIMUM , I want an About page that contains your name, a brief description about you, a picture of yourself, a link to your github, and a section on your Education."
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#marks-and-channels",
    "href": "posts/A4-MarksChannels/index.html#marks-and-channels",
    "title": "ASSIGNMENT 4",
    "section": "MARKS AND CHANNELS",
    "text": "MARKS AND CHANNELS\nBack to your new post!\nUse that shiny new index.qmd file to perform the tasks below. Every time you create a figure, it needs a caption. The text in that section of your assignment should also briefly describe the data set you are using, especially the attributes used for the visualization. In addition, make sure the visualization task actually requires the particular concept. For example, don’t just make a scatterplot with one red dot for the Popout exercise. You need to describe a task that requires we IDENTIFY that point.\n\nExpressiveness and Effectiveness\nFrom Munzner, 5.4.1:\nThe expressiveness principle: Visual encoding should express all of, and only, the information in the dataset attributes. Ordered data should be shown in a way that our perceptual system intrinsically senses as ordered. Conversely, unordered data should not be shown in a way that perceptually implies an ordering that does not exist.\nThe effectiveness principle: The importance of the attribute should match the salience of the channel (its noticeablity). The most important attributes should be encoded with the most effective channels.\nUsing whatever data set you choose, create a visualization called Figure 1 that adheres to these two principles. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 2 that violates these principles in the most extreme ways you can imagine while still having the visualization be recognizibly the same as the first. Write a caption that explains your choices of marks and channels and why they are bad compared to Figure 1.\n\n\nDiscriminability\nUsing whatever data set you choose (it need not be the same data used for Figures 1 and 2), create a visualization called Figure 3 that uses a magnitude channel and a number of bins that facilitate discriminiability of an attribute. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 4 that uses this same channel for WAY TOO MANY BINS, violating the guidelines for discriminability. Write a caption that explains why this number of bins is bad compared to Figure 3.\n\n\nSeparability\nUsing whatever data set you choose (it need not be the same data used for Figures 1-4), create a visualization called Figure 5 that uses a two or more channels to encode two or more attributes while maintainig separability. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 6 that uses channels that are integral, or at least much less separable. Write a caption that explains why choosing these channels is bad compared to Figure 5.\n\n\nPopout\nUsing whatever data set you choose (it need not be the same data used for Figures 1-6… you get the idea), create a visualization called Figure 7 that effectively uses the concept of popout. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 8 that makes the identification task in Figure 7 much more difficult. Write a caption that explains why Figure 8 sucks compared to Figure 7."
  },
  {
    "objectID": "posts/A2-YourData/index.html",
    "href": "posts/A2-YourData/index.html",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "",
    "text": "A key feature of this course is that students should be using their own data whenever possible. This is critical to forging a learning experience that is customized to each student’s aspirations and the eccentricities of their chosen research domain. This assignment begins the process of helping you identify the data sets with which you want to work, and aligns with the notion of understanding the concepts of Data Semantics and Data Abstraction."
  },
  {
    "objectID": "posts/A2-YourData/index.html#summary",
    "href": "posts/A2-YourData/index.html#summary",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "",
    "text": "A key feature of this course is that students should be using their own data whenever possible. This is critical to forging a learning experience that is customized to each student’s aspirations and the eccentricities of their chosen research domain. This assignment begins the process of helping you identify the data sets with which you want to work, and aligns with the notion of understanding the concepts of Data Semantics and Data Abstraction."
  },
  {
    "objectID": "posts/A2-YourData/index.html#assignment",
    "href": "posts/A2-YourData/index.html#assignment",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThe basic structure of this assignment is for you to identify, import, describe, and host a data set. I’ll break down the specifics for each of these actions below.\n\nIdentify a Data Set\nThe main criteria is that the data set has to matter to you in some way. Often, this will mean that it is your data set. It was collected by you and has a central role in your current or past graduate research. Awesome! Another scenario is that the data you want to use comes from your current job. Maybe it isn’t part of a research project, but you are motivated to learn how to better visualize the data or you are very interested in learning more about it. Also Awesome!\nSome of you might not have your own data. Perhaps you have just started your graduate training. Maybe your job doesn’t yet have data that you need to work with. No Problem!\nIt is perfectly fine to find publicly available data sets online. As long as the data set is interesting to you! You just need to make sure that the data:\n\nAre publicly available.\nAre not restricted by some kind of license or copyright.\nDo not contain private information.\nAre not covered by HIPPA, FERPA, CMMC, or other federal regulations related to data.\n\nIf you need help finding a data set, just let me know.\nSome fun potential categories for data sources include:\n\nSports Analytics from your favorite sport or team.\nPublicly available genomics data bases.\nKeggle.\nThe movie data base.\nClassic data sets from your field.\n\n\n\nImport the Data Set\nThis one is probably straightforward if your data set comes from your own research and lives on your local hard drive already.\n\n\nDescribe the Data Set\nThis is the bulk of the assignment. I want you to use the framework described in Dr. Munzner’s textbook to understand your data set and describe it to someone who is unfamiliar with your work. The basis of this approach is descibed in this lecture. In addition, this figure from the textbook summarizes the kinds of data types, data set types, and attribute types you might have in your data:\n\n\n\nBONUS OBJECTIVE: Host your Data Set\nUltimately, we are moving toward each of you hosting your assignments within an online repository that can serve as your data science portfolio. For this course, we are going to assume this is GitHub. At the very least, I want everyone to create (or dust off and log in to) your own GitHub account. We’ll try to use this assignment to set up a project repository, and perhaps even a simple web site using GitHub pages."
  },
  {
    "objectID": "posts/A2-YourData/index.html#resources",
    "href": "posts/A2-YourData/index.html#resources",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nBarrie attempts this assignment in Tutorial 4."
  },
  {
    "objectID": "posts/A10-FinalProject/index.html",
    "href": "posts/A10-FinalProject/index.html",
    "title": "MIDTERM AND FINAL PROJECT",
    "section": "",
    "text": "This assignment provides you the opportunity to synthesize all of the concepts we’ve covered in the course to date. The basic framework is that you will create a COMPLETE data visualization BLOG post that is suitable as a showcase component of your Data Science Portfolio. The point is to SHOW people your skills."
  },
  {
    "objectID": "posts/A10-FinalProject/index.html#overview",
    "href": "posts/A10-FinalProject/index.html#overview",
    "title": "MIDTERM AND FINAL PROJECT",
    "section": "",
    "text": "This assignment provides you the opportunity to synthesize all of the concepts we’ve covered in the course to date. The basic framework is that you will create a COMPLETE data visualization BLOG post that is suitable as a showcase component of your Data Science Portfolio. The point is to SHOW people your skills."
  },
  {
    "objectID": "posts/A10-FinalProject/index.html#structure",
    "href": "posts/A10-FinalProject/index.html#structure",
    "title": "MIDTERM AND FINAL PROJECT",
    "section": "STRUCTURE",
    "text": "STRUCTURE\nThe basic formatting guidelines for this assignment are:\n\nInclude code fold or code tools options (or both) that allow users to view and copy your code while maintaining overall readability of your post.\nSuppress all output and warnings that might distract from your visualizations and writing.\nProperly title your assignment. The main title should be “BCB 520 - Final Project”, and the subtitle should be a descriptive title related to your question or topic.\nInclude author, date, categories, and a description in your YAML header.\nWrite clear, complete sentences for a target audience with some scientific background but little training in your specific discipline.\nInclude references if appropriate and use hyperlinks to external sources of data, inspiration, or examples.\nUse the header hierarchy and create a sensible document outline with white space. Format for readability! Use bold and italic fonts to emphasize things! Use color by customizing your .css file!\n\nIn addition to the above formatting guidelines, your portfolio post must conform to my feedback and suggestions regarding your midterm assignment. Unless you have a very good reason to deviate, use the following general format:\n\nPreamble\nWrite a brief paragraph describing the primary question or purpose of the post. Ideally, the concept should be challenging enough that it requires at least two visualizations that use different idioms (ie. don’t just make two scatterplots with different variables). The concept should also be challenging enough that it captures the interest of the reader (i.e. a plot of height and weight that shows they are correlated is trivial and not appropriate). The best approach is to explore a topic or question in which YOU are very interested.\n\n\nData\nWrite a summary of the data sources you will use. Include a Data Dictionary table that fully describes each individual data file used. You may use your own research data or publicly available data from any source you like (with attribution). There aren’t any minimum or maximum data set size requirements, other than you need something big enough to be interesting and not so big that we don’t have a supercomputer capable of creating your visualization.\nNEW REQUIREMENT: Your assignment must feature one of the two new data types we have considered since the midterm: NETWORK DATA or SPATIAL DATA.\n\n\nVisualizations\nCreate your visualizations and include text that explains any steps or design choices. Be sure to include clearly labeled axes and a concise but complete figure caption for each visualization. Make deliberate choices for color palettes, point marks, line types, etc. Demonstrate that you understand the concepts we have covered!\n\n\nConclusions or Summary\nAnswer your research question. Draw a conclusion or inference related to your topic. Summarize your results. What new questions have emerged as a result of your visualizations? What interesting next steps have emerged?"
  },
  {
    "objectID": "posts/A10-FinalProject/index.html#rubric",
    "href": "posts/A10-FinalProject/index.html#rubric",
    "title": "MIDTERM AND FINAL PROJECT",
    "section": "RUBRIC",
    "text": "RUBRIC\nI will evaluate the following for your portfolio post:\n1. Clarity of writing (15%): Complete, clear sentences. Good Grammar. Understandable to target audience. Logical flow of ideas.\n2. Adherence to format (10%): Did you follow directions?\n3. Topic suitability (15%): Is the topic interesting? Are the visualizations challenging and interesting enough to showcase your skills?\n4. Viz Execution (40%): Are the visualizations effective? Do they adhere to the principles of effectiveness? Are choices for idiom, marks, channels, etc made deliberately and well justified?\n5. Creativity (20%): Did you push your boundaries and learn new techniques? Is the overall post compelling and interesting? Are the visualizations inspiring, creative, unique, and generally impressive? If I were recruiting a new data scientist (and I often am), would this portfolio post impress me, or would it damage your candidacy during review?"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#dataset-types-networks",
    "href": "posts/L8-NetworkData/index.html#dataset-types-networks",
    "title": "LECTURE 8 - Network Data",
    "section": "DATASET TYPES: NETWORKS",
    "text": "DATASET TYPES: NETWORKS\n\n\n\nNetworks (also called graphs)\n\nmodel relationships between things\ntwo kinds of items, both can have attributes\n\nnodes\nlinks\n\n\ntrees are a special case\n\nno cycles\none parent per node"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#arrange-networkstrees",
    "href": "posts/L8-NetworkData/index.html#arrange-networkstrees",
    "title": "LECTURE 8 - Network Data",
    "section": "ARRANGE NETWORKS/TREES",
    "text": "ARRANGE NETWORKS/TREES"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#network-tasks",
    "href": "posts/L8-NetworkData/index.html#network-tasks",
    "title": "LECTURE 8 - Network Data",
    "section": "NETWORK TASKS",
    "text": "NETWORK TASKS\n\n\n\nTopology based tasks\n\nfind paths\nfind (topological) neighbors\ncompare centrality / importance measures\nidentify clusters / communities\n\nAttribute based tasks (similar to tabular data)\n\nfind distributions, etc…\n\nCombination tasks, incorporating both\n\nexample: find friends-of-friends who like cats\n\ntopology: find all adjacent nodes of given node\nattributes: check if has-pet (node attribute) == cat"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#node-link-diagrams",
    "href": "posts/L8-NetworkData/index.html#node-link-diagrams",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK DIAGRAMS",
    "text": "NODE-LINK DIAGRAMS\n\n\n\nNodes: point marks\nLinks: line marks\n\nstraight lines or arcs\nconnections between nodes\n\nIntuitive & familiar\n\nmost common\nmany, many variants"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#node-link-layout",
    "href": "posts/L8-NetworkData/index.html#node-link-layout",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK LAYOUT",
    "text": "NODE-LINK LAYOUT\n\n\n\nMinimize:\n\nedge crossings, node overlaps\ndistances between topological neighbor nodes\ntotal drawing area\nedge bends\n\nMaximize:\n\nangular distance between different edges\naspect ratio disparities\n\nEmphasize symmetry\n\nsimilar graph structures should look similar in layout"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#node-link-challenges",
    "href": "posts/L8-NetworkData/index.html#node-link-challenges",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK CHALLENGES",
    "text": "NODE-LINK CHALLENGES\nMany criteria directly conflict with each other."
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#optimization-based-layouts",
    "href": "posts/L8-NetworkData/index.html#optimization-based-layouts",
    "title": "LECTURE 8 - Network Data",
    "section": "OPTIMIZATION BASED LAYOUTS",
    "text": "OPTIMIZATION BASED LAYOUTS\n\nFormulate layout problem as an optimization problem\nConvert criteria into a weighted cost function\n\nF(layout) = a(crossing counts) + b(drawing space used)+…\n\nUse known optimization techniques to find layout at minimal cost\n\nenergy-based physics models\nforce-directed placement\nspring embedders"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#force-directed-placement",
    "href": "posts/L8-NetworkData/index.html#force-directed-placement",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED PLACEMENT",
    "text": "FORCE DIRECTED PLACEMENT\n\n\n\nPhysics model\n\nLinks = springs pull together\nNodes = magnets repulse apart\n\nAlgorithm\n\nPlace vertices in random locations\nWhile not equilibrium, calculate force on vertex as sum of:\n\npairwise repulsion of all nodes\nattraction between connected nodes\n\nMove vertex by c(vertex_force)"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#force-directed-proscons",
    "href": "posts/L8-NetworkData/index.html#force-directed-proscons",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED PROS/CONS",
    "text": "FORCE DIRECTED PROS/CONS\n\n\n\nStrengths\n\nreasonable layout for small, sparse graphs\nclusters typically visible\nedge length uniformity\n\nWeaknesses\n\nnondeterministic\ncomputationally expensive: O(n^3) for n nodes\n\neach step is n^2, takes ~n cycles to reach equilibrium\nnaive FD doesn’t scale well beyond 1K nodes\n\niterative progress: engaging but distracting"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#force-directed-idiom",
    "href": "posts/L8-NetworkData/index.html#force-directed-idiom",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED IDIOM",
    "text": "FORCE DIRECTED IDIOM\n\n\n\nVisual Encoding:. link connection marks, node point marks\nConsiderations:\n\nSpatial position: no meaning directly encoded, left free to minimize crossings\nProximity semantics:\n\nsometimes meaningful, sometimes an artifact of layout algorithm\ntension with length (long edges more visually salient than short)\n\n\nTasks:\n\nexplore topology; locate paths and/or clusters\n\nScalability\n\nnode/edge density E &lt; 4N"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#force-directed-examples",
    "href": "posts/L8-NetworkData/index.html#force-directed-examples",
    "title": "LECTURE 8 - Network Data",
    "section": "FORCE DIRECTED EXAMPLES",
    "text": "FORCE DIRECTED EXAMPLES\nForce Directed Graph\nDisjoint Force Directed\nOBSERVABLE D3 LIBRARY"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#circular-layouts-arc-diagrams",
    "href": "posts/L8-NetworkData/index.html#circular-layouts-arc-diagrams",
    "title": "LECTURE 8 - Network Data",
    "section": "CIRCULAR LAYOUTS / ARC DIAGRAMS",
    "text": "CIRCULAR LAYOUTS / ARC DIAGRAMS\n\nRestricted node-link layouts: lay out nodes around circle or along line\nData\n\noriginal: network\nderived: node ordering attribute (global computation)\n\nConsiderations: node ordering crucial to avoid excessive clutter from edge crossings\n\nexamples: before & after barycentric ordering. The goal of barycentric ordering is to optimize this layout by considering the positions of the nodes based on their connections, thus reducing the overlap and crossings of the arcs that represent the links between nodes.\n\n\nArc Diagram"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#adjacency-matrix",
    "href": "posts/L8-NetworkData/index.html#adjacency-matrix",
    "title": "LECTURE 8 - Network Data",
    "section": "ADJACENCY MATRIX",
    "text": "ADJACENCY MATRIX\nMatrix derived from network"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#node-order",
    "href": "posts/L8-NetworkData/index.html#node-order",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE ORDER",
    "text": "NODE ORDER\nANIMATED VERSION built with D3 by Mike Bostock."
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#matrix-proscons",
    "href": "posts/L8-NetworkData/index.html#matrix-proscons",
    "title": "LECTURE 8 - Network Data",
    "section": "MATRIX PROS/CONS",
    "text": "MATRIX PROS/CONS\nGood for topology tasks related to neighborhoods. Bad for topology tasks related to paths."
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#node-link-vs-matrix",
    "href": "posts/L8-NetworkData/index.html#node-link-vs-matrix",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK vs MATRIX",
    "text": "NODE-LINK vs MATRIX\n\nNode-link diagram strengths\n\ntopology understanding, path tracing\nintuitive, flexible, no training needed\n\nAdjacency matrix strengths\n\nfocus on edges rather than nodes\nlayout straightforward (reordering needed)\npredictability, scalability\nsome topology tasks trainable\n\nEmpirical studies:\n\nnode-link best for small networks\nmatrix best for large networks if tasks don’t involve path tracing"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#trees",
    "href": "posts/L8-NetworkData/index.html#trees",
    "title": "LECTURE 8 - Network Data",
    "section": "TREES",
    "text": "TREES"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#node-link-trees",
    "href": "posts/L8-NetworkData/index.html#node-link-trees",
    "title": "LECTURE 8 - Network Data",
    "section": "NODE-LINK TREES",
    "text": "NODE-LINK TREES\n\n\n\nReingold-Tilford\n\ntidy drawings of trees\nexploit parent/child structure\nallocate space: compact but without overlap\nrectilinear and radial variants\n\n\nThese are COOOOOL!"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#radial-node-link-tree",
    "href": "posts/L8-NetworkData/index.html#radial-node-link-tree",
    "title": "LECTURE 8 - Network Data",
    "section": "RADIAL NODE-LINK TREE",
    "text": "RADIAL NODE-LINK TREE\n\n\n\nData: tree\nEncoding\n\nlink connection marks\npoint node marks\nradial axis orientation\n\nangular proximity: siblings\ndistance from center: depth in tree\n\n\nTasks:\n\nunderstanding topology, following paths\n\nScalability:\n\n1K - 10K nodes (with/without labels)\n\n\n\n EXAMPLE"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#connection-and-containment",
    "href": "posts/L8-NetworkData/index.html#connection-and-containment",
    "title": "LECTURE 8 - Network Data",
    "section": "CONNECTION AND CONTAINMENT",
    "text": "CONNECTION AND CONTAINMENT\n\n\n\nMarks as links (vs. nodes)\n\ncommon case in network drawing\n1D case: connection\n\nex: all node-link diagrams\nemphasizes topology, path tracing\nnetworks and trees\n\n2D case: containment\n\nex: all treemap variants\nemphasizes attribute values at leaves (size coding)\nonly trees"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#treemap",
    "href": "posts/L8-NetworkData/index.html#treemap",
    "title": "LECTURE 8 - Network Data",
    "section": "TREEMAP",
    "text": "TREEMAP\n\n\n\nData: tree with 1 quantitative attribute at leaf nodes\nEncoding:\n\narea containment marks for hierarchical structure\nrectilinear orientation\nsize encodes quant attrib\n\nTasks:\n\nquery attribute at leaf nodes\nex: disk space usage within filesystem\n\nScalability:\n\n1M leaf nodes\n\n\n\n\nTREEMAP\nVORONOI TREEMAP"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#implicit-tree-layouts",
    "href": "posts/L8-NetworkData/index.html#implicit-tree-layouts",
    "title": "LECTURE 8 - Network Data",
    "section": "IMPLICIT TREE LAYOUTS",
    "text": "IMPLICIT TREE LAYOUTS\nAn alternative to connection and containment is position. These show parent-child relationships only through relative positions."
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#tree-idioms",
    "href": "posts/L8-NetworkData/index.html#tree-idioms",
    "title": "LECTURE 8 - Network Data",
    "section": "TREE IDIOMS",
    "text": "TREE IDIOMS\n\n\n\nData shown\n\nlink relationships\ntree depth\nsibling order\n\nDesign choices\n\nconnection vs containment link marks\nrectilinear vs radial layout\nspatial position channels\n\nConsiderations\n\nredundant? arbitrary?\ninformation density?\n\navoid wasting space\nconsider where to fit labels!"
  },
  {
    "objectID": "posts/L8-NetworkData/index.html#arrange-networkstrees-1",
    "href": "posts/L8-NetworkData/index.html#arrange-networkstrees-1",
    "title": "LECTURE 8 - Network Data",
    "section": "ARRANGE NETWORKS/TREES",
    "text": "ARRANGE NETWORKS/TREES\n\n\n\n\nHOME"
  },
  {
    "objectID": "posts/A7-SpatialData/index.html",
    "href": "posts/A7-SpatialData/index.html",
    "title": "Practice with Spatial Data",
    "section": "",
    "text": "In this assignment, we’ll consider some of the tools and techniques for visualizing spatial data. Spatial data comes in two broad categories, geographic and spatial fields. Let’s practice a few visualizations to get a feel for how these things work!"
  },
  {
    "objectID": "posts/A7-SpatialData/index.html#overview",
    "href": "posts/A7-SpatialData/index.html#overview",
    "title": "Practice with Spatial Data",
    "section": "",
    "text": "In this assignment, we’ll consider some of the tools and techniques for visualizing spatial data. Spatial data comes in two broad categories, geographic and spatial fields. Let’s practice a few visualizations to get a feel for how these things work!"
  },
  {
    "objectID": "posts/A7-SpatialData/index.html#geographic-maps",
    "href": "posts/A7-SpatialData/index.html#geographic-maps",
    "title": "Practice with Spatial Data",
    "section": "GEOGRAPHIC MAPS!",
    "text": "GEOGRAPHIC MAPS!\nIn class I bet Ronald that he would end up creating some kind of map based visualization before he graduated with his PHD. This is because he works on Malaria - a terrible disease with a strong spatial component to its risk levels. Let’s get some Malaria data and map it!\nThe data I obtained were from the Malaria Atlas. I downloaded a csv for 10 years of data for all the countries the had on file.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readxl)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n\n\nAttaching package: 'rnaturalearthdata'\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110\n\n\nCode\nlibrary(dplyr)\n\nMalaria &lt;- read.csv(\"National_Unit_data.csv\")\n\nIncidence&lt;- Malaria%&gt;%\n  filter(Metric == \"Infection Prevalence\")%&gt;%\n  mutate(Prevalence = Value, Year = as.factor(Year))\n\n#%&gt;%\n  #select(c(ISO3, Prevalence, Year))\n\n\nNow I’m going to use the rnaturalearth package to create contry polygons. Then I’ll add the Malaria data to that data frame.\n\n\nCode\nworld_map &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data &lt;- world_map %&gt;%\n  left_join(Incidence, by = c(\"iso_a3\" = \"ISO3\"))%&gt;%\n  filter(!is.na(Prevalence))\n\n\nNow I will make a plot!\n\n\nCode\nlibrary(gganimate)\nlibrary(transformr)\nlibrary(magick)\n\n\nLinking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\n\nCode\nlibrary(gifski)\n# \nggplot() +\n  geom_sf(data = map_data%&gt;%\n            filter(continent==\"Africa\"),\n          aes(fill = Prevalence)) +\n  scale_fill_gradient(low = \"white\", high = \"red\", na.value = \"gray\", name = \"Malaria Prevalence\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Malaria Prevalence by Country\")\n\n\n\n\n\n\n\n\n\nOh SNAP! We made a MAP!\nHey! That rhymes!\nWhat is missing? Units? Is that actually prevalence? We sure left a lot of data on the table? Can we add some kind of time thing? Change the variable?"
  },
  {
    "objectID": "posts/A7-SpatialData/index.html#spatial-fields",
    "href": "posts/A7-SpatialData/index.html#spatial-fields",
    "title": "Practice with Spatial Data",
    "section": "SPATIAL FIELDS",
    "text": "SPATIAL FIELDS\nSpatial field data refers to data that has a continuous spatial distribution and can be measured at any location within the defined area. Here are some interesting examples of spatial field data:\n\nAir temperature: Air temperature data collected from weather stations or remote sensing technologies can be used to create temperature maps or to study climate change, urban heat islands, and other environmental phenomena. TROPICAL CYCLONE!\nPrecipitation: Rainfall, snowfall, or other forms of precipitation data collected from weather stations or satellites can be used to study the hydrological cycle, flood risk, droughts, or water resource management.\nSoil moisture: Soil moisture data collected from in situ sensors or remote sensing technologies can be used to study agricultural productivity, irrigation management, droughts, and land degradation. Elevation data (Digital Elevation Models, DEMs):\nElevation data collected from satellite-based radar, LiDAR, or photogrammetry can be used to study topography, watershed delineation, flood risk, landslides, or geomorphology.\nVegetation indices: Indices like the Normalized Difference Vegetation Index (NDVI) or Enhanced Vegetation Index (EVI) derived from satellite imagery can be used to study vegetation health, land cover change, deforestation, agricultural productivity, and carbon sequestration.\nAir quality: Data on air pollutants like PM2.5, PM10, NO2, SO2, O3, and CO collected from ground-based monitors or satellites can be used to study the impact of pollution on human health, urban planning, or environmental policy.\nOceanographic data: Sea surface temperature, salinity, and chlorophyll-a concentration data collected from buoys, ships, or satellites can be used to study ocean currents, climate change, or marine ecosystems. OCEAN CURRENTS!\nPopulation density: Spatially explicit population density data can be used to study urbanization, migration patterns, infrastructure planning, or public health.\nLand use and land cover: Land use and land cover data collected from satellite imagery can be used to study urban growth, deforestation, habitat fragmentation, or landscape ecology.\nSeismic activity: Spatial distribution of earthquakes and their magnitudes can be used to study tectonics, fault zones, seismic hazards, or infrastructure resilience. DARK NIGHTS IN ANTAKYA\nSPORTS! Let’s check out a baseball example!\n\n\n\nCode\n#install.packages(\"baseballr\")\nlibrary(remotes)\n\nlibrary(devtools)\n\n\nLoading required package: usethis\n\n\n\nAttaching package: 'devtools'\n\n\nThe following objects are masked from 'package:remotes':\n\n    dev_package_deps, install_bioc, install_bitbucket, install_cran,\n    install_deps, install_dev, install_git, install_github,\n    install_gitlab, install_local, install_svn, install_url,\n    install_version, update_packages\n\n\nCode\n#install_github(\"bayesball/CalledStrike\")\n\n\n\n\nCode\nlibrary(CalledStrike)\n\n\nLoading required package: shiny\n\n\nLoading required package: baseballr\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n\nLoading required package: metR\n\n\n\nAttaching package: 'metR'\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nCode\nlibrary(baseballr)\n#ShinyDemo()\n\n\nThis example is the Pitch_Locations example from byaesball’s CalledStrike Github Repository\n\nIntroduction\nThere are three functions for visualizing pitch locations.\n\nThe function location_compare() will graph the pitch location for a data frame or list of data frames.\nThe function location_count() will show the locations of pitches for a specific pitcher on a particular count.\nThe function location_count_compare() will graph the pitch locations for a specific pitcher for several values of the count.\n\n\n\nData\nThe package includes the dataset sc_pitchers_2019 that contains Statcast data for 20 pitchers for the 2019 season.\n\n\nPitch Locations for a List\nSuppose we want to compare the locations of the fastballs thrown by Aaron Nola and Trevor Bauer.\nI find the subset of data I need and then create a list dividing the data by pitcher.\n\n\nCode\nd &lt;- filter(sc_pitchers_2019, \n            pitcher %in% c(605400, 545333),\n            pitch_type == \"FF\")\nds &lt;- split(d, d$pitcher)\nnames(ds) &lt;- c(\"Bauer\", \"Nola\")\n\n\nNow we can construct the graph.\n\n\nCode\nlocation_compare(ds)\n\n\nWarning: Removed 22 rows containing non-finite outside the scale range\n(`stat_density2d_filled()`).\n\n\n\n\n\n\n\n\n\n\n\nPitch Locations for a Specific Count\nSuppose we want to look at the locations of Aaron Nola’s pitches on a 0-0 count. I can find Nola’s MLBAM id number by use of the chadwick dataset (also included in the package) that contains the id numbers for all players.\n\n\nCode\nchadwick %&gt;% \n  filter(name_last == \"Nola\", name_first == \"Aaron\")\n\n\n  name_first name_last key_mlbam\n1      Aaron      Nola    605400\n\n\nTo produce the graph, type\n\n\nCode\nlocation_count(sc_pitchers_2019, \n               605400, \"Aaron Nola\", \"0-0\")\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density2d_filled()`).\n\n\n\n\n\n\n\n\n\n\n\nPitch Locations Across a Group of Counts\nSuppose we want to compare Nola’s pitch locations across the counts “0-0”, “1-0”, “0-1”, “0-2”\n\n\nCode\nlocation_count_compare(sc_pitchers_2019, \n               605400, \"Aaron Nola\", \n               \"R\", \"Offspeed\", \n               c(\"0-0\", \"1-0\", \"0-1\", \"0-2\"))\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density2d_filled()`).\n\n\n\n\n\n\n\n\n\nEND"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "This class will help students establish a core understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R and Python programming languages. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using the language of their choice. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\n\n\nI am maintaining the course here and I do not intend to use CANVAS this semester.\nSYLLABUS\nBarrie’s GitHub\n\n\nKM Github\nBC Github\nAM Github\nSD Github\nYG Github\nME Github\nIW Github\nCS Github\nJC Github\nEL Github\nAda\nMS\nHT\nJK\n\n\n\n\nThe Functional Art\nGGSIDE! A companion to ggplot for making side plots! COOL!\nAwesome Quarto: A potentially interesting repository of Quarto documents, talks, tools, examples, etc.\nThe MockUp Blog - TABLES! This blog post explores the R packages gt and gtextras which will help us up our table game!\nRiffomonas Project: Pat Schloss is a Professor at the University of Michigan. The Riffomonas Project is his Youtube channel, which has HUNDREDS of easy to follow and amazingly useful instructional videos on R, ggplot, version control, and literate programming.\nDr. Tamara Munzner’s Website: It isn’t fancy, but Dr. Munzner’s website has tons of resources from her textbook and the many data visualization courses she has offered. This includes recorded lectures that align directly with the chapters of the text, much like what we are using.\nCheat Sheets: So many visual guides for many R packages, including the tidyverse, ggplot, dplyr, etc.\nLearning Vis Tools: Teaching Data Visualization Tutorials An interesting paper for discussion as we forge the structure for this class."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "I am maintaining the course here and I do not intend to use CANVAS this semester.\nSYLLABUS\nBarrie’s GitHub\n\n\nKM Github\nBC Github\nAM Github\nSD Github\nYG Github\nME Github\nIW Github\nCS Github\nJC Github\nEL Github\nAda\nMS\nHT\nJK"
  },
  {
    "objectID": "index.html#learning-resources",
    "href": "index.html#learning-resources",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "The Functional Art\nGGSIDE! A companion to ggplot for making side plots! COOL!\nAwesome Quarto: A potentially interesting repository of Quarto documents, talks, tools, examples, etc.\nThe MockUp Blog - TABLES! This blog post explores the R packages gt and gtextras which will help us up our table game!\nRiffomonas Project: Pat Schloss is a Professor at the University of Michigan. The Riffomonas Project is his Youtube channel, which has HUNDREDS of easy to follow and amazingly useful instructional videos on R, ggplot, version control, and literate programming.\nDr. Tamara Munzner’s Website: It isn’t fancy, but Dr. Munzner’s website has tons of resources from her textbook and the many data visualization courses she has offered. This includes recorded lectures that align directly with the chapters of the text, much like what we are using.\nCheat Sheets: So many visual guides for many R packages, including the tidyverse, ggplot, dplyr, etc.\nLearning Vis Tools: Teaching Data Visualization Tutorials An interesting paper for discussion as we forge the structure for this class."
  }
]