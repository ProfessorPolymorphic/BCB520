[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "SYLLABUS 2024",
    "section": "",
    "text": "Barrie D. Robison\nSpring 2024\n\n\nThis class will help students establish a core understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\n\n\n\nStudents completing this course will be able to:\n\nDescribe and manipulate tabular, network, and spatial data; transform these data into a form suitable for visualization.\nAnalyze data visualization design choices related to marks and channels, spatial arrangement, and components of color.\nDesign new data visualizations with appropriate use of visual channels for tabular, network, and spatial data with quantitative and categorical attributes.\nImplement their data visualization designs using existing tools in R (or other toolkits preferred by the student).\nExplain whether a visual encoding is perceptually appropriate for a specific combination of task and data.\nDemonstrate their skills with at least two novel visualizations suitable for inclusion in an online Data Science Portfolio.\n\n\n\n\nTamara Munzner. Visualization Analysis and Design. A K Peters Visualization Series, CRC Press, 2014. While the book is not required, I do emphasize the structure and approach to visualization that Dr. Munzner has developed.\nHard Copy on Amazon\nkindle/ebook on Amazon\n\n\n\n\n\n50% of your grade will be determined by homework exercises related to each course unit.\n20% of your grade will be determined by a mid term project (which would be a great item to include in your Data Science Portfolio).\n20% of your grade will be determined by a final project (which would be great item to include in your Data Science Portfolio).\n10% of your grade will be determined by participation in class discussions.\nGRADING SCALE: The grading scale is standard: A (90 -100 %), B (89 - 80 %), C (79 - 70 %), D (69-60 %), F( below 60 %).\n\n\n\nMissing a scheduled class session is at your discretion. I will be posting all the course materials online. If a discussion or in-class exercise occurs and you miss it, you will lose those participation points. There is no way to make up those points.\n\n\n\nThe R Markdown template I used for this syllabus was created by Dr. Steven V. Miller at Stockholm University. It contained this section, which I found amusing and have therefore retained. Professor Miller’s current university asks professors to have policies written into their syllabus about what students should do if the professor is more than 15 minutes late to class. Here is my version of that policy:\nI will inform students via e-mail in advance of class if class is cancelled for the day. Events that might create such a scenario include travel obligations that emerged after the semester has begun, a family emergency that encompasses multiple days, or some other thing. I will also contact our department secretary in emergent situations, such as something happening on the way to work. Failing that, assume the worst. Alien abduction, the return of one or more Old Ones to our plane, or some kind of attack by wizards are all viable explanations for my inability to attend class. I ask that the students make sure that my story gets the proper treatment on the “Mr. Ballen” YouTube channel. I also ask that my story be narrated by Morgan Freeman and that the role of me in the made for TV movie be played by Keanu Reeves or Danny DeVito.\n\n\n\nThe bad news is that there are NO make-ups for missed exams. Don’t bother asking. The good news is that there aren’t any exams.\n\n\n\nAll students are expected to uphold the highest standards of academic honesty. This includes but is not limited to: not cheating, not using the ideas of others without giving appropriate credit (including AI tools), and not falsifying data. Any incident of academic dishonesty will be handled according to the guidelines of the University of Idaho.\n\n\n\n\n\nlibrary(readxl)\nSchedule &lt;- read_excel(\"Schedule.xlsx\")\n\nknitr::kable(Schedule, caption = '')\n\n\n\n\nDATE\nTOPIC\nACTIVITY\nRESOURCES\n\n\n\n\n2024-01-11\nIntroduction and Overview\nNA\nNA\n\n\n2024-01-16\nThe Imporance of Visualization\nLiterate Programming\nTutorial 1, Tutorial 2\n\n\n2024-01-18\nNA\nNA\nNA\n\n\n2024-01-23\nNA\nNA\nNA\n\n\n2024-01-25\nNA\nNA\nNA\n\n\n2024-01-30\nWHAT?  Abstraction of Data\nNA\nNA\n\n\n2024-02-01\nWHY?  Task Abstraction\nNA\nNA\n\n\n2024-02-06\nNA\nNA\nNA\n\n\n2024-02-08\nMARKS. Geometric elements to depict data\nNA\nNA\n\n\n2024-02-13\nNA\nNA\nNA\n\n\n2024-02-15\nNA\nNA\nNA\n\n\n2024-02-20\nNA\nNA\nNA\n\n\n2024-02-22\nCHANNELS. Controlling the appearance of marks.\nNA\nNA\n\n\n2024-02-27\nRULES OF THUMB.\nMidterm Presentations\nNA\n\n\n2024-02-29\nTABULAR DATA I\nMidterm Presentations\nNA\n\n\n2024-03-05\nTABULAR DATA II\nNA\nNA\n\n\n2024-03-07\nSPATIAL DATA I: Geographic Maps\nNA\nNA\n\n\n2024-03-12\nSpring Recess\nNA\nNA\n\n\n2024-03-14\nSpring Recess\nNA\nNA\n\n\n2024-03-19\nBarrie in Vermont\nNA\nNA\n\n\n2024-03-21\nSPATIAL DATA II:  Spatial Fields\nNA\nNA\n\n\n2024-03-26\nNETWORK DATA I\nNA\nNA\n\n\n2024-03-28\nNETWORK DATA II\nNA\nNA\n\n\n2024-04-02\nCOLOR I\nNA\nNA\n\n\n2024-04-04\nCOLOR II\nNA\nNA\n\n\n2024-04-09\nCOLOR III\nNA\nNA\n\n\n2024-04-11\nINTERACTIVITY\nNA\nNA\n\n\n2024-04-16\nMULTIPLE VIEWS\nNA\nNA\n\n\n2024-04-18\nAGGREGATION\nNA\nNA\n\n\n2024-04-23\nFILTERING\nNA\nNA\n\n\n2024-04-25\nEMBEDDING: Focus and Context\nNA\nNA\n\n\n2024-04-30\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2024-05-02\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2024-05-07\nFINALS WEEK\nNA\nNA"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "SYLLABUS 2024",
    "section": "",
    "text": "This class will help students establish a core understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R programming language. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using R. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "SYLLABUS 2024",
    "section": "",
    "text": "Students completing this course will be able to:\n\nDescribe and manipulate tabular, network, and spatial data; transform these data into a form suitable for visualization.\nAnalyze data visualization design choices related to marks and channels, spatial arrangement, and components of color.\nDesign new data visualizations with appropriate use of visual channels for tabular, network, and spatial data with quantitative and categorical attributes.\nImplement their data visualization designs using existing tools in R (or other toolkits preferred by the student).\nExplain whether a visual encoding is perceptually appropriate for a specific combination of task and data.\nDemonstrate their skills with at least two novel visualizations suitable for inclusion in an online Data Science Portfolio."
  },
  {
    "objectID": "syllabus.html#required-readings",
    "href": "syllabus.html#required-readings",
    "title": "SYLLABUS 2024",
    "section": "",
    "text": "Tamara Munzner. Visualization Analysis and Design. A K Peters Visualization Series, CRC Press, 2014. While the book is not required, I do emphasize the structure and approach to visualization that Dr. Munzner has developed.\nHard Copy on Amazon\nkindle/ebook on Amazon"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "SYLLABUS 2024",
    "section": "",
    "text": "50% of your grade will be determined by homework exercises related to each course unit.\n20% of your grade will be determined by a mid term project (which would be a great item to include in your Data Science Portfolio).\n20% of your grade will be determined by a final project (which would be great item to include in your Data Science Portfolio).\n10% of your grade will be determined by participation in class discussions.\nGRADING SCALE: The grading scale is standard: A (90 -100 %), B (89 - 80 %), C (79 - 70 %), D (69-60 %), F( below 60 %).\n\n\n\nMissing a scheduled class session is at your discretion. I will be posting all the course materials online. If a discussion or in-class exercise occurs and you miss it, you will lose those participation points. There is no way to make up those points.\n\n\n\nThe R Markdown template I used for this syllabus was created by Dr. Steven V. Miller at Stockholm University. It contained this section, which I found amusing and have therefore retained. Professor Miller’s current university asks professors to have policies written into their syllabus about what students should do if the professor is more than 15 minutes late to class. Here is my version of that policy:\nI will inform students via e-mail in advance of class if class is cancelled for the day. Events that might create such a scenario include travel obligations that emerged after the semester has begun, a family emergency that encompasses multiple days, or some other thing. I will also contact our department secretary in emergent situations, such as something happening on the way to work. Failing that, assume the worst. Alien abduction, the return of one or more Old Ones to our plane, or some kind of attack by wizards are all viable explanations for my inability to attend class. I ask that the students make sure that my story gets the proper treatment on the “Mr. Ballen” YouTube channel. I also ask that my story be narrated by Morgan Freeman and that the role of me in the made for TV movie be played by Keanu Reeves or Danny DeVito.\n\n\n\nThe bad news is that there are NO make-ups for missed exams. Don’t bother asking. The good news is that there aren’t any exams.\n\n\n\nAll students are expected to uphold the highest standards of academic honesty. This includes but is not limited to: not cheating, not using the ideas of others without giving appropriate credit (including AI tools), and not falsifying data. Any incident of academic dishonesty will be handled according to the guidelines of the University of Idaho."
  },
  {
    "objectID": "syllabus.html#class-schedule",
    "href": "syllabus.html#class-schedule",
    "title": "SYLLABUS 2024",
    "section": "",
    "text": "library(readxl)\nSchedule &lt;- read_excel(\"Schedule.xlsx\")\n\nknitr::kable(Schedule, caption = '')\n\n\n\n\nDATE\nTOPIC\nACTIVITY\nRESOURCES\n\n\n\n\n2024-01-11\nIntroduction and Overview\nNA\nNA\n\n\n2024-01-16\nThe Imporance of Visualization\nLiterate Programming\nTutorial 1, Tutorial 2\n\n\n2024-01-18\nNA\nNA\nNA\n\n\n2024-01-23\nNA\nNA\nNA\n\n\n2024-01-25\nNA\nNA\nNA\n\n\n2024-01-30\nWHAT?  Abstraction of Data\nNA\nNA\n\n\n2024-02-01\nWHY?  Task Abstraction\nNA\nNA\n\n\n2024-02-06\nNA\nNA\nNA\n\n\n2024-02-08\nMARKS. Geometric elements to depict data\nNA\nNA\n\n\n2024-02-13\nNA\nNA\nNA\n\n\n2024-02-15\nNA\nNA\nNA\n\n\n2024-02-20\nNA\nNA\nNA\n\n\n2024-02-22\nCHANNELS. Controlling the appearance of marks.\nNA\nNA\n\n\n2024-02-27\nRULES OF THUMB.\nMidterm Presentations\nNA\n\n\n2024-02-29\nTABULAR DATA I\nMidterm Presentations\nNA\n\n\n2024-03-05\nTABULAR DATA II\nNA\nNA\n\n\n2024-03-07\nSPATIAL DATA I: Geographic Maps\nNA\nNA\n\n\n2024-03-12\nSpring Recess\nNA\nNA\n\n\n2024-03-14\nSpring Recess\nNA\nNA\n\n\n2024-03-19\nBarrie in Vermont\nNA\nNA\n\n\n2024-03-21\nSPATIAL DATA II:  Spatial Fields\nNA\nNA\n\n\n2024-03-26\nNETWORK DATA I\nNA\nNA\n\n\n2024-03-28\nNETWORK DATA II\nNA\nNA\n\n\n2024-04-02\nCOLOR I\nNA\nNA\n\n\n2024-04-04\nCOLOR II\nNA\nNA\n\n\n2024-04-09\nCOLOR III\nNA\nNA\n\n\n2024-04-11\nINTERACTIVITY\nNA\nNA\n\n\n2024-04-16\nMULTIPLE VIEWS\nNA\nNA\n\n\n2024-04-18\nAGGREGATION\nNA\nNA\n\n\n2024-04-23\nFILTERING\nNA\nNA\n\n\n2024-04-25\nEMBEDDING: Focus and Context\nNA\nNA\n\n\n2024-04-30\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2024-05-02\nDEAD WEEK\nFinal Presentations\nNA\n\n\n2024-05-07\nFINALS WEEK\nNA\nNA"
  },
  {
    "objectID": "posts/T2-Anscombe/index.html",
    "href": "posts/T2-Anscombe/index.html",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "",
    "text": "Do the summary statistics reveal the truth? Or are they FILLED WITH LIES? A simple demonstration with Anscombe’s Quartet."
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#more-quarto",
    "href": "posts/T2-Anscombe/index.html#more-quarto",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "",
    "text": "Do the summary statistics reveal the truth? Or are they FILLED WITH LIES? A simple demonstration with Anscombe’s Quartet."
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#the-data",
    "href": "posts/T2-Anscombe/index.html#the-data",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "The Data",
    "text": "The Data\nAnscombe’s Quartet is comprised of four pairs of x,y data:\n\n\nCode\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(datasets)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\n\n\nCode\ndatasets::anscombe\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89"
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#example-hypotheses",
    "href": "posts/T2-Anscombe/index.html#example-hypotheses",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Example Hypotheses",
    "text": "Example Hypotheses\n\nEric has four replicates in which he is measuring the expression of two proteins (x, y) on 11 samples.\nCody has four replicates in which he is measuring gene expression of two genes (x, y) in 11 cell images.\nRonald has four replicates in which he is measuring a blood protein (x) and a malaria marker (y) in 11 patients.\n\nYour hypothesis is that the four replicates do not differ in the correlation between x and y."
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#summary-statistics",
    "href": "posts/T2-Anscombe/index.html#summary-statistics",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\ntidy_anscombe &lt;- anscombe %&gt;%\n pivot_longer(cols = everything(),\n              names_to = c(\".value\", \"set\"),\n              names_pattern = \"(.)(.)\")\ntidy_anscombe_summary &lt;- tidy_anscombe %&gt;%\n  group_by(set) %&gt;%\n  summarise(across(.cols = everything(),\n                   .fns = lst(min,max,median,mean,sd,var),\n                   .names = \"{col}_{fn}\"))\n#&gt; `summarise()` ungrouping output (override with `.groups` argument)\n\nvars&lt;-c(\"set\", \"x_mean\", \"x_var\",  \"y_mean\", \"y_var\")\nthing&lt;- as.data.frame(tidy_anscombe_summary[vars])\nknitr::kable(thing)\n\n\n\n\n\nset\nx_mean\nx_var\ny_mean\ny_var\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249"
  },
  {
    "objectID": "posts/T2-Anscombe/index.html#visualization-reveals-hidden-patterns",
    "href": "posts/T2-Anscombe/index.html#visualization-reveals-hidden-patterns",
    "title": "TUTORIAL 2 - Literate Programming and Anscombe’s Quartet",
    "section": "Visualization reveals hidden patterns!",
    "text": "Visualization reveals hidden patterns!\n\n\nCode\nggplot(tidy_anscombe,\n       aes(x = x,\n           y = y)) +\n  geom_point() +\n  geom_point(data = tidy_anscombe_summary, aes(x=x_mean, y = y_mean, color = \"red\", size = 5),\n             show.legend = FALSE)+\n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html",
    "href": "posts/A4-MarksChannels/index.html",
    "title": "ASSIGNMENT 4",
    "section": "",
    "text": "It’s one thing to “learn” about the theory of marks and channels for data visualization. It’s an entirely different thing to LEARN these concepts by IMPLEMENTING them in your own visualizations. In this assignment, you will do exactly that. Create some simple visualizations that highlight some of the key concepts from this section of the class."
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#summary",
    "href": "posts/A4-MarksChannels/index.html#summary",
    "title": "ASSIGNMENT 4",
    "section": "",
    "text": "It’s one thing to “learn” about the theory of marks and channels for data visualization. It’s an entirely different thing to LEARN these concepts by IMPLEMENTING them in your own visualizations. In this assignment, you will do exactly that. Create some simple visualizations that highlight some of the key concepts from this section of the class."
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#a-new-portfolio-post",
    "href": "posts/A4-MarksChannels/index.html#a-new-portfolio-post",
    "title": "ASSIGNMENT 4",
    "section": "A NEW PORTFOLIO POST",
    "text": "A NEW PORTFOLIO POST\nThis assignment will appear as a new post in your nascent data science portfolio. To create a new post, you need to navigate to your posts directory in your RStudio Project. I do this using the Files tab in the lower right quadrant of RStudio.\n\n\nMake a new directory within posts called MarksChannels .\nClick on that new directory to enter it and then use that create a new blank file button and choose Quarto Document.\nName the new file index.qmd .\nAdd YAML text to the top of the file. In particular, add the following:\n\ntitle: “ASSIGNMENT 4”\nsubtitle: “Marks and Channels”\nauthor: “YOUR NAME”\ndate: “2024-02-08”\ncategories: [Assignment, DataViz]\nimage: “some fun image you put in that new directory.png”\ncode-fold: true\ncode-tools: true\ndescription: “A clever description that describes the stuff”"
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#make-your-portfolio-less-terrible",
    "href": "posts/A4-MarksChannels/index.html#make-your-portfolio-less-terrible",
    "title": "ASSIGNMENT 4",
    "section": "MAKE YOUR PORTFOLIO LESS TERRIBLE",
    "text": "MAKE YOUR PORTFOLIO LESS TERRIBLE\nWhile we are at it, let’s improve the overall look and structure of your portfolio. Navigate to the about.qmd file in your portfolio root directory and open it in RStudio. Use the information found here to customize your About page.\nAT MINIMUM , I want an About page that contains your name, a brief description about you, a picture of yourself, a link to your github, and a section on your Education."
  },
  {
    "objectID": "posts/A4-MarksChannels/index.html#marks-and-channels",
    "href": "posts/A4-MarksChannels/index.html#marks-and-channels",
    "title": "ASSIGNMENT 4",
    "section": "MARKS AND CHANNELS",
    "text": "MARKS AND CHANNELS\nBack to your new post!\nUse that shiny new index.qmd file to perform the tasks below. Every time you create a figure, it needs a caption. The text in that section of your assignment should also briefly describe the data set you are using, especially the attributes used for the visualization. In addition, make sure the visualization task actually requires the particular concept. For example, don’t just make a scatterplot with one red dot for the Popout exercise. You need to describe a task that requires we IDENTIFY that point.\n\nExpressiveness and Effectiveness\nFrom Munzner, 5.4.1:\nThe expressiveness principle: Visual encoding should express all of, and only, the information in the dataset attributes. Ordered data should be shown in a way that our perceptual system intrinsically senses as ordered. Conversely, unordered data should not be shown in a way that perceptually implies an ordering that does not exist.\nThe effectiveness principle: The importance of the attribute should match the salience of the channel (its noticeablity). The most important attributes should be encoded with the most effective channels.\nUsing whatever data set you choose, create a visualization called Figure 1 that adheres to these two principles. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 2 that violates these principles in the most extreme ways you can imagine while still having the visualization be recognizibly the same as the first. Write a caption that explains your choices of marks and channels and why they are bad compared to Figure 1.\n\n\nDiscriminability\nUsing whatever data set you choose (it need not be the same data used for Figures 1 and 2), create a visualization called Figure 3 that uses a magnitude channel and a number of bins that facilitate discriminiability of an attribute. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 4 that uses this same channel for WAY TOO MANY BINS, violating the guidelines for discriminability. Write a caption that explains why this number of bins is bad compared to Figure 3.\n\n\nSeparability\nUsing whatever data set you choose (it need not be the same data used for Figures 1-4), create a visualization called Figure 5 that uses a two or more channels to encode two or more attributes while maintainig separability. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 6 that uses channels that are integral, or at least much less separable. Write a caption that explains why choosing these channels is bad compared to Figure 5.\n\n\nPopout\nUsing whatever data set you choose (it need not be the same data used for Figures 1-6… you get the idea), create a visualization called Figure 7 that effectively uses the concept of popout. Write a caption that explains your choices of marks and channels in these terms. Then, using the exact same data, create a visualization called Figure 8 that makes the identification task in Figure 7 much more difficult. Write a caption that explains why Figure 8 sucks compared to Figure 7."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#tldr-version",
    "href": "posts/L5-ThumbRules/index.html#tldr-version",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "TLDR VERSION",
    "text": "TLDR VERSION\nThese are guidelines and considerations, not really absolute rules:\n\nWhen to use 3D? when to use 2D?\nWhen to use eyes instead of memory?\nWhen does immersion help?\nWhen to use overviews?\nHow long is too long?\nWhich comes first, form or function?"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#unjustified-use-of-3d",
    "href": "posts/L5-ThumbRules/index.html#unjustified-use-of-3d",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "UNJUSTIFIED USE OF 3D",
    "text": "UNJUSTIFIED USE OF 3D\nCommon in news and some infographics.\n\n\n\nCheck this out on WTFViz\n\n\nCheck this out on WTFViz"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#depth-vs-planar-position",
    "href": "posts/L5-ThumbRules/index.html#depth-vs-planar-position",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "DEPTH VS PLANAR POSITION",
    "text": "DEPTH VS PLANAR POSITION\nPosition channels are very powerful… when they are PLANAR SPATIAL POSITION, not depth!"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#the-dangers-of-depth",
    "href": "posts/L5-ThumbRules/index.html#the-dangers-of-depth",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "THE DANGERS OF DEPTH",
    "text": "THE DANGERS OF DEPTH"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#the-actual-dangers-of-depth",
    "href": "posts/L5-ThumbRules/index.html#the-actual-dangers-of-depth",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "THE ACTUAL DANGERS OF DEPTH",
    "text": "THE ACTUAL DANGERS OF DEPTH\nWe don’t really live in 3D: we see in 2.05D.\n\nWe acquire more info on an image plane quickly using eye movements\nWe acquire more info for depth much more slowly using head/body motion"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#occlusion",
    "href": "posts/L5-ThumbRules/index.html#occlusion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "OCCLUSION",
    "text": "OCCLUSION\n“Occlusion occurs when one 3D graphic partially blocks another. It is the result of mimicking space in the natural world–where objects have differing X, Y, and Z coordinates. In data visualization, occlusion obscures important data and creates false hierarchies wherein unobstructed graphics appear most important.” … From This BLOG post by M. Bowers.\n\nInteraction can resolve occlusion, but at cost of time and cognitive load."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#perspective-distortion",
    "href": "posts/L5-ThumbRules/index.html#perspective-distortion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "PERSPECTIVE DISTORTION",
    "text": "PERSPECTIVE DISTORTION\nPerspective in 3D visualizations interferes with all size channel encodings. The power of 2D planar positioning is lost!\n“Distortion occurs when 3D graphics recede into or project out from the picture plane through foreshortening. In drawing, foreshortening makes objects seem as though they inhabit three-dimensional space, but in data visualization, it creates more false hierarchies. Foreground graphics appear larger, background graphics smaller, and the relationship between data series is needlessly skewed.”…… From This BLOG post by M. Bowers."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-bar-charts---probably-not",
    "href": "posts/L5-ThumbRules/index.html#d-bar-charts---probably-not",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D BAR CHARTS - Probably not…",
    "text": "3D BAR CHARTS - Probably not…\n3D bars are very difficult to justify!\nPerspective distortion and occlusion make faceting into 2D the better choice in most situations."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#tilted-text",
    "href": "posts/L5-ThumbRules/index.html#tilted-text",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "TILTED TEXT",
    "text": "TILTED TEXT\nSkewed perspective in 3D visualizations does not interact well with fonts.\n\nExploring and Reducing the Effects of Orientation on Text Readability in Volumetric Displays.Grossman et al. CHI 2007"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#example-3d-extruded-time-series",
    "href": "posts/L5-ThumbRules/index.html#example-3d-extruded-time-series",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "EXAMPLE: 3D EXTRUDED TIME SERIES",
    "text": "EXAMPLE: 3D EXTRUDED TIME SERIES\nWhat information can we decode from this visualization?\n\nCluster and Calendar based Visualization of Time Series Data. van Wijk and van Selow, Proc. InfoVis 99."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#planar-alternative",
    "href": "posts/L5-ThumbRules/index.html#planar-alternative",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "PLANAR ALTERNATIVE",
    "text": "PLANAR ALTERNATIVE\nThis version of the extruded time series visualization uses derived data by computing a cluster hierarchy of power usage patterns. It then juxtaposes multiple views: a calendar and superimposed 2D curves differentiated with a color channel."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-shape-perception",
    "href": "posts/L5-ThumbRules/index.html#d-shape-perception",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D: SHAPE PERCEPTION",
    "text": "3D: SHAPE PERCEPTION\nThe benefits of 3D visualization outweigh its costs when the task is shape perception for 3D spatial data. Interactive navigation often supports synthesis across many viewpoints.\n\nImage-Based Streamline Generation and Rendering. Li and Shen. IEEE Trans. Visualization and Computer Graphics (TVCG) 13:3 (2007), 630–640."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-constrained-navigation",
    "href": "posts/L5-ThumbRules/index.html#d-constrained-navigation",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D: CONSTRAINED NAVIGATION",
    "text": "3D: CONSTRAINED NAVIGATION\n\nNYTimes Subscribers can access here"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#d-summary",
    "href": "posts/L5-ThumbRules/index.html#d-summary",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "3D SUMMARY",
    "text": "3D SUMMARY\n\n3D legitimate for true 3D spatial data.\n3D needs very careful justification for abstract data\nThe benefits outweigh costs when topological structure/context important for the task."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#eyes-beat-memory",
    "href": "posts/L5-ThumbRules/index.html#eyes-beat-memory",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "EYES BEAT MEMORY",
    "text": "EYES BEAT MEMORY\nExternal Cognition vs. Internal Memory\nIt is easy to compare by moving eyes between side-by-side views.\nIt is much more difficult to compare a visible item to your memory of what you saw.\n\nImplications for animation:\ngreat for choreographed storytelling\ngreat for transitions between two states\npoor for many states with changes everywhere\nconsider small multiples instead"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#resolution-beats-immersion",
    "href": "posts/L5-ThumbRules/index.html#resolution-beats-immersion",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "RESOLUTION BEATS IMMERSION",
    "text": "RESOLUTION BEATS IMMERSION\nImmersion is typically not helpful for abstract data because we do not need a sense of presence or stereoscopic 3D. A desktop view is also usually better for workflow integration.\nResolution in VR is a critical constraint, as pixels are the scarcest resource.\nFirst wave: virtual reality for abstract data! This is difficult to justify.\nSecond wave: AR/MR (augmented/mixed reality) has more promise."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#overview",
    "href": "posts/L5-ThumbRules/index.html#overview",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "OVERVIEW",
    "text": "OVERVIEW\n“Overview first, zoom and filter, details on demand”\nThe Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations. Shneiderman. Proc. IEEE Visual Languages, pp. 336–343, 1996.\n\nThis seems simple enough, until you realize that “Overview” is really a microcosm of the full vis design problem space."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#responsiveness",
    "href": "posts/L5-ThumbRules/index.html#responsiveness",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "RESPONSIVENESS",
    "text": "RESPONSIVENESS\nResponsiveness (visual feedback to the user) has 3 rough categories:\n\n0.1 seconds: perceptual processing\n\nsubsecond response for mouseover highlighting\n\n1 second: immediate response\n\nfast response after mouseclick, button press\nFitts’ Law limits on motor control\n\n10 seconds: brief tasks\n\nbounded response after dialog box\nmental model of heavyweight operation (file load)\n\nShow hourglass for multi-second operations (check for cancel/undo)\nShow progress bar for long operations (process in background thread)\nConsider rendering speed when item count is large (guaranteed frame rate)"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#function-then-form",
    "href": "posts/L5-ThumbRules/index.html#function-then-form",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "FUNCTION, THEN FORM",
    "text": "FUNCTION, THEN FORM\nIt is dangerous to start with aesthetics because it is usually impossible to add function retroactively.\nStart with focus on functionality because you can improve and refine aesthetics later.\nAesthetics do matter! They are another level of function. Consider working with a graphic designer on important visualizations."
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#form-basic-graphic-design",
    "href": "posts/L5-ThumbRules/index.html#form-basic-graphic-design",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "FORM: BASIC GRAPHIC DESIGN",
    "text": "FORM: BASIC GRAPHIC DESIGN\n\n\nProximity:\nDO group related items together\nAVOID equal whitespace between unrelated items\nAlignment:\nDO find/make a strong line and stick to it\nAVOID automatic centering\nRepetition:\nDO unify by pushing existing consistencies\nContrast:\nIf not identical, then very different\nAVOID not quite the same\n\n\n\n\nThe Non-Designer’s Design Book, 4th ed. Robin Williams, Peachpit Press, 2015. fast read, very practical to work through whole thing"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#labelling",
    "href": "posts/L5-ThumbRules/index.html#labelling",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "LABELLING",
    "text": "LABELLING\nMake visualizations as self-documenting as possible!\nMeaningful & useful title, labels, legends.\nAxes and panes/subwindows should have labels and axes should have good mix/max boundary tick marks.\nEverything that’s plotted should have a legend and its own header/labels if not redundant with main title.\nUse reasonable numerical format and avoid scientific notation in most cases.\n\nhttps://xkcd.com/833/"
  },
  {
    "objectID": "posts/L5-ThumbRules/index.html#summary---rules-of-thumb",
    "href": "posts/L5-ThumbRules/index.html#summary---rules-of-thumb",
    "title": "LECTURE 5 - Rules of Thumb",
    "section": "SUMMARY - RULES OF THUMB",
    "text": "SUMMARY - RULES OF THUMB\n\nNo unjustified 3D\n\nPower of the plane\nDisparity of depth\nOcclusion hides information\nPerspective distortion dangers\nTilted text isn’t legible\n\nEyes beat memory\nResolution over immersion\nOverview first, zoom and filter, details on demand\nResponsiveness is required\nFunction first, form next\n\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#the-three-major-data-types",
    "href": "posts/L6-TabularData1/index.html#the-three-major-data-types",
    "title": "LECTURE 6 - Tabular Data",
    "section": "THE THREE MAJOR DATA TYPES",
    "text": "THE THREE MAJOR DATA TYPES"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#what",
    "href": "posts/L6-TabularData1/index.html#what",
    "title": "LECTURE 6 - Tabular Data",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#semantics-for-data",
    "href": "posts/L6-TabularData1/index.html#semantics-for-data",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SEMANTICS FOR DATA",
    "text": "SEMANTICS FOR DATA\n\n\n\nSemantics\n\nitem: individual entity, discrete\n\neg patient, car, stock, city\n“independent variable”\n\nattribute: property that is measured, observed, logged…\n\neg height, blood pressure for patient\neg horsepower, make for car\n“dependent variable”\n\n\n\n\nData Table\n\n\n\n\nITEM: Person\nATTRIBUTES: Name, Age, Shirt Size, Favorite Fruit"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#dataset-types-tables",
    "href": "posts/L6-TabularData1/index.html#dataset-types-tables",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DATASET TYPES: TABLES",
    "text": "DATASET TYPES: TABLES\nFlat Table\n\n\n\nOne ITEM per row\n\noften called an observation\n\nEach column is an ATTRIBUTE\n\noften called a variable\n\nA cell holds the VALUE for an item/attribute pair\nA unique KEY can be used (implicitly or explicitly) to identify each item even if they share all measured attributes"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#flat-table-example",
    "href": "posts/L6-TabularData1/index.html#flat-table-example",
    "title": "LECTURE 6 - Tabular Data",
    "section": "FLAT TABLE EXAMPLE",
    "text": "FLAT TABLE EXAMPLE\n\n\n\n\n\n\n\n\nA dumb spreadsheet\n\n\n\n\n\n\n\nA cool spreadsheet\n\n\n\n\n\n\nFigure 1: Flat Tables"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#multidimensional-tables",
    "href": "posts/L6-TabularData1/index.html#multidimensional-tables",
    "title": "LECTURE 6 - Tabular Data",
    "section": "MULTIDIMENSIONAL TABLES",
    "text": "MULTIDIMENSIONAL TABLES\nIndexing based on multiple keys (eg genes, patients)"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#keys-and-values",
    "href": "posts/L6-TabularData1/index.html#keys-and-values",
    "title": "LECTURE 6 - Tabular Data",
    "section": "KEYS AND VALUES",
    "text": "KEYS AND VALUES\n\n\nKEY: an independent attribute used as unique index to look up items.\n\nSimple tables: 1 key\nMultidimensional tables: multiple keys\n\nValue: a dependent attribute, value of cell\nWe will classify visualization idioms by the number of keys used.\n0, 1, 2, …"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#framework",
    "href": "posts/L6-TabularData1/index.html#framework",
    "title": "LECTURE 6 - Tabular Data",
    "section": "FRAMEWORK",
    "text": "FRAMEWORK"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-scatterplot",
    "href": "posts/L6-TabularData1/index.html#idiom-scatterplot",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Scatterplot",
    "text": "IDIOM: Scatterplot\n0 KEYS: Only values are available or salient to the task.\n\n\nExpress values (magnitudes) of quantitative attributes.\nData: 2 quantitative attributes\nMark: points\nChannels: horizontal and verical position\nTasks: find trends, outliers, distribution, correlation, clusters\nScalability: hundreds of items\n\n\n\n\n\n\nHeight and Weight of drafted NHL players."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-scatterplot-1",
    "href": "posts/L6-TabularData1/index.html#idiom-scatterplot-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Scatterplot",
    "text": "IDIOM: Scatterplot\n0 KEYS: Only values are available or salient to the task.\n\n\nExpress values (magnitudes) of quantitative attributes.\nData: 2 quantitative attributes\nMark: points\nChannels: horizontal and verical position\nTasks: find trends, outliers, distribution, correlation, clusters\nScalability: hundreds of items\n\n\n\n\n\n\nHeight and Weight of drafted NHL players."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#scatterplots-more-channels",
    "href": "posts/L6-TabularData1/index.html#scatterplots-more-channels",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SCATTERPLOTS: More channels",
    "text": "SCATTERPLOTS: More channels\nAdditional channels are viable with scatterplots since we are using point marks.\n\n\nExamples:\nColor: But pay attention to Discriminability!\nSize: But pay attention to using a quantitative attribute used to control 2D area. Directly encoding radius would mislead. Take the square root since area grows quadratically.\nShape:\n\n\n\n\n\n\nHeight and Weight of drafted NHL goaltenders."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#scatterplot-tasks",
    "href": "posts/L6-TabularData1/index.html#scatterplot-tasks",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SCATTERPLOT TASKS",
    "text": "SCATTERPLOT TASKS\nCorrelation\n\nClusters/groups, and clusters vs classes"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#keys-categorical-regions",
    "href": "posts/L6-TabularData1/index.html#keys-categorical-regions",
    "title": "LECTURE 6 - Tabular Data",
    "section": "KEYS: Categorical Regions",
    "text": "KEYS: Categorical Regions\n\n\n\nRegions: contiguous bounded areas distinct from each other\n\nNO OVERPLOTTING: separate into spatial regions: one mark per region (for now)\n\nuse categorical or ordered attribute to separate into regions\n\nno conflict with expressiveness principle for categorical attributes\n\nuse ordered attribute to order and align regions"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#separated-and-aligned-and-ordered",
    "href": "posts/L6-TabularData1/index.html#separated-and-aligned-and-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated and aligned and ordered",
    "text": "Separated and aligned and ordered\nBest case!"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#separatedaligned---not-ordered",
    "href": "posts/L6-TabularData1/index.html#separatedaligned---not-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated/aligned - not ordered",
    "text": "Separated/aligned - not ordered\nLimitation: hard to know rank. what’s 4th? what’s 7th?"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#separated-but-not-aligned-or-ordered",
    "href": "posts/L6-TabularData1/index.html#separated-but-not-aligned-or-ordered",
    "title": "LECTURE 6 - Tabular Data",
    "section": "Separated but not aligned or ordered",
    "text": "Separated but not aligned or ordered\nLimitation: hard to make comparisons with size (vs aligned position)"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-bar-chart",
    "href": "posts/L6-TabularData1/index.html#idiom-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Bar Chart",
    "text": "IDIOM: Bar Chart\nOne key, One value\n\n\nData: 1 categorical attribute, 1 quantitative attribute\nMark: Lines\nChannels: Length to express quantitative value\nSpatial regions: one per mark - separated horizontally, aligned vertically. Ordered by quantitative attribute: by label (alphabetical), by length attribute (data-driven)\nTasks: compare, lookup values\nScalability: dozens to hundreds of levels for key attribute (bars), hundreds for values."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-stacked-bar-chart",
    "href": "posts/L6-TabularData1/index.html#idiom-stacked-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Stacked Bar Chart",
    "text": "IDIOM: Stacked Bar Chart\nTwo keys, One value\n\n\nData: 2 categorical attribute, 1 quantitative attribute\nMark: vertical stack of line marks\nGlyph: composite object, internal structure from multiple marks\nChannels: length and color hue\nSpatial regions: one per glyph\nAligned: full glyph, lowest bar component\nUnaligned: other bar components\nTask: part-to-whole relationship\nScalability: asymmetric for stacked key attrib, 10-12 levels segments for main key attrib, dozens to hundreds of levels bars"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-streamgraph",
    "href": "posts/L6-TabularData1/index.html#idiom-streamgraph",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Streamgraph",
    "text": "IDIOM: Streamgraph\nGeneralized stacked graph emphasizing horizontal continuity vs vertical items (example)\n\n\nData: 1 categ key attrib (movies) 1 ordered key attrib (time) 1 quant value attrib (counts) derived data geometry: layers, where height encodes counts 1 quant attrib (layer ordering)\nMark: vertical stack of line marks\nChannels: length and color hue\nTask: part-to-whole relationship\nScalability: hundreds of time keys dozens to hundreds of movies keys more than stacked bars: most layers don’t extend across whole chart"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-dotline-chart",
    "href": "posts/L6-TabularData1/index.html#idiom-dotline-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Dot/Line Chart",
    "text": "IDIOM: Dot/Line Chart\nOne key, One value\n\n\nData:2 quant attribs\nMark: points AND line connection marks between them\nChannels: aligned lengths to express quant value separated and ordered by key attrib into horizontal regions\nTask: find trend connection marks emphasize ordering of items along key axis by explicitly showing relationship between one item and the next\nScalability: hhundreds of key levels, hundreds of value levels"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#bar-vs-line-charts",
    "href": "posts/L6-TabularData1/index.html#bar-vs-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "BAR vs LINE CHARTS",
    "text": "BAR vs LINE CHARTS\nChoice depends on the type of key attributes.\nBar charts if categorical, Line charts if ordered. Do not use line charts for categorical key attributes as it violates the expressiveness principle. The implication of trend is so strong that it overrides semantics. “The more male a person is, the taller he/she is”"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#chart-axes",
    "href": "posts/L6-TabularData1/index.html#chart-axes",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CHART AXES",
    "text": "CHART AXES\n\n\nBest practice to label axes, with few exceptions. Individual small multiple views could share axis label.\nInclude 0 at bottom left or the slope misleads. There are some exceptions (arbitrary 0, small change matters).\nTruncating the Y-Axis: Threat or Menace? Correll, Bertini, & Franconeri, CHI 2020."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-indexed-line-charts",
    "href": "posts/L6-TabularData1/index.html#idiom-indexed-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Indexed Line Charts",
    "text": "IDIOM: Indexed Line Charts\n\n\nData: 2 quant attribs 1 key + 1 value\nDerived data: new quant value attrib index plot instead of original value\nTask: show change over time\nPrinciple: normalized, not absolute\nScalability: same as standard line chart\n\n\nCool Example"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-gantt-charts",
    "href": "posts/L6-TabularData1/index.html#idiom-gantt-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Gantt Charts",
    "text": "IDIOM: Gantt Charts\none key, two (related) values\n\n\nData: 1 categ attrib, 2 quant attribs\nMark: line length: duration\nChannels: horiz position: start time (+end from duration)\nTask: mphasize temporal overlaps & start/end dependencies between items\nScalability: dozens of key levels bars hundreds of value levels durations"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-slopegraphs",
    "href": "posts/L6-TabularData1/index.html#idiom-slopegraphs",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Slopegraphs",
    "text": "IDIOM: Slopegraphs\ntwo values\n\n\nData: 2 quant value attribs (1 derived attrib: change magnitude)\nMark: point + line line connecting mark between pts\nChannels: 2 vertical pos: express attrib value (linewidth/size, color)\nTask: emphasize changes in rank/value\nScalability: hundreds of value levels dozens of item"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#keys",
    "href": "posts/L6-TabularData1/index.html#keys",
    "title": "LECTURE 6 - Tabular Data",
    "section": "2 KEYS",
    "text": "2 KEYS"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#idiom-heatmap",
    "href": "posts/L6-TabularData1/index.html#idiom-heatmap",
    "title": "LECTURE 6 - Tabular Data",
    "section": "IDIOM: Heatmap",
    "text": "IDIOM: Heatmap\ntwo keys, one value\n\n\nData: 2 categ attribs (gene, experimental condition) 1 quant attrib (expression levels)\nMark: point separate and align in 2D matrix indexed by 2 categorical attributes\nChannels: color by quant attrib (ordered diverging colormap)\nTask: find clusters, outliers\nScalability: 1M items, 100s of categ levels, ~10 quant attrib levels"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#heatmap-reordering",
    "href": "posts/L6-TabularData1/index.html#heatmap-reordering",
    "title": "LECTURE 6 - Tabular Data",
    "section": "HEATMAP REORDERING",
    "text": "HEATMAP REORDERING"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#clustered-heatmap",
    "href": "posts/L6-TabularData1/index.html#clustered-heatmap",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CLUSTERED HEATMAP",
    "text": "CLUSTERED HEATMAP\nUse derived data: compute 2 cluster hierarchies and represent as a dendrogram. Parent-child relationships represented in tree with connection line marks, leaves aligned so interior branch heights are easy to compare. Heatmap marks are (re-)ordered by cluster hierarchy traversal. Task: assess quality of clusters found by automatic methods"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#axis-orientation",
    "href": "posts/L6-TabularData1/index.html#axis-orientation",
    "title": "LECTURE 6 - Tabular Data",
    "section": "AXIS ORIENTATION",
    "text": "AXIS ORIENTATION"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#discuss",
    "href": "posts/L6-TabularData1/index.html#discuss",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DISCUSS",
    "text": "DISCUSS"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#radial-bar-chart",
    "href": "posts/L6-TabularData1/index.html#radial-bar-chart",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADIAL BAR CHART",
    "text": "RADIAL BAR CHART\n\n\n\n\nStar plot: line mark, radial axes meet at central point\nRadial bar chart: line mark, radial axes meet at central ring\nChannels: length, angle/orientation\nBar chart: rectilinear axes, aligned vertically\nAccuracy: length not aligned with radial layouts, making them less accurately perceived than rectilinear aligned layouts.\nVismon: Facilitating Risk Assessment and Decision Making In Fisheries Management. Booshehrian, Möller, Peterman, and Munzner. Technical Report TR 2011-04, Simon Fraser University, School of Computing Science, 2011."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#radar-plot",
    "href": "posts/L6-TabularData1/index.html#radar-plot",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADAR PLOT",
    "text": "RADAR PLOT\nRadial line chart with point marks, radial layout, and connecting line marks. Avoid unless data are cyclic."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#radar-plot-example",
    "href": "posts/L6-TabularData1/index.html#radar-plot-example",
    "title": "LECTURE 6 - Tabular Data",
    "section": "RADAR PLOT EXAMPLE",
    "text": "RADAR PLOT EXAMPLE\n\nTheFunctionalArt"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#pie-and-coxcomb-charts",
    "href": "posts/L6-TabularData1/index.html#pie-and-coxcomb-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE AND COXCOMB CHARTS",
    "text": "PIE AND COXCOMB CHARTS\n1 categorical key attribute, 1 quantitative value attribute\n\n\nPie chart: interlocking area marks with angle channel causes variation in 2D area. Keys are separated & ordered radially with uniform height. accuracy: area less accurate than rectilinear aligned line length.\nTask: part-to-whole judgements.\nCoxcomb chart: line marks with length channel, creating variation in only 1D length. Keys are separated & ordered radially with uniform width. These charts are a direct analog to radial bar charts."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#nightengale-rose",
    "href": "posts/L6-TabularData1/index.html#nightengale-rose",
    "title": "LECTURE 6 - Tabular Data",
    "section": "NIGHTENGALE ROSE",
    "text": "NIGHTENGALE ROSE\nCoxcomb / Nightengale Rose/ Polar Area Chart Invented by Florence Nightingale."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#coxcomb-perception",
    "href": "posts/L6-TabularData1/index.html#coxcomb-perception",
    "title": "LECTURE 6 - Tabular Data",
    "section": "COXCOMB: Perception",
    "text": "COXCOMB: Perception\n\n\nEncode: 1D length\nDecode/perceive: 2D area\nThe nonuniform relationship between line/sector width and mark length causes area variation to scale nonlinearly with line mark length!\nA standard bar chart is safer: bars are uniform width, so area is linear with line mark length in both radial & rectilinear cases."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#pie-charts-perception",
    "href": "posts/L6-TabularData1/index.html#pie-charts-perception",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE CHARTS: Perception",
    "text": "PIE CHARTS: Perception\n\n\nSome empirical evidence that people respond to arc length.\nDecode/perceive: arc length, maybe also areas, but not angles.\nDonut charts are no worse than pie charts.\n\n\n\n\nArcs, Angles, or Areas: Individual Data Encodings in Pie and Donut Charts. Skau and Kosara. Proc. EuroVis 2016\nExplore this concept"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#pie-charts-best-practices",
    "href": "posts/L6-TabularData1/index.html#pie-charts-best-practices",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PIE CHARTS: Best Practices",
    "text": "PIE CHARTS: Best Practices\nNot so bad for two (or few) levels and part-to-whole tasks. Dubious for several levels if details matter. Terrible for many levels."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#normalized-stacked-bar",
    "href": "posts/L6-TabularData1/index.html#normalized-stacked-bar",
    "title": "LECTURE 6 - Tabular Data",
    "section": "NORMALIZED STACKED BAR",
    "text": "NORMALIZED STACKED BAR\n\n\nTask: part-to-whole judgements\nNormalized stacked bar chart: stacked bar chart, normalized to full vertical height. A single stacked bar is equivalent to a full pie chart.\nHigh information density can be achieved with narrow rectangles. Pie chart information density is much more limited and requires a large circle."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#glyphmaps",
    "href": "posts/L6-TabularData1/index.html#glyphmaps",
    "title": "LECTURE 6 - Tabular Data",
    "section": "GLYPHMAPS",
    "text": "GLYPHMAPS\nRectilinear glyphs are good for linear vs nonlinear trends. Radial glyphs are good for cyclic patterns and evaluating periodicity.\n\nGlyph-maps for Visually Exploring Temporal Patterns in Climate Data and Models. Wickham, Hofmann, Wickham, and Cook. Environmetrics 23:5 (2012), 382–393."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-axes",
    "href": "posts/L6-TabularData1/index.html#parallel-axes",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL AXES",
    "text": "PARALLEL AXES"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#splom",
    "href": "posts/L6-TabularData1/index.html#splom",
    "title": "LECTURE 6 - Tabular Data",
    "section": "SPLOM",
    "text": "SPLOM\n\n\nScatterplot matrix (SPLOM): Rectilinear axes with point mark. All possible pairs of axes are visualized.\nScalability: about a dozen attributes and dozens to hundreds of items."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-coordinates",
    "href": "posts/L6-TabularData1/index.html#parallel-coordinates",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES",
    "text": "PARALLEL COORDINATES\nScatterplot limitation: visual representation with orthogonal axes can show only two attributes with spatial position channel."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-coordinates-1",
    "href": "posts/L6-TabularData1/index.html#parallel-coordinates-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES",
    "text": "PARALLEL COORDINATES\n\n\nAlternative: Line up axes in parallel to show many attributes with position. Items are encoded with a line with n segments (n is the number of attributes shown). Ordering is a major challenge.\nScalability: dozens of attributes and hundreds of items."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#parallel-coordinates-limitations",
    "href": "posts/L6-TabularData1/index.html#parallel-coordinates-limitations",
    "title": "LECTURE 6 - Tabular Data",
    "section": "PARALLEL COORDINATES: Limitations",
    "text": "PARALLEL COORDINATES: Limitations\nPatterns only visible between neighboring axis pairs. How to pick axis order? The usual solution is reorderable axes and interactive exploration. The downside of interaction is human-powered search.\n\nAs usual, OBSERVABLE really shines for these types of interactive plots!"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#orientation-limitations",
    "href": "posts/L6-TabularData1/index.html#orientation-limitations",
    "title": "LECTURE 6 - Tabular Data",
    "section": "ORIENTATION LIMITATIONS",
    "text": "ORIENTATION LIMITATIONS\n\n\nRectilinear: Scalability is limited with regard to the number of axes. (2 axes best, 3 problematic, 4+ impossible.)\nParallel: Unfamiliarity and training time.\nRadial: Perceptual limits include polar coordinate asymmetry, lower precision with angle channles compated to length channels, nonuniform sector width/size depending on radial distance. While thes limits are frequently problematic, they can sometimes be deliberately exploited. (Example: for 2 attribs of very unequal importance)\n\n\n\n\nUncovering Strengths and Weaknesses of Radial Visualizations - an Empirical Approach. Diehl, Beck and Burch. IEEE TVCG (Proc. InfoVis) 16(6):935–942, 2010."
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#chart-axes-1",
    "href": "posts/L6-TabularData1/index.html#chart-axes-1",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CHART AXES",
    "text": "CHART AXES\n\n\nLabelled axes are critical! Avoid cropping the y-axis (include 0 at bottom left) or the slope misleads the viewer.\n\n\n\n\nIf you see bullshit…"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#dual-axis-line-charts",
    "href": "posts/L6-TabularData1/index.html#dual-axis-line-charts",
    "title": "LECTURE 6 - Tabular Data",
    "section": "DUAL AXIS LINE CHARTS",
    "text": "DUAL AXIS LINE CHARTS\nThis approach is controversial. Dual axes are acceptable if they are commensurate, but beware, as they make it very easy to mislead!"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#connected-scatterplots",
    "href": "posts/L6-TabularData1/index.html#connected-scatterplots",
    "title": "LECTURE 6 - Tabular Data",
    "section": "CONNECTED SCATTERPLOTS",
    "text": "CONNECTED SCATTERPLOTS\nScatterplot with line connection marks. These are popular in journalism.\n\n\nHoriz + vert axes: value attributes\nLine connection marks: temporal order\nAlternative to dual-axis charts (horiz: time vert: two value attributes).\nempirical study suggests these are engaging, but correlations are unclear.\n\n\n\n\nhttp://steveharoz.com/research/connected_scatterplot/"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#breaking-conventions",
    "href": "posts/L6-TabularData1/index.html#breaking-conventions",
    "title": "LECTURE 6 - Tabular Data",
    "section": "BREAKING CONVENTIONS",
    "text": "BREAKING CONVENTIONS\nThe inverted y axis is evocative of blood dripping down on Poe.\n\nhttps://public.tableau.com/profile/ben.jones#!/vizhome/EdgarAllanPoeViz/EdgarAllanPoeViz"
  },
  {
    "objectID": "posts/L6-TabularData1/index.html#arranging-tabular-data",
    "href": "posts/L6-TabularData1/index.html#arranging-tabular-data",
    "title": "LECTURE 6 - Tabular Data",
    "section": "ARRANGING TABULAR DATA",
    "text": "ARRANGING TABULAR DATA\nWhat did we miss?\nggplot cheatsheet\n\n\n\nHOME"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html",
    "href": "posts/A3-PrototypeVizPortfolio/index.html",
    "title": "ASSIGNMENT 3",
    "section": "",
    "text": "Enough with the theory and conceptual mumbo jumbo! Let’s get down to making a visualization and posting it somewhere for all the world to see! The basic idea of this assignment is to set up a repository that will serve as an experimental portfolio, and then create your first novel visualization element inside the portfolio. Its gonna be fun! Or at least educational…"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#summary",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#summary",
    "title": "ASSIGNMENT 3",
    "section": "",
    "text": "Enough with the theory and conceptual mumbo jumbo! Let’s get down to making a visualization and posting it somewhere for all the world to see! The basic idea of this assignment is to set up a repository that will serve as an experimental portfolio, and then create your first novel visualization element inside the portfolio. Its gonna be fun! Or at least educational…"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#assignment",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#assignment",
    "title": "ASSIGNMENT 3",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThis assignment has two parts. The first part is technical. We’ll set up a Quarto Blog project as a new repository in your GitHub account. Then you’ll be a Blogger! Prestigious! The second part should be more fun. We are going to create your first Blog post as a visualization that explores an ACTION - TARGET pair relevant to your data set from Assignment 2."
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#part-1-technical-sorcery",
    "title": "ASSIGNMENT 3",
    "section": "PART 1 TECHNICAL SORCERY",
    "text": "PART 1 TECHNICAL SORCERY\n\n1A - Create your BLOG project.\nHopefully by now you have created / dusted off / logged in to your GitHub account. Go ahead and log in to your account on the web and leave it open in a tab in your browser. There is a really great video about the next few steps (also linked below in RESORUCES) from Posit, but I’m giving you the condensed version here.\n\nFire up RStudio.\nGo to File-&gt;New Project and then select NEW DIRECTORY.\nNow select QUARTO BLOG.\nYou are going to create a the project in a new working directory. It is best practice to put this in a senstible directory structure on your local hard drive where your other GitHub repositories also live. Here is what mine looks like:\n\n 4. I suggest making the directory name something informative, like BCB504Portfolio, but hey… If you want to call your repository HasturBoxerShorts I won’t stop you. 5. Most of you will select Knitr as your Engine, but Cody “Mr. Hacker McPythonPants” might select Jupyter. 6. Check Create a git repository. The other boxes are optional and we can talk more about them later. 7. Click CREATE PROJECT.\nNow you’ve got a BLOG template all set up! Ha Ha! Onward to Internet Fame!\n\n\n1B - Make the BLOG about you.\nWe won’t spend a ton of time here, because this will be an ongoing process. You’ll go and watch all those cool videos and tutorials this weekend to figure this out. But lets do a couple things.\n\nMaybe you should modify the about.qmd file so that your name is in there somewhere.\nMaybe you should modify the index.qmd file with a better title in the YAML header.\nMaybe you should navigate to the posts folder, open the Welcome to my blog folder, open index.qmd from that directory, and add a sentence or two.\n\n\n\n1C - Customize your first post.\n\nNavigate to the posts folder, open the post with code folder, and open index.qmd.\nReplace ALL of the content of index.qmd with the most recent version of your .qmd file from ASSIGNMENT 2. Keep the file name index.qmd. Save that file!\nMove your data files to the post with code folder.\nRender the index.qmd file from this folder. Hopefully it worked!\n\n\n\n1D - Render the BLOG as a website.\n\nIMPORTANT Open your _quarto.yml file and add output-dir: docs under project:\n\n\nThe indentations matter here.\n\nSave all the files you’ve modified.\nGo to the BUILD tab in the (probably) top right section of RStudio.\nClick RENDER WEBSITE.\nClick through your new Blog and see how it works!\n\n\n\n1E - Push to GitHub.\nThere are quite a few ways to do this part. I’m going to use GitHub Desktop, but those video will show you other ways.\n\nGo to GitHub Desktop.\nType some text in the summary box.\nClick COMMIT TO MASTER.\nClick PUSH ORIGIN.\nGo to your GitHub in your browser. You should see your new repository! Yay!\n\n\n\n1F - Make it a website with GitHub pages.\n\nIn your browser, click on your repository.\nGo to SETTINGS.\nSelect PAGES.\nSet the SOURCE option to Deploy from a branch.\nSet the BRANCH to master and the directory to docs\nDeploy that stuff and wait. Then visit your site!"
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#part-2-eldritch-visualization-ritual",
    "title": "ASSIGNMENT 3",
    "section": "PART 2 ELDRITCH VISUALIZATION RITUAL",
    "text": "PART 2 ELDRITCH VISUALIZATION RITUAL\n\n2A Define your ACTION - TARGET pair(s)\nIn [LECTURE 3] we discussed the concept of Task Abstraction in which you define the viz task that you want to help the user accomplish. This was represented as sets of ACTIONS that the user would perform (e.g. Discover, Present, Browse, Identify) on TARGETS related to the data set (e.g. Trends, Attributes, etc.).\nThink about one or two visualizations you wish to construct with your data, and try to define them in terms of ACTION - TARGET pairs. While you are at it, why don’t you update the index.qmd file of your BLOG POST with a new seciton at the bottom titled TASK ABSTRACTION, and put a sentence describing your visualizations and the ACTION - TARGET pairs they represent?\n\n\n2B Construct your Visualization\nLet’s get to work! Using whatever tools you can, code up your visualization in that new section of your BLOG post. You can check out how I approached this part in TUTORIAL 4."
  },
  {
    "objectID": "posts/A3-PrototypeVizPortfolio/index.html#resources",
    "href": "posts/A3-PrototypeVizPortfolio/index.html#resources",
    "title": "ASSIGNMENT 3",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nQuarto’s BLOG Documentation\nA YouTube Video from Posit on Building a BLOG with Quarto"
  },
  {
    "objectID": "posts/Examplepost/Example.html",
    "href": "posts/Examplepost/Example.html",
    "title": "Examples are fun",
    "section": "",
    "text": "I LIKE HOCKEY AND CANDY.\n\ndata &lt;- tribble(\n  ~from, ~to, ~value,\n  \"Martonick\", \"NCURA Workshops\", 100,\n  \"Martoncik\", \"Project Reporting\", 300,\n  \"Sheneman\", \"Open Source Data Sets\", 200,\n  \"Sheneman\", \"AI Tools\", 100,\n  \"Pal\", \"Open Source Data Sets\", 100,\n  \"Pal\", \"Common Data Model\", 100,\n  \"Martoncik\", \"Community of Practice\", 300,\n  \"Robison\", \"Project Reporting\", 200,\n  \"Brunsfeld\", \"AI Tools\", 100,\n  \"Trainer\", \"Open Source Data Sets\", 100,\n  \"Trainer\", \"NCURA Workshops\", 100,\n  \"Trainer\", \"Online Training Modules\", 150,\n  \"Robison\", \"Online Training Modules\", 50\n)\n\n\n# Using ggplot2 and ggalluvial\nggplot(data = data,\n       aes(axis1 = from, axis2 = to, y = value)) +\n  geom_alluvium(aes(fill = to)) + \n  geom_stratum() + \n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  theme_minimal() +\n  ggtitle(\"Sankey Diagram with tidyverse and ggalluvial\")\n\n\n\n\n\n\n\ndf &lt;- mtcars %&gt;%\n  make_long(cyl, vs, am, gear, carb)\n\nggplot(df, aes(x = x, \n               next_x = next_x, \n               node = node, \n               next_node = next_node,\n               fill = factor(node))) +\n  geom_sankey()"
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html",
    "href": "posts/A1-Lit-Prog/index.html",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "",
    "text": "The idea of Literate Programming is that source code that is executed as part of the program’s purpose is interspersed with documentation that describes the program’s logic. The concept of literate programming was first articulated by David Knuth in 1984. You know… back when music was good? Modern Data Science leans pretty heavily on literate programming, and to be honest, there aren’t very many good arguments as to why you WOULDN’T want to implement this approach in your own work. Bearing this in mind, we will adopt this framework for most of the activities, exercises, and assignments in this course. All of us will benefit by practicing these skills."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#summary",
    "href": "posts/A1-Lit-Prog/index.html#summary",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "",
    "text": "The idea of Literate Programming is that source code that is executed as part of the program’s purpose is interspersed with documentation that describes the program’s logic. The concept of literate programming was first articulated by David Knuth in 1984. You know… back when music was good? Modern Data Science leans pretty heavily on literate programming, and to be honest, there aren’t very many good arguments as to why you WOULDN’T want to implement this approach in your own work. Bearing this in mind, we will adopt this framework for most of the activities, exercises, and assignments in this course. All of us will benefit by practicing these skills."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "href": "posts/A1-Lit-Prog/index.html#literate-programming-publishing-systems",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LITERATE PROGRAMMING PUBLISHING SYSTEMS",
    "text": "LITERATE PROGRAMMING PUBLISHING SYSTEMS\nI’m trying to keep this course as technology agnostic as I can. The idea is that you should be practicing and building competencies in the languages and algorithms that are most useful to you. Who am I to tell you to use R instead of Python? If you have skills in a particular language I encourage you to keep using that during this course. That being said, I am going to work the examples using R and R Studio, and I will (mostly) use Quarto as the literate programming framework.\nIf all of this is new to you, no problem. Just follow along in R and Quarto and start your skill building journey with those languages.\nIf you are a Python person, great! Quarto can accommodate that language as well. If you have another preference for literate programming, such as sticking with R Markdown until the Quarto bugs are fixed, that is great. Find the framework and tools that work for you, and practice, practice, practice!\n\nQuarto\nAn open source publishing system that allows you to create websites, documents, blogs, books, publications, presentations, and more while using R, Python, Julia, or Observable. Quarto is intended to be the more functional successor of R Markdown. I intend to use Quarto for most of my work in this course.\n\n\nR Markdown\nAnother publishing system for creating all the things … websites, slides, manuscripts, dashboards, etc. While most people (including me!) instinctively think of R and Python within R Markdown, the list of supported language engines is pretty extensive.\n\nnames(knitr::knit_engines$get())\n\n [1] \"awk\"       \"bash\"      \"coffee\"    \"gawk\"      \"groovy\"    \"haskell\"  \n [7] \"lein\"      \"mysql\"     \"node\"      \"octave\"    \"perl\"      \"php\"      \n[13] \"psql\"      \"Rscript\"   \"ruby\"      \"sas\"       \"scala\"     \"sed\"      \n[19] \"sh\"        \"stata\"     \"zsh\"       \"asis\"      \"asy\"       \"block\"    \n[25] \"block2\"    \"bslib\"     \"c\"         \"cat\"       \"cc\"        \"comment\"  \n[31] \"css\"       \"ditaa\"     \"dot\"       \"embed\"     \"eviews\"    \"exec\"     \n[37] \"fortran\"   \"fortran95\" \"go\"        \"highlight\" \"js\"        \"julia\"    \n[43] \"python\"    \"R\"         \"Rcpp\"      \"sass\"      \"scss\"      \"sql\"      \n[49] \"stan\"      \"targets\"   \"tikz\"      \"verbatim\"  \"ojs\"       \"mermaid\""
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#languages-and-toolsets",
    "href": "posts/A1-Lit-Prog/index.html#languages-and-toolsets",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "LANGUAGES AND TOOLSETS",
    "text": "LANGUAGES AND TOOLSETS\nThere are quite a few, but the five that seemed to keep coming up as I prepped this course are:\n\nR\nA very powerful open source framework for statistical computing and graphics. R has a lot of base functionality, and its capabilities are increased by 100 fold with packages created by R users. Packages are the core units of R code. I’m going to use R for the vast majority of demonstrations in this course.\n\n\nPython\nPython is an open source general purpose programming language. It wasn’t developed just for statistical computing or data science, and people use this language for tons of different applications. There is no denying it has become a very powerful language for data science and data visualization.\n\n\nTableau\nTableau is proprietary software that is very powerful for creating beautiful and functional data visualizations. It can integrate with all sorts of data sources and is used a lot for analytics, especially in the business world. The downsides (that occur to me at least) are that it costs money, it is not open source, and is more of a one-trick-pony than the programming languages on this list.\n\n\nJavascript\nJavascript has been around for about 25 years, and is (I think) the world’s most popular programming language. Along with HTML and CSS, Javascript drives pretty much the entire internet. I mention Javascript here because it has the D3 library, which can create super cool interactive data visualizaitons. In my experience, the learning curve with Javascript and D3 was pretty steep. I bought a book about it once, but just haven’t been able to allocate the amount of time necessary to really start using it. Check out the gallery of examples. Amazing!\n\n\nObservable / D3\nObservable is a set of extensions to Javascript that features something called reactive runtime. This means that the code blocks are executed and compiled as they are written, and changes are implemented instantaneously. Observable is pretty great for data exploration, and is well supported by Quarto. In addition, you can use the Observable JS libraries in Quarto to access D3. We’ll use some of these tools in this course, especially when we start considering interactivity."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#assignment",
    "href": "posts/A1-Lit-Prog/index.html#assignment",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nAfter that long introduction, I suppose you are wondering what I want you to actually DO today.\nWell, I want you to set up your publishing system and preferred language on your computer. Then I want you to recreate the classic figure from Anscombe’s Quartet.\nNow, you might be asking…\n“How am I supposed to do that? You haven’t taught me how to do anything yet!”\nHere is the dirty little secret of modern education.\nThe Internet Exists.\nWhile I could use up an entire 90 minute lecture telling you how to:\n\nDownload and install R, R-Studio, and Quarto (included by default with R-Studio).\nCreate a Quarto document that will publish in the .html format\nInstall the R packages you will need\nTidy up the Anscombe’s Quartet data\nCalculate the summary statistics for each x y pair\nMake a nice little plot…\n\nI’m not going to do that.\nInstead, I want you to use the resources I point towards, or other resources that make more sense to you, to figure out how to do those things."
  },
  {
    "objectID": "posts/A1-Lit-Prog/index.html#resources",
    "href": "posts/A1-Lit-Prog/index.html#resources",
    "title": "ASSIGNMENT 1 - Fun with literate programming.",
    "section": "RESOURCES",
    "text": "RESOURCES\nTidyverse and Anscombe’s Quartet\nHandy cheat-sheets for many different R packages\nTutorial 1 - Literate Programming\nTutorial 2 - Literate Programming and Anscombe’s Quartet\nTutorial 3 - Python"
  },
  {
    "objectID": "posts/L1-Intro/index.html#who-am-i",
    "href": "posts/L1-Intro/index.html#who-am-i",
    "title": "LECTURE 1 - INTRO",
    "section": "WHO AM I?",
    "text": "WHO AM I?\nBarrie Robison\nDepartment of Biological Sciences\nInstitute for Interdisicplinary Data Sciences\nPolymorphic Games\nUniversity of Idaho"
  },
  {
    "objectID": "posts/L1-Intro/index.html#who-are-you",
    "href": "posts/L1-Intro/index.html#who-are-you",
    "title": "LECTURE 1 - INTRO",
    "section": "WHO ARE YOU?",
    "text": "WHO ARE YOU?\n“…The course is designed to be”discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests…”\nBriefly:\n\nYour name\nYour discipline\nYour degree progress\nYour technical proficiency with data visualization"
  },
  {
    "objectID": "posts/L1-Intro/index.html#course-summary",
    "href": "posts/L1-Intro/index.html#course-summary",
    "title": "LECTURE 1 - INTRO",
    "section": "COURSE SUMMARY",
    "text": "COURSE SUMMARY\nStudents completing this course will be able to:\n\n\nDescribe and manipulate tabular, network, and spatial data; transform these data into a form suitable for visualization.\nMake effective data visualization design choices related to marks and channels, spatial arrangement, and components of color.\nDesign effective data visualizations for tabular, network, and spatial data with quantitative and categorical attributes.\nImplement their data visualization designs using existing tools in R (or other toolkits preferred by the student).\nExplain whether a visual encoding is perceptually appropriate for a specific combination of task and data.\nDemonstrate their skills with at least two novel visualizations suitable suitable for inclusion in an online Data Science Portfolio.\n\nThe course materials are located on Canvas and the course website."
  },
  {
    "objectID": "posts/L1-Intro/index.html#visualization",
    "href": "posts/L1-Intro/index.html#visualization",
    "title": "LECTURE 1 - INTRO",
    "section": "VISUALIZATION",
    "text": "VISUALIZATION\nComputers provide visual representations of datasets designed to help people carry out tasks more effectively.\nTamara Munzner\nDepartment of Computer Science\nInfoVis Group\nUniversity of British Columbia"
  },
  {
    "objectID": "posts/L1-Intro/index.html#the-human",
    "href": "posts/L1-Intro/index.html#the-human",
    "title": "LECTURE 1 - INTRO",
    "section": "THE HUMAN",
    "text": "THE HUMAN\nWhy have a human in the loop?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\nWe don’t need visualization when a trusted fully automatic solution exists.\nVisualization is suitable when there is a need to augment human capabilities rather than replace people with computational decision-making methods."
  },
  {
    "objectID": "posts/L1-Intro/index.html#when-to-visualize",
    "href": "posts/L1-Intro/index.html#when-to-visualize",
    "title": "LECTURE 1 - INTRO",
    "section": "WHEN TO VISUALIZE",
    "text": "WHEN TO VISUALIZE\nVisualization is useful when:\n\n\nThe analysis problem is ill-specified and we don’t know exactly what questions to ask in advance.\nWe are interested in long-term use for end users (ex: exploratory analysis of scientific data).\nWe are presenting known results (ex: DATA JOURNALISM - New York Times Upshot).\nWe need a stepping stone to assess requirements before developing models.\nDevelopers of an automatic solution want to refine & determine parameters.\nWe need to help end users of automatic solutions verify and build trust."
  },
  {
    "objectID": "posts/L1-Intro/index.html#the-representation",
    "href": "posts/L1-Intro/index.html#the-representation",
    "title": "LECTURE 1 - INTRO",
    "section": "THE REPRESENTATION",
    "text": "THE REPRESENTATION\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\nEXTERNAL REPRESENTATIONS: Replace cognition with perception."
  },
  {
    "objectID": "posts/L1-Intro/index.html#why-depend-on-vision",
    "href": "posts/L1-Intro/index.html#why-depend-on-vision",
    "title": "LECTURE 1 - INTRO",
    "section": "WHY DEPEND ON VISION?",
    "text": "WHY DEPEND ON VISION?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\nThe human visual system is a high-bandwidth channel to the brain.\nOverview is possible due to background processing, providing the subjective experience of seeing everything simultaneously.\nSignificant processing occurs in parallel and pre-attentively.\nWhat about sound? lower bandwidth and different semantics, overview not supported, subjective experience of sequential stream.\nWhat about touch/haptics? impoverished record/replay capacity, only very low-bandwidth communication thus far.\nWhat about taste, smell? no viable record/replay devices."
  },
  {
    "objectID": "posts/L1-Intro/index.html#why-represent-all-the-data",
    "href": "posts/L1-Intro/index.html#why-represent-all-the-data",
    "title": "LECTURE 1 - INTRO",
    "section": "WHY REPRESENT (ALL THE) DATA?",
    "text": "WHY REPRESENT (ALL THE) DATA?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\nsummaries lose information\ndetails matter\nconfirm expected and find unexpected patterns\nassess validity of statistical model\nANSCOMBE’S QUARTET is a fun example that we shall use to illustrate these points!"
  },
  {
    "objectID": "posts/L1-Intro/index.html#anscombes-quartet",
    "href": "posts/L1-Intro/index.html#anscombes-quartet",
    "title": "LECTURE 1 - INTRO",
    "section": "ANSCOMBE’S QUARTET",
    "text": "ANSCOMBE’S QUARTET\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(datasets)\nlibrary(tidyverse)\nlibrary(dplyr)\ndatasets::anscombe\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\n\nAnscombe’s Quartet\nThe four x-y pairs have identical summary statistics.\n\n\nCode\ntidy_anscombe &lt;- anscombe %&gt;%\n pivot_longer(cols = everything(),\n              names_to = c(\".value\", \"set\"),\n              names_pattern = \"(.)(.)\")\ntidy_anscombe_summary &lt;- tidy_anscombe %&gt;%\n  group_by(set) %&gt;%\n  summarise(across(.cols = everything(),\n                   .fns = lst(min,max,median,mean,sd,var),\n                   .names = \"{col}_{fn}\"))\n#&gt; `summarise()` ungrouping output (override with `.groups` argument)\n\nvars&lt;-c(\"set\", \"x_mean\", \"x_var\",  \"y_mean\", \"y_var\")\nthing&lt;- as.data.frame(tidy_anscombe_summary[vars])\nknitr::kable(thing)\n\n\n\n\n\nset\nx_mean\nx_var\ny_mean\ny_var\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249"
  },
  {
    "objectID": "posts/L1-Intro/index.html#viz-matters",
    "href": "posts/L1-Intro/index.html#viz-matters",
    "title": "LECTURE 1 - INTRO",
    "section": "VIZ MATTERS",
    "text": "VIZ MATTERS\n\n\nCode\nggplot(tidy_anscombe,\n       aes(x = x,\n           y = y)) +\n  geom_point() +\n  geom_point(data = tidy_anscombe_summary, aes(x=x_mean, y = y_mean, color = \"red\", size = 5),\n             show.legend = FALSE)+\n  facet_wrap(~set) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nLearn more: TIDY ANSCOMBE"
  },
  {
    "objectID": "posts/L1-Intro/index.html#resource-limitations",
    "href": "posts/L1-Intro/index.html#resource-limitations",
    "title": "LECTURE 1 - INTRO",
    "section": "RESOURCE LIMITATIONS",
    "text": "RESOURCE LIMITATIONS\nVisualization designers must take into account three very different kinds of resource limitations:\n\nLimitations of computers.\nLimitations of humans.\nLimitations of displays."
  },
  {
    "objectID": "posts/L1-Intro/index.html#computational-limits",
    "href": "posts/L1-Intro/index.html#computational-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "COMPUTATIONAL LIMITS",
    "text": "COMPUTATIONAL LIMITS\nCPU time\nSystem Memory"
  },
  {
    "objectID": "posts/L1-Intro/index.html#display-limits",
    "href": "posts/L1-Intro/index.html#display-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "DISPLAY LIMITS",
    "text": "DISPLAY LIMITS\nPixels are precious and are the most constrained resource.\n\nInformation Density: ratio of space used to encode information vs unused whitespace.\nThere is a tradeoff between clutter and wasting space.\nDesigner must find the sweet spot between dense and sparse."
  },
  {
    "objectID": "posts/L1-Intro/index.html#human-limits",
    "href": "posts/L1-Intro/index.html#human-limits",
    "title": "LECTURE 1 - INTRO",
    "section": "HUMAN LIMITS",
    "text": "HUMAN LIMITS\n\nTime\nMemory\nAttention\n\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#vad-model",
    "href": "posts/L4-Marks-Channels/index.html#vad-model",
    "title": "LECTURE 4",
    "section": "VAD MODEL",
    "text": "VAD MODEL"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#understand-the-data",
    "href": "posts/L4-Marks-Channels/index.html#understand-the-data",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE DATA",
    "text": "UNDERSTAND THE DATA\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#understand-the-task",
    "href": "posts/L4-Marks-Channels/index.html#understand-the-task",
    "title": "LECTURE 4",
    "section": "UNDERSTAND THE TASK",
    "text": "UNDERSTAND THE TASK\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#visual-encoding",
    "href": "posts/L4-Marks-Channels/index.html#visual-encoding",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING",
    "text": "VISUAL ENCODING\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#other-frameworks",
    "href": "posts/L4-Marks-Channels/index.html#other-frameworks",
    "title": "LECTURE 4",
    "section": "OTHER FRAMEWORKS",
    "text": "OTHER FRAMEWORKS\n\nThe Tidyverse\nThe Grammar of Graphics\nTufte"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#tidyverse",
    "href": "posts/L4-Marks-Channels/index.html#tidyverse",
    "title": "LECTURE 4",
    "section": "TIDYVERSE",
    "text": "TIDYVERSE\nR packages for data science:\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The best way to explore and understand the tidyverse is with cheetsheets, like this one for tidyr!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "href": "posts/L4-Marks-Channels/index.html#grammar-of-graphics",
    "title": "LECTURE 4",
    "section": "GRAMMAR OF GRAPHICS",
    "text": "GRAMMAR OF GRAPHICS\nThe ggplot2 cheatsheet!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#tufte",
    "href": "posts/L4-Marks-Channels/index.html#tufte",
    "title": "LECTURE 4",
    "section": "TUFTE",
    "text": "TUFTE\nTufte’s Website\nA Quarto Page Layout Example"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#analysis-framework",
    "href": "posts/L4-Marks-Channels/index.html#analysis-framework",
    "title": "LECTURE 4",
    "section": "ANALYSIS FRAMEWORK",
    "text": "ANALYSIS FRAMEWORK\nFour levels, three questions\n\n\n\nDomain situation defines the target users.\nAbstraction translate from specifics of domain to vocabulary of vis\n\nWHAT is shown? data abstraction\nWHY is the user looking at it? task abstraction\n\nIdiom defines the visualization\n\nHOW is it shown?\n\nvisual encoding idiom: how to draw\ninteraction idiom: how to manipulate\n\n\nAlgorithm creates the visualization\n\nevaluated with computational efficiency"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#encoding",
    "href": "posts/L4-Marks-Channels/index.html#encoding",
    "title": "LECTURE 4",
    "section": "ENCODING",
    "text": "ENCODING\nWe are defining the structure of the visualization (the idiom).\nTo do this, we use MARKS and CHANNELS:\n\nMARKS represent ITEMS or LINKS (aka OBSERVATIONS)\nCHANNELS change the appearance of MARKS based on ATTRIBUTES (aka VARIABLES)"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#marks-for-items",
    "href": "posts/L4-Marks-Channels/index.html#marks-for-items",
    "title": "LECTURE 4",
    "section": "MARKS FOR ITEMS",
    "text": "MARKS FOR ITEMS"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#marks-for-links",
    "href": "posts/L4-Marks-Channels/index.html#marks-for-links",
    "title": "LECTURE 4",
    "section": "MARKS FOR LINKS",
    "text": "MARKS FOR LINKS\n\n Bubblesets\n Force Directed Graph"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "href": "posts/L4-Marks-Channels/index.html#observable-in-quarto",
    "title": "LECTURE 4",
    "section": "OBSERVABLE IN QUARTO!",
    "text": "OBSERVABLE IN QUARTO!\n\n\nCode\nd3 = require(\"d3@7\")\n\n\nchart = ForceGraph(miserables, {\n  nodeId: d =&gt; d.id,\n  nodeGroup: d =&gt; d.group,\n  nodeTitle: d =&gt; `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l =&gt; Math.sqrt(l.value),\n  width,\n  height: 1000,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d =&gt; d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) =&gt; source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) =&gt; target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 1000, // outer width, in pixels\n  height = 1000, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) =&gt; N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) =&gt; ({id: N[i]}));\n  links = d3.map(links, (_, i) =&gt; ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) =&gt; N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) =&gt; W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) =&gt; L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) =&gt; color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) =&gt; T[i]);\n  if (invalidation != null) invalidation.then(() =&gt; simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d =&gt; d.source.x)\n      .attr(\"y1\", d =&gt; d.source.y)\n      .attr(\"x2\", d =&gt; d.target.x)\n      .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n      .attr(\"cx\", d =&gt; d.x)\n      .attr(\"cy\", d =&gt; d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\""
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channels",
    "href": "posts/L4-Marks-Channels/index.html#channels",
    "title": "LECTURE 4",
    "section": "CHANNELS",
    "text": "CHANNELS\n\n\n\nCHANNELS control the appearance of MARKS.\nThey are proportional to or based on ATTRIBUTES (aka VARIABLES).\nTheir properties differ in the type and amount of information that can be conveyed to the human perceptual system."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#this-is-important",
    "href": "posts/L4-Marks-Channels/index.html#this-is-important",
    "title": "LECTURE 4",
    "section": "THIS IS IMPORTANT",
    "text": "THIS IS IMPORTANT\nChannel properties differ in the type and amount of information that can be conveyed to the human perceptual system."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "href": "posts/L4-Marks-Channels/index.html#visual-encoding-example",
    "title": "LECTURE 4",
    "section": "VISUAL ENCODING EXAMPLE",
    "text": "VISUAL ENCODING EXAMPLE\nLet’s analyze the idiom structures below in terms of marks and channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#analyze-the-marks-and-channels",
    "href": "posts/L4-Marks-Channels/index.html#analyze-the-marks-and-channels",
    "title": "LECTURE 4",
    "section": "ANALYZE THE MARKS AND CHANNELS",
    "text": "ANALYZE THE MARKS AND CHANNELS\nMarks are defined by the ITEMS or OBSERVATIONS they represent.\nChannels are defined by the visually detectable properties that are mapped on to ATTRIBUTES or VARIABLES.\n\n\n\n\n\n\nKonrad’s Trees\n\n\n\n\n\n\n\nRobyn’s Books\n\n\n\n\n\n\n\nYaotian’s Wheat"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#redundant-encoding",
    "href": "posts/L4-Marks-Channels/index.html#redundant-encoding",
    "title": "LECTURE 4",
    "section": "REDUNDANT ENCODING",
    "text": "REDUNDANT ENCODING\n\n\nUses multiple channels for the same attribute.\n\nSends a stronger message\nUses up channels\nBonus points if you can identify BOTH channels in this figure!"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#wheat-growth-by-area",
    "href": "posts/L4-Marks-Channels/index.html#wheat-growth-by-area",
    "title": "LECTURE 4",
    "section": "WHEAT GROWTH BY AREA*",
    "text": "WHEAT GROWTH BY AREA*\nBoth of Yaotian’s plots contain Redundant Encoding. Is this approach equally valuable for both plots?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#choosing-channels",
    "href": "posts/L4-Marks-Channels/index.html#choosing-channels",
    "title": "LECTURE 4",
    "section": "CHOOSING CHANNELS",
    "text": "CHOOSING CHANNELS\n\nEXPRESSIVENESS\n\nThe visual encoding should express all of, and only, the information in the dataset attributes.\n\nEFFECTIVENESS\n\nChannels differ in accuracy of perception.\nThe importance of the attribute should match the salience of the channel (its noticability)."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#expressiveness",
    "href": "posts/L4-Marks-Channels/index.html#expressiveness",
    "title": "LECTURE 4",
    "section": "EXPRESSIVENESS",
    "text": "EXPRESSIVENESS\nThe advantages and disadvantages of jitter plots. How might this relate to the idea of expressiveness?\n\nHeidi’s Cytoswine DataEXPRESSIVENESS: The visual encoding should express all of, and only, the information in the dataset attributes."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channel-effectiveness-rankings",
    "href": "posts/L4-Marks-Channels/index.html#channel-effectiveness-rankings",
    "title": "LECTURE 4",
    "section": "CHANNEL EFFECTIVENESS RANKINGS",
    "text": "CHANNEL EFFECTIVENESS RANKINGS\n\nNote that spatial position ranks high for both types of channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#grouping",
    "href": "posts/L4-Marks-Channels/index.html#grouping",
    "title": "LECTURE 4",
    "section": "GROUPING",
    "text": "GROUPING\n\n\n\nContainment\nConnection\nProximity\n\nSame spatial region.\n\nSimilarity\n\nSame values as other channels."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#summary-so-far",
    "href": "posts/L4-Marks-Channels/index.html#summary-so-far",
    "title": "LECTURE 4",
    "section": "SUMMARY SO FAR",
    "text": "SUMMARY SO FAR"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "href": "posts/L4-Marks-Channels/index.html#channel-effectiveness",
    "title": "LECTURE 4",
    "section": "CHANNEL EFFECTIVENESS",
    "text": "CHANNEL EFFECTIVENESS\n\nAccuracy: how precisely can we tell the difference between encoded items?\nDiscriminability: how many unique steps can we perceive?\nSeparability: is our ability to use this channel affected by another one?\nPopout: can things jump out using this channel?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy-theory",
    "href": "posts/L4-Marks-Channels/index.html#accuracy-theory",
    "title": "LECTURE 4",
    "section": "ACCURACY (THEORY)",
    "text": "ACCURACY (THEORY)\nSteven’s Psychophisical Power Law: \\(S=I^N\\)\n\n\n\n\n\n\n\n\n\n\n\n\nLENGTH (N=1)\nELECTRIC SHOCK (N=3.5)\nSATURATION (N=1.7)\nAREA (N=0.7)\nBRIGHTNESS (N=0.5)"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "href": "posts/L4-Marks-Channels/index.html#accuracy-experimental",
    "title": "LECTURE 4",
    "section": "ACCURACY (EXPERIMENTAL)",
    "text": "ACCURACY (EXPERIMENTAL)\n\n\n\n\n[Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods]"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#accuracy",
    "href": "posts/L4-Marks-Channels/index.html#accuracy",
    "title": "LECTURE 4",
    "section": "ACCURACY?",
    "text": "ACCURACY?\nDepends on the task. Which genre has the most game titles? Which genres are the top 4 in terms of game titles? Are there more Puzzle games than Strategy games?\n\nGeraline’s Video Games"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#discriminability",
    "href": "posts/L4-Marks-Channels/index.html#discriminability",
    "title": "LECTURE 4",
    "section": "DISCRIMINABILITY",
    "text": "DISCRIMINABILITY\nHow many usable steps are in the channel? Are the differences between items perceptible to the human as intended?"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#discriminability-and-colors",
    "href": "posts/L4-Marks-Channels/index.html#discriminability-and-colors",
    "title": "LECTURE 4",
    "section": "DISCRIMINABILITY and COLORS",
    "text": "DISCRIMINABILITY and COLORS\n\nGeraline’s Gaming Platforms"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "href": "posts/L4-Marks-Channels/index.html#separability-vs-integrality",
    "title": "LECTURE 4",
    "section": "SEPARABILITY VS INTEGRALITY",
    "text": "SEPARABILITY VS INTEGRALITY\nSeparable channels are orthogonal and independent. Integral channels are inextricably combined. Attempts to encode different information with integral channels creates Interference.\n\n\nFigure 5.10. Pairs of visual channels fall along a continuum from fully separable to intrinsically integral. Color and location are separable channels well suited to encode different data attributes for two different groupings that can be selectively attended to. However, size interacts with hue, which is harder to perceive for small objects. The horizontal size and and vertical size channels are automatically fused into an integrated perception of area, yielding three groups. Attempts to code separate information along the red and green axes of the RGB color space fail, because we simply perceive four different hues."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#separability",
    "href": "posts/L4-Marks-Channels/index.html#separability",
    "title": "LECTURE 4",
    "section": "SEPARABILITY",
    "text": "SEPARABILITY\n\nRedundancy may be desirable, but area interferes with hue, with larger shapes having more visual salience."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout",
    "href": "posts/L4-Marks-Channels/index.html#popout",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\nVISUAL POPOUT is often called preattentive processing or tunable detection.\n\n\nfind the red dot! How long does it take?\nPopout results from our low-level visual system performing massively parallel processing on certain visual channels, eliminating the need for the viewer to consciously direct attention to items one by one (serial search).\n\nFigure 5.11. Visual popout. (a) The red circle pops out from a small set of blue circles. (b) The red circle pops out from a large set of blue circles just as quickly. (c) The red circle also pops out from a small set of square shapes, although a bit slower than with color. (d) The red circle also pops out of a large set of red squares. (e) The red circle does not take long to find from a small set of mixed shapes and colors. (f) The red circle does not pop out from a large set of red squares and blue circles, and it can only be found by searching one by one through all the objects."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout-1",
    "href": "posts/L4-Marks-Channels/index.html#popout-1",
    "title": "LECTURE 4",
    "section": "POPOUT",
    "text": "POPOUT\n\n\nMany channels are compatible with preattentive processing and facilitate popout:\n\ntilt\nsize\nshape\nproximity\nshadow direction\n\nBut not all!\n\nExample: parallel line pairs do not pop out from tilted pairs."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#popout-goes-the-weevil",
    "href": "posts/L4-Marks-Channels/index.html#popout-goes-the-weevil",
    "title": "LECTURE 4",
    "section": "POPOUT GOES THE WEEVIL?",
    "text": "POPOUT GOES THE WEEVIL?\n\n\n\n\n\n\nLucas Weevils before chemicals\n\n\n\n\n\n\n\nLucas Weevils after chemicals"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-vs-absolute-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE VS ABSOLUTE JUDGEMENTS",
    "text": "RELATIVE VS ABSOLUTE JUDGEMENTS\nThe human perceptual system is fundamentally based on relative judgements, not absolute ones. This is why accuracy increases with common frame/scale and alignment.\nWeber’s Law: The detectable difference in stimulus intensity \\(I\\) as a fixed percentage \\(K\\) of the object magnitude: \\(dI/I=K\\) .\n\nThe filled rectangles differ in length by 1:9, and it is therefore difficult to detect the difference without aligment. The white rectangles differ in length by 1:2, it is easier to see this difference even when the objects are unaligned."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE JUDGEMENTS",
    "text": "RELATIVE JUDGEMENTS\n\nHeidi’s Microbiomes"
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-luminance-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE LUMINANCE JUDGEMENTS",
    "text": "RELATIVE LUMINANCE JUDGEMENTS\nHuman perception of luminance is completely contextual, and is based on contrast with surrounding colors."
  },
  {
    "objectID": "posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "href": "posts/L4-Marks-Channels/index.html#relative-color-judgements",
    "title": "LECTURE 4",
    "section": "RELATIVE COLOR JUDGEMENTS",
    "text": "RELATIVE COLOR JUDGEMENTS\nOur visual system evolved to provide color constancy so that the same surface is identifiable across a broad set of illumination conditions, even though a physical light meter would yield very different readings. While the visual system works very well in natural environments, many of its mechanisms work against simple approaches to visually encoding information with color.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.15 shows two colorful cubes. In Figure 5.15(a) corresponding squares both appear to be red. In Figure 5.15(b), masks show that the tile color in the image apparently illuminated by a yellowish light source is actually orange, and for the bluish light the tiles are actually purple.\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "posts/T4-BarriesData/index.html",
    "href": "posts/T4-BarriesData/index.html",
    "title": "TUTORIAL 4",
    "section": "",
    "text": "In this assignment (detalied here), I will identify, import, describe, and host a data set that will be used throughout the remainder of the BCB 520 course for Data Visualizations.\n\n\nI’ve chosen a subset of a large dataset produced by our evolutionary video game, Project Hastur. We built Project Hastur to be an evolutionary video game, and we are bold in our assertions of that fact. But we haven’t really published any evidence that the evolutionary model works. This data set is the beginning of that exercise.\n\n\n\n\n\n\nNote\n\n\n\nPROJECT HASTUR creates a unique challenge by combining elements of 3D tower defense and real-time strategy with biological evolution. Fight against alien Proteans that evolve - using biologically accurate models of evolution - to overcome the player’s defenses.\nEach creature you will face has its own unique genome controlling its abilities, behaviors, and appearance. Those that make it the furthest and do the most damage to your defenses have the most offspring you will have to defeat in the next generation. The result? Evolution responds to the player’s strategy and makes every playthrough a unique experience.\nUse four upgradable turret classes, plus airstrikes and combat robots, to fight against the Protean invasion. Make strategic decisions about which turrets to build, when to upgrade them, and where to place them on the hex grid. A well-timed airstrike can change the flow of the game, but you’ll have to wait before you can use it again. Unlock powerful upgrades for each turret class as you move across the Nyx system. As you play, the Proteans evolve new weapon resistances, behaviors, and movement capabilities to better destroy your defenses.\nIn CAMPAIGN MODE, battle through a series of maps as a military defense commander to protect the planet Nyx from the ever-evolving threat of the Proteans. Unlock weapons and upgrades and use them to fight against the Protean swarm and learn about the mysteries of Project Hastur.\nIn EXPERIMENT MODE, choose any map, tweak the parameters, and play infinitely to see what you can evolve. Change the number of creatures and the parameters of evolution, make your turrets invincible, or crank up the biomatter and experiment with the most powerful turret upgrades. Experiment mode lets you experience Project Hastur your way.\n\n\n\n\nThe data were collected by running Project Hastur in Experiment mode using four predefined conditions:\nI: The CHIP SHREDDER towers when Fitness Functions were turned ON and Civilians were PRESENT.\nH: The CHIP SHREDDER towers when Fitness Functions were turned OFF and Civilians were PRESENT.\nG: The CHIP SHREDDER towers when Fitness Functions were turned ON and Civilians were ABSENT.\nK: The AUTOCANNON towers when Fitness Functions were turned ON and Civilians were ABSENT.\nEach experimental condition was run 9 times (9 replicates).\n\n\n\n\nI’m going to use the vroom package to import multiple files. Each file is a replicate and the filename tells us about the experimental condition. Below I convert the filename variable (I named it path) into a a single categorical attribute called Fit that uses the letter codes above.\n\n\nCode\nlibrary(vroom)\nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(readxl)\nfiles &lt;- fs::dir_ls(glob = \"*.csv\")\n\nHastur &lt;- vroom(files, id = \"path\", \n                col_select = c(path, Generation, ID, Origin, AsexualReproduction, Fitness, Health,\n                               SightRange, Armor, Damage, WalkSpeed, RunSpeed, Acceleration, \n                               TurnRate, Attraction0, Attraction1, Attraction2))\n\nHastur$Fit &lt;- str_split_i(Hastur$path, pattern = \"\", 1)\nHastur$replicate &lt;- str_split_i(Hastur$path, pattern = \"\", 4)\n\n\nThe glimpse command in the Tidyverse package is a nice way to summarize the data frame:\n\n\nCode\nglimpse(Hastur)\n\n\nRows: 412,246\nColumns: 19\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n$ path                &lt;chr&gt; \"GSC1.csv\", \"GSC1.csv\", \"GSC1.csv\", \"GSC1.csv\", \"G…\n$ Generation          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ID                  &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ Origin              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ AsexualReproduction &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Fitness             &lt;dbl&gt; 57.83508, 66.87755, 66.14652, 65.88873, 62.12119, …\n$ Health              &lt;dbl&gt; 1006, 1012, 1011, 992, 983, 1020, 982, 963, 996, 9…\n$ SightRange          &lt;dbl&gt; 9.952521, 10.096590, 9.954091, 10.066170, 10.02955…\n$ Armor               &lt;dbl&gt; 0.05081077, 0.05080924, 0.05010696, 0.04903501, 0.…\n$ Damage              &lt;dbl&gt; 49, 51, 51, 50, 50, 51, 50, 49, 51, 50, 49, 50, 49…\n$ WalkSpeed           &lt;dbl&gt; 6.930266, 7.034348, 6.970608, 6.903729, 6.962081, …\n$ RunSpeed            &lt;dbl&gt; 20.03562, 19.88800, 19.80754, 19.94738, 19.95583, …\n$ Acceleration        &lt;dbl&gt; 14.70648, 15.05868, 14.85994, 14.89853, 15.01570, …\n$ TurnRate            &lt;dbl&gt; 356.3890, 361.2032, 358.9919, 361.8476, 360.6143, …\n$ Attraction0         &lt;dbl&gt; 0.158477500, -0.007134318, -0.063494000, 0.0125864…\n$ Attraction1         &lt;dbl&gt; -0.070695680, 0.059872570, -0.003332689, 0.0458469…\n$ Attraction2         &lt;dbl&gt; -0.108705200, 0.015018780, -0.007600136, 0.0120656…\n$ Fit                 &lt;chr&gt; \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", …\n$ replicate           &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\n\n\n\n\n\n\nWhat we have here is a (big) Flat Table. The Items are the rows, and each row is an individual alien enemy that existed during one of the replicates. Each Item (alien) is described by Attributes, which are arranged in the columns.\n\n\n\nThe glimpse we did in the preceding section gives us a hint as to what each attribute type might be. Let’s flesh that out a bit though. I’m going to create a new data frame that describes the attributes.\n\n\nCode\nAttributes &lt;- read_excel(\"Attributes.xlsx\")\nknitr::kable(Attributes)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nNote\n\n\n\n\npath\nCategorical\neach File Name is a unique replicate\n\n\nGeneration\nQuantitative\nEach Enemy Wave is a Generation\n\n\nID\nOrdinal\nEach enemy has a unique ID within each replicate\n\n\nOrigin\nCategorical\nThe hive from which the enemy was spawned\n\n\nAsexualReproduction\nCategorical\nWas the enemy spawned by infectiing a civilian?\n\n\nFitness\nQuantitative\nThe value of Fitness is used to determine probability of reproduction\n\n\nHealth\nQuantitative\nHit Points\n\n\nSightRange\nQuantitative\nHow far they can see civilians, towers, etc\n\n\nArmor\nQuantitative\nresistance to physical damage\n\n\nDamage\nQuantitative\nhow much damage they do to towers\n\n\nWalkSpeed\nQuantitative\nhow fast they can walk\n\n\nRunSpeed\nQuantitative\nhow fast they can run\n\n\nAcceleration\nQuantitative\nhow fast they can transition from walking to running\n\n\nTurnRate\nQuantitative\nhow fast they can turn\n\n\nAttraction0\nQuantitative\nnegative values is attraction to civilians, positive is avoidance\n\n\nAttraction1\nQuantitative\nnegative values is attraction to towers, positive is avoidance\n\n\nAttraction2\nQuantitative\nnegative values is attraction to the base, positive is avoidance\n\n\nFit\nCategorical\nI, H, G, or K - an inscrutable code about fitness conditions\n\n\n\n\n\nThe problem here is my inscrutable filename codes for that Fit variable. Those letter codes actually contain information on a couple hidden variables. I’m going to create a new variable called Gun and another called Civilians. I’ll add those to the main data file and also the Data Dicttionary.\n\n\nCode\nHastur$Gun &lt;- \"CHIP SHREDDER\"\nHastur$Civilians &lt;- \"Present\"\n  \n\n  Hastur$Gun[Hastur$Fit==\"K\"]&lt;- \"AUTOCANNON\"\n     \n  Hastur$Civilians[Hastur$Fit==\"K\" | Hastur$Fit ==\"G\"] &lt;- \"ABSENT\"\n     \n\n  Attributes&lt;-rbind(Attributes, c(\"Gun\",\"Categorical\", \"Autocannon or Chip Shredder\"))\n  Attributes&lt;-rbind(Attributes, c(\"Civilians\",\"Categorical\", \"Present or Absent\"))\n\n\n\n\nCode\n  knitr::kable(Attributes)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nNote\n\n\n\n\npath\nCategorical\neach File Name is a unique replicate\n\n\nGeneration\nQuantitative\nEach Enemy Wave is a Generation\n\n\nID\nOrdinal\nEach enemy has a unique ID within each replicate\n\n\nOrigin\nCategorical\nThe hive from which the enemy was spawned\n\n\nAsexualReproduction\nCategorical\nWas the enemy spawned by infectiing a civilian?\n\n\nFitness\nQuantitative\nThe value of Fitness is used to determine probability of reproduction\n\n\nHealth\nQuantitative\nHit Points\n\n\nSightRange\nQuantitative\nHow far they can see civilians, towers, etc\n\n\nArmor\nQuantitative\nresistance to physical damage\n\n\nDamage\nQuantitative\nhow much damage they do to towers\n\n\nWalkSpeed\nQuantitative\nhow fast they can walk\n\n\nRunSpeed\nQuantitative\nhow fast they can run\n\n\nAcceleration\nQuantitative\nhow fast they can transition from walking to running\n\n\nTurnRate\nQuantitative\nhow fast they can turn\n\n\nAttraction0\nQuantitative\nnegative values is attraction to civilians, positive is avoidance\n\n\nAttraction1\nQuantitative\nnegative values is attraction to towers, positive is avoidance\n\n\nAttraction2\nQuantitative\nnegative values is attraction to the base, positive is avoidance\n\n\nFit\nCategorical\nI, H, G, or K - an inscrutable code about fitness conditions\n\n\nGun\nCategorical\nAutocannon or Chip Shredder\n\n\nCivilians\nCategorical\nPresent or Absent\n\n\n\n\n\n\n\n\n\nI’m publishing to GitHub! We will elaborate on this step as everyone progresses through the assignment.\n\n\n\nFor this data set, I am currently defining the user as … me! My hypothesis is that the two Fitness conditions create different evolutionary outcomes of the aliens in Project Hastur. Some relevant ACTION TARGET pairs might be:\nDISCOVER TRENDS\nDISCOVER DISTRIBUTION\nDISCOVER SIMILARITY\nCOMPARE TRENDS\nCOMPARE DISTRIBUTION\nI’m going to try COMPARE TRENDS. I want to COMPARE the TREND in Health over time (Generation) between the two Gun types. To do this, I’ll create a scatterplot, faceted by Gun. I’m suspicious that Acceleration is involved somehow, so I’m coloring with that variable.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Health))+\n  geom_point(aes(color=Acceleration), alpha = 0.01, size = 1)+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_grid(replicate~Gun)\n\n\n\n\n\n\n\n\n\nInteresting… it looks like a clear trend for Health to increase under the withering fire of the AUTO CANNONS, but not when the player uses the CHIP SHREDDER. It is a bit hard to see what is going on with Acceleration, so let’s reverse the graphs so that we plot Acceleration on the y axis but color by Health.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Acceleration))+\n  geom_point(aes(color=Health), alpha = 0.01, size = 1)+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_grid(replicate~Gun)\n\n\n\n\n\n\n\n\n\nI’m now confident that the replicates within each Gun type are pretty similar, and I can SUMMARIZE the individual data points. This will help with the COMPARE TRENDS task, I think.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Health))+\n  geom_point(aes(color=Acceleration), alpha = 0.01, size = 1)+\n  geom_smooth()+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_wrap(~Gun)\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Acceleration))+\n  geom_point(aes(color=Health), alpha = 0.01, size = 1)+\n  geom_smooth()+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_wrap(~Gun)\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "posts/T4-BarriesData/index.html#my-dataset",
    "href": "posts/T4-BarriesData/index.html#my-dataset",
    "title": "TUTORIAL 4",
    "section": "",
    "text": "I’ve chosen a subset of a large dataset produced by our evolutionary video game, Project Hastur. We built Project Hastur to be an evolutionary video game, and we are bold in our assertions of that fact. But we haven’t really published any evidence that the evolutionary model works. This data set is the beginning of that exercise.\n\n\n\n\n\n\nNote\n\n\n\nPROJECT HASTUR creates a unique challenge by combining elements of 3D tower defense and real-time strategy with biological evolution. Fight against alien Proteans that evolve - using biologically accurate models of evolution - to overcome the player’s defenses.\nEach creature you will face has its own unique genome controlling its abilities, behaviors, and appearance. Those that make it the furthest and do the most damage to your defenses have the most offspring you will have to defeat in the next generation. The result? Evolution responds to the player’s strategy and makes every playthrough a unique experience.\nUse four upgradable turret classes, plus airstrikes and combat robots, to fight against the Protean invasion. Make strategic decisions about which turrets to build, when to upgrade them, and where to place them on the hex grid. A well-timed airstrike can change the flow of the game, but you’ll have to wait before you can use it again. Unlock powerful upgrades for each turret class as you move across the Nyx system. As you play, the Proteans evolve new weapon resistances, behaviors, and movement capabilities to better destroy your defenses.\nIn CAMPAIGN MODE, battle through a series of maps as a military defense commander to protect the planet Nyx from the ever-evolving threat of the Proteans. Unlock weapons and upgrades and use them to fight against the Protean swarm and learn about the mysteries of Project Hastur.\nIn EXPERIMENT MODE, choose any map, tweak the parameters, and play infinitely to see what you can evolve. Change the number of creatures and the parameters of evolution, make your turrets invincible, or crank up the biomatter and experiment with the most powerful turret upgrades. Experiment mode lets you experience Project Hastur your way.\n\n\n\n\nThe data were collected by running Project Hastur in Experiment mode using four predefined conditions:\nI: The CHIP SHREDDER towers when Fitness Functions were turned ON and Civilians were PRESENT.\nH: The CHIP SHREDDER towers when Fitness Functions were turned OFF and Civilians were PRESENT.\nG: The CHIP SHREDDER towers when Fitness Functions were turned ON and Civilians were ABSENT.\nK: The AUTOCANNON towers when Fitness Functions were turned ON and Civilians were ABSENT.\nEach experimental condition was run 9 times (9 replicates)."
  },
  {
    "objectID": "posts/T4-BarriesData/index.html#importing-the-data",
    "href": "posts/T4-BarriesData/index.html#importing-the-data",
    "title": "TUTORIAL 4",
    "section": "",
    "text": "I’m going to use the vroom package to import multiple files. Each file is a replicate and the filename tells us about the experimental condition. Below I convert the filename variable (I named it path) into a a single categorical attribute called Fit that uses the letter codes above.\n\n\nCode\nlibrary(vroom)\nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(readxl)\nfiles &lt;- fs::dir_ls(glob = \"*.csv\")\n\nHastur &lt;- vroom(files, id = \"path\", \n                col_select = c(path, Generation, ID, Origin, AsexualReproduction, Fitness, Health,\n                               SightRange, Armor, Damage, WalkSpeed, RunSpeed, Acceleration, \n                               TurnRate, Attraction0, Attraction1, Attraction2))\n\nHastur$Fit &lt;- str_split_i(Hastur$path, pattern = \"\", 1)\nHastur$replicate &lt;- str_split_i(Hastur$path, pattern = \"\", 4)\n\n\nThe glimpse command in the Tidyverse package is a nice way to summarize the data frame:\n\n\nCode\nglimpse(Hastur)\n\n\nRows: 412,246\nColumns: 19\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n$ path                &lt;chr&gt; \"GSC1.csv\", \"GSC1.csv\", \"GSC1.csv\", \"GSC1.csv\", \"G…\n$ Generation          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ID                  &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ Origin              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ AsexualReproduction &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Fitness             &lt;dbl&gt; 57.83508, 66.87755, 66.14652, 65.88873, 62.12119, …\n$ Health              &lt;dbl&gt; 1006, 1012, 1011, 992, 983, 1020, 982, 963, 996, 9…\n$ SightRange          &lt;dbl&gt; 9.952521, 10.096590, 9.954091, 10.066170, 10.02955…\n$ Armor               &lt;dbl&gt; 0.05081077, 0.05080924, 0.05010696, 0.04903501, 0.…\n$ Damage              &lt;dbl&gt; 49, 51, 51, 50, 50, 51, 50, 49, 51, 50, 49, 50, 49…\n$ WalkSpeed           &lt;dbl&gt; 6.930266, 7.034348, 6.970608, 6.903729, 6.962081, …\n$ RunSpeed            &lt;dbl&gt; 20.03562, 19.88800, 19.80754, 19.94738, 19.95583, …\n$ Acceleration        &lt;dbl&gt; 14.70648, 15.05868, 14.85994, 14.89853, 15.01570, …\n$ TurnRate            &lt;dbl&gt; 356.3890, 361.2032, 358.9919, 361.8476, 360.6143, …\n$ Attraction0         &lt;dbl&gt; 0.158477500, -0.007134318, -0.063494000, 0.0125864…\n$ Attraction1         &lt;dbl&gt; -0.070695680, 0.059872570, -0.003332689, 0.0458469…\n$ Attraction2         &lt;dbl&gt; -0.108705200, 0.015018780, -0.007600136, 0.0120656…\n$ Fit                 &lt;chr&gt; \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", \"G\", …\n$ replicate           &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …"
  },
  {
    "objectID": "posts/T4-BarriesData/index.html#describe-the-data",
    "href": "posts/T4-BarriesData/index.html#describe-the-data",
    "title": "TUTORIAL 4",
    "section": "",
    "text": "What we have here is a (big) Flat Table. The Items are the rows, and each row is an individual alien enemy that existed during one of the replicates. Each Item (alien) is described by Attributes, which are arranged in the columns.\n\n\n\nThe glimpse we did in the preceding section gives us a hint as to what each attribute type might be. Let’s flesh that out a bit though. I’m going to create a new data frame that describes the attributes.\n\n\nCode\nAttributes &lt;- read_excel(\"Attributes.xlsx\")\nknitr::kable(Attributes)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nNote\n\n\n\n\npath\nCategorical\neach File Name is a unique replicate\n\n\nGeneration\nQuantitative\nEach Enemy Wave is a Generation\n\n\nID\nOrdinal\nEach enemy has a unique ID within each replicate\n\n\nOrigin\nCategorical\nThe hive from which the enemy was spawned\n\n\nAsexualReproduction\nCategorical\nWas the enemy spawned by infectiing a civilian?\n\n\nFitness\nQuantitative\nThe value of Fitness is used to determine probability of reproduction\n\n\nHealth\nQuantitative\nHit Points\n\n\nSightRange\nQuantitative\nHow far they can see civilians, towers, etc\n\n\nArmor\nQuantitative\nresistance to physical damage\n\n\nDamage\nQuantitative\nhow much damage they do to towers\n\n\nWalkSpeed\nQuantitative\nhow fast they can walk\n\n\nRunSpeed\nQuantitative\nhow fast they can run\n\n\nAcceleration\nQuantitative\nhow fast they can transition from walking to running\n\n\nTurnRate\nQuantitative\nhow fast they can turn\n\n\nAttraction0\nQuantitative\nnegative values is attraction to civilians, positive is avoidance\n\n\nAttraction1\nQuantitative\nnegative values is attraction to towers, positive is avoidance\n\n\nAttraction2\nQuantitative\nnegative values is attraction to the base, positive is avoidance\n\n\nFit\nCategorical\nI, H, G, or K - an inscrutable code about fitness conditions\n\n\n\n\n\nThe problem here is my inscrutable filename codes for that Fit variable. Those letter codes actually contain information on a couple hidden variables. I’m going to create a new variable called Gun and another called Civilians. I’ll add those to the main data file and also the Data Dicttionary.\n\n\nCode\nHastur$Gun &lt;- \"CHIP SHREDDER\"\nHastur$Civilians &lt;- \"Present\"\n  \n\n  Hastur$Gun[Hastur$Fit==\"K\"]&lt;- \"AUTOCANNON\"\n     \n  Hastur$Civilians[Hastur$Fit==\"K\" | Hastur$Fit ==\"G\"] &lt;- \"ABSENT\"\n     \n\n  Attributes&lt;-rbind(Attributes, c(\"Gun\",\"Categorical\", \"Autocannon or Chip Shredder\"))\n  Attributes&lt;-rbind(Attributes, c(\"Civilians\",\"Categorical\", \"Present or Absent\"))\n\n\n\n\nCode\n  knitr::kable(Attributes)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nNote\n\n\n\n\npath\nCategorical\neach File Name is a unique replicate\n\n\nGeneration\nQuantitative\nEach Enemy Wave is a Generation\n\n\nID\nOrdinal\nEach enemy has a unique ID within each replicate\n\n\nOrigin\nCategorical\nThe hive from which the enemy was spawned\n\n\nAsexualReproduction\nCategorical\nWas the enemy spawned by infectiing a civilian?\n\n\nFitness\nQuantitative\nThe value of Fitness is used to determine probability of reproduction\n\n\nHealth\nQuantitative\nHit Points\n\n\nSightRange\nQuantitative\nHow far they can see civilians, towers, etc\n\n\nArmor\nQuantitative\nresistance to physical damage\n\n\nDamage\nQuantitative\nhow much damage they do to towers\n\n\nWalkSpeed\nQuantitative\nhow fast they can walk\n\n\nRunSpeed\nQuantitative\nhow fast they can run\n\n\nAcceleration\nQuantitative\nhow fast they can transition from walking to running\n\n\nTurnRate\nQuantitative\nhow fast they can turn\n\n\nAttraction0\nQuantitative\nnegative values is attraction to civilians, positive is avoidance\n\n\nAttraction1\nQuantitative\nnegative values is attraction to towers, positive is avoidance\n\n\nAttraction2\nQuantitative\nnegative values is attraction to the base, positive is avoidance\n\n\nFit\nCategorical\nI, H, G, or K - an inscrutable code about fitness conditions\n\n\nGun\nCategorical\nAutocannon or Chip Shredder\n\n\nCivilians\nCategorical\nPresent or Absent"
  },
  {
    "objectID": "posts/T4-BarriesData/index.html#host-the-data",
    "href": "posts/T4-BarriesData/index.html#host-the-data",
    "title": "TUTORIAL 4",
    "section": "",
    "text": "I’m publishing to GitHub! We will elaborate on this step as everyone progresses through the assignment."
  },
  {
    "objectID": "posts/T4-BarriesData/index.html#task-abstraction",
    "href": "posts/T4-BarriesData/index.html#task-abstraction",
    "title": "TUTORIAL 4",
    "section": "",
    "text": "For this data set, I am currently defining the user as … me! My hypothesis is that the two Fitness conditions create different evolutionary outcomes of the aliens in Project Hastur. Some relevant ACTION TARGET pairs might be:\nDISCOVER TRENDS\nDISCOVER DISTRIBUTION\nDISCOVER SIMILARITY\nCOMPARE TRENDS\nCOMPARE DISTRIBUTION\nI’m going to try COMPARE TRENDS. I want to COMPARE the TREND in Health over time (Generation) between the two Gun types. To do this, I’ll create a scatterplot, faceted by Gun. I’m suspicious that Acceleration is involved somehow, so I’m coloring with that variable.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Health))+\n  geom_point(aes(color=Acceleration), alpha = 0.01, size = 1)+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_grid(replicate~Gun)\n\n\n\n\n\n\n\n\n\nInteresting… it looks like a clear trend for Health to increase under the withering fire of the AUTO CANNONS, but not when the player uses the CHIP SHREDDER. It is a bit hard to see what is going on with Acceleration, so let’s reverse the graphs so that we plot Acceleration on the y axis but color by Health.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Acceleration))+\n  geom_point(aes(color=Health), alpha = 0.01, size = 1)+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_grid(replicate~Gun)\n\n\n\n\n\n\n\n\n\nI’m now confident that the replicates within each Gun type are pretty similar, and I can SUMMARIZE the individual data points. This will help with the COMPARE TRENDS task, I think.\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Health))+\n  geom_point(aes(color=Acceleration), alpha = 0.01, size = 1)+\n  geom_smooth()+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_wrap(~Gun)\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nCode\nggplot(Hastur, aes(x=Generation, y = Acceleration))+\n  geom_point(aes(color=Health), alpha = 0.01, size = 1)+\n  geom_smooth()+\n  scale_color_continuous(low=\"red\", high = \"blue\")+\n  facet_wrap(~Gun)\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "posts/T6-APIsandJSON/index.html",
    "href": "posts/T6-APIsandJSON/index.html",
    "title": "TUTORIAL 6 - APIs and JSON data",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(repurrrsive)\nlibrary(tibblify)\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(rjson)\nlibrary(jsonlite)\nlibrary(ggplot2)\n\n\nBio&lt;-fromJSON(\"draft.json\")\n\nnoway&lt;-as.data.frame(Bio)\n\nplayers&lt;- noway %&gt;%\n  select(data.draftYear, data.playerId, data.playerName, data.draftYear,\n         data.roundNumber, data.overallPickNumber, data.pickInRound,\n         data.height, data.weight, data.position)\n\ncleanplayers &lt;- drop_na(players)\ncleanplayers$playerId&lt;-cleanplayers$data.playerId\n\ncleanplayers2 &lt;- cleanplayers %&gt;%\n  distinct(playerId, .keep_all= TRUE)\n\nskaters&lt;-cleanplayers2%&gt;%\n  filter(data.position!=\"G\")\n\n\n\n\nCode\napi_url &lt;- \"https://statsapi.web.nhl.com\"\n\n\nrecentdrafts&lt;-skaters%&gt;%\n  filter(data.draftYear&gt;1999)\nrecentdrafts$playerId&lt;-recentdrafts$data.playerId\nplayerIds &lt;- as.list(recentdrafts$data.playerId)\n\ntestloop&lt;-data.frame()\ni&lt;-0\nfor (player_id in playerIds){\n  \n  \n\nendpoint &lt;- paste0(\"/api/v1/people/\", player_id, \"/stats?stats=yearByYear\")\n\n\nurl_json &lt;- paste0(api_url,endpoint)\n\nraw_json &lt;- url_json %&gt;% \n  httr::GET() %&gt;% \n  httr::content()\n\nargh &lt;- tibblify(raw_json$stats[[1]]$splits)\n\nplayerstats&lt;- unnest_wider(argh, col=c(stat,league))\nplayerstats$playerId&lt;-player_id\nplayerstatsfinal&lt;-playerstats %&gt;%\n  filter(name==\"National Hockey League\") %&gt;%\n  select(season, games, points, goals, assists,  \n         name, playerId) \n  \ntestloop&lt;-rbind(testloop,playerstatsfinal)\ni&lt;-i+1\nprint(i)\n}\n\n\n\nd2000on&lt;-full_join(testloop,recentdrafts, by=\"playerId\")\n\n\n\n\nCode\ndataIwant &lt;- d2000on\nwrite.csv(dataIwant, file=\"NHLskaterstats.csv\")\n\n\n\n\nCode\nlibrary(dplyr)\n\ndataIwant2&lt;- dataIwant %&gt;%\n  mutate(seasonshort = as.numeric(str_sub(season, start=1, end=4)))\n\ndataIwant2$postdraft&lt;- dataIwant2$seasonshort-dataIwant2$data.draftYear\n\ndataIwant2$postdraft &lt;- paste(\"dY\", dataIwant2$postdraft)\n\ndataIwant3&lt;-dataIwant2 %&gt;%\n  group_by(playerId, data.draftYear, data.playerName, data.roundNumber,\n           data.pickInRound, data.overallPickNumber, data.position, data.height,\n           data.weight, postdraft) %&gt;%\n  summarise(totgames=sum(games), totgoals=sum(goals), totassist=sum(assists), totpoint=sum(points))\n\ndatawide&lt;-dataIwant3 %&gt;%\n  pivot_wider(names_from = postdraft, values_from = c(totgames, totgoals, totassist, totpoint),\n              values_fill = 0)\n\nallplayerswide&lt;-full_join(cleanplayers,datawide, by=\"playerId\")\n\nallplayerswide &lt;- allplayerswide %&gt;% \n    mutate_at(19:106, ~replace_na(.,0))\n\nlookup &lt;- c(name = \"data.playerName.x\", \n            draftyear = \"data.draftYear.x\",\n            round = \"data.roundNumber.x\",\n            overall = \"data.overallPickNumber.x\",\n            pickinRound = \"data.pickInRound.x\",\n            height = \"data.height.x\",\n            weight = \"data.weight.x\",\n            position = \"data.position.x\")\n\ntestthing&lt;-rename(allplayerswide, all_of(lookup))\n\nkeepthis&lt;-testthing[c(1, 3:10, 19:106)]\n\ngames&lt;-keepthis%&gt;%\n  select(c(1:9, starts_with(\"totgames\")))\n\ngameslong &lt;- games %&gt;%\n  pivot_longer(\n    cols = starts_with(\"totgames\"),\n    names_to = \"postdraft\",\n    names_prefix = \"totgames_dY \",\n    values_to = \"NHLgames\")\n\ngameslong &lt;- gameslong%&gt;%\n  filter(postdraft!=\"NA\")\n\ngameslong$postdraft&lt;-as.numeric(gameslong$postdraft)\n\n\npoints&lt;-keepthis%&gt;%\n  select(c(1:9, starts_with(\"totpoint\")))\n\npointslong &lt;- points %&gt;%\n  pivot_longer(\n    cols = starts_with(\"totpoint\"),\n    names_to = \"postdraft\",\n    names_prefix = \"totpoint_dY \",\n    values_to = \"points\")\n\npointslong &lt;- pointslong%&gt;%\n  filter(postdraft!=\"NA\")\n\npointslong$postdraft&lt;-as.numeric(pointslong$postdraft)\n\nassists&lt;-keepthis%&gt;%\n  select(c(1:9, starts_with(\"totassist\")))\n\nassistslong &lt;- assists %&gt;%\n  pivot_longer(\n    cols = starts_with(\"totassist\"),\n    names_to = \"postdraft\",\n    names_prefix = \"totassist_dY \",\n    values_to = \"assists\")\n\nassistslong &lt;- assistslong%&gt;%\n  filter(postdraft!=\"NA\")\n\nassistslong$postdraft&lt;-as.numeric(assistslong$postdraft)\n\ngoals&lt;-keepthis%&gt;%\n  select(c(1:9, starts_with(\"totgoal\")))\n\ngoalslong &lt;- goals %&gt;%\n  pivot_longer(\n    cols = starts_with(\"totgoal\"),\n    names_to = \"postdraft\",\n    names_prefix = \"totgoals_dY \",\n    values_to = \"goals\")\n\ngoalslong &lt;- goalslong%&gt;%\n  filter(postdraft!=\"NA\")\n\ngoalslong$postdraft&lt;-as.numeric(goalslong$postdraft)\n\n\n\ngamegoal&lt;- left_join(gameslong,goalslong, by = c(\"playerId\", \"draftyear\",\n                                                 \"name\", \"round\", \"pickinRound\",\n                                                 \"height\", \"weight\", \"position\",\n                                                 \"overall\", \"postdraft\"))\ngga&lt;-left_join(gamegoal,assistslong, by = c(\"playerId\", \"draftyear\",\n                                                 \"name\", \"round\", \"pickinRound\",\n                                                 \"height\", \"weight\", \"position\",\n                                                 \"overall\", \"postdraft\"))\nggap&lt;-left_join(gga,pointslong, by = c(\"playerId\", \"draftyear\",\n                                                 \"name\", \"round\", \"pickinRound\",\n                                                 \"height\", \"weight\", \"position\",\n                                                 \"overall\", \"postdraft\"))\n\n\n\n\nstatslong &lt;- ggap %&gt;%\n  mutate(position = replace(position, position == \"G\", \"Goaltender\"),\n         position = replace(position, position == \"D\", \"Defense\"),\n         position = replace(position, position == \"C\" | position ==\"LW\", \"Forward\"),\n         position = replace(position, position == \"RW\" | position == \"F\", \"Forward\"),\n         position = replace(position, position == \"C/LW\" | position == \"C/RW\", \"Forward\"),\n         position = replace(position, position == \"LW/RW\", \"Forward\"))\n  \n\nactualdata&lt;- statslong %&gt;%\n  filter(round&lt;8, draftyear&gt;2000)\n\nactualdata2&lt;-actualdata%&gt;%\n  distinct(playerId,postdraft, .keep_all = TRUE)\n\n\n\n\n \nwrite.csv(actualdata2, file = \"NHLdraftstats.csv\")"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#last-lecture",
    "href": "posts/L2-Data-Abstraction/index.html#last-lecture",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "LAST LECTURE",
    "text": "LAST LECTURE\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#what",
    "href": "posts/L2-Data-Abstraction/index.html#what",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\n Before you design a visualization, you need to understand the data. Here, we consider the semantics to describe the DATA TYPES and DATA ATTRIBUTES."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#the-three-major-data-types",
    "href": "posts/L2-Data-Abstraction/index.html#the-three-major-data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "THE THREE MAJOR DATA TYPES",
    "text": "THE THREE MAJOR DATA TYPES"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-attributes",
    "href": "posts/L2-Data-Abstraction/index.html#data-attributes",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA ATTRIBUTES",
    "text": "DATA ATTRIBUTES"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#what-1",
    "href": "posts/L2-Data-Abstraction/index.html#what-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat does this sequence of six numbers mean?\n14, 2.6, 30, 30, 15, 100001 Two points far from each other in 3D space?\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-1",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat does this sequence of six numbers mean?\n14, 2.6, 30, 30, 15, 100001 Two points close to each other in 2D space, with 15 links between them, and a weight of 100001 for the link?\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-2",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat about this data?\nBasil, 7, S, Pear:\nFood shipment of produce (basil & pear) arrived in satisfactory condition on 7th day of month\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-3",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-3",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat about this data?\nBasil, 7, S, Pear:\nBasil Point neighborhood of city had 7 inches of snow cleared by the Pear Creek Limited snow removal service\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-semantics-4",
    "href": "posts/L2-Data-Abstraction/index.html#data-semantics-4",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA SEMANTICS",
    "text": "DATA SEMANTICS\n\n\n\nWhat about this data?\nBasil, 7, S, Pear:\nLab rat Basil made 7 attempts to find way through south section of maze, these trials used pear as reward food\n\n\nVIZ"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#semantics",
    "href": "posts/L2-Data-Abstraction/index.html#semantics",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SEMANTICS",
    "text": "SEMANTICS\nThe meaning of a word, phrase, sentence, or text.\nBasil, 7, S, Pear"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#semantics-for-data",
    "href": "posts/L2-Data-Abstraction/index.html#semantics-for-data",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SEMANTICS FOR DATA",
    "text": "SEMANTICS FOR DATA\n\n\n\nSemantics\n\nitem: individual entity, discrete\n\neg patient, car, stock, city\n“independent variable”\n\nattribute: property that is measured, observed, logged…\n\neg height, blood pressure for patient\neg horsepower, make for car\n“dependent variable”\n\n\n\n\nData Table\n\n\n\n\nITEM: Person\nATTRIBUTES: Name, Age, Shirt Size, Favorite Fruit"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#other-data-types",
    "href": "posts/L2-Data-Abstraction/index.html#other-data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "OTHER DATA TYPES",
    "text": "OTHER DATA TYPES\n\nLinks\n\nexpress relationship between two items\ne.g/ friendship on facebook, interaction between proteins\n\nPositions\n\nspatial data: location in 2D or 3D\ne.g. pixels in photo, voxels in MRI scan, latitude/longitude\n\nGrids\n\nsampling strategy for continuous data"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#what-2",
    "href": "posts/L2-Data-Abstraction/index.html#what-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "WHAT?",
    "text": "WHAT?\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#dataset-types-tables",
    "href": "posts/L2-Data-Abstraction/index.html#dataset-types-tables",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATASET TYPES: TABLES",
    "text": "DATASET TYPES: TABLES\nFlat Table\n\n\n\nOne ITEM per row\n\noften called an observation\n\nEach column is an ATTRIBUTE\n\noften called a variable\n\nA cell holds the VALUE for an item/attribute pair\nA UNIQUE key can be used (implicitly or explicitly) to identify each item even if they share all measured attributes"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#flat-table-example",
    "href": "posts/L2-Data-Abstraction/index.html#flat-table-example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "FLAT TABLE EXAMPLE",
    "text": "FLAT TABLE EXAMPLE"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#multidimensional-tables",
    "href": "posts/L2-Data-Abstraction/index.html#multidimensional-tables",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MULTIDIMENSIONAL TABLES",
    "text": "MULTIDIMENSIONAL TABLES\nindexing based on multiple keys (eg genes, patients)"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#networks",
    "href": "posts/L2-Data-Abstraction/index.html#networks",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "NETWORKS",
    "text": "NETWORKS\nNetwork/graph nodes (vertices) connected by links (edges). A tree is special case: no cycles, often have roots, and are directed."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#fields",
    "href": "posts/L2-Data-Abstraction/index.html#fields",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "FIELDS",
    "text": "FIELDS"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#spatial-fields-1",
    "href": "posts/L2-Data-Abstraction/index.html#spatial-fields-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 1",
    "text": "SPATIAL FIELDS 1\nAttribute values associated w/ cells cell contains value from continuous domain (eg temperature, pressure, wind velocity measured or simulated)."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#spatial-fields-2",
    "href": "posts/L2-Data-Abstraction/index.html#spatial-fields-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 2",
    "text": "SPATIAL FIELDS 2\n\n\n\n\nAttribute values associated w/ cells.\nCell contains value from continuous domain\n\n(eg temperature, pressure, wind velocity)\n\nMeasured or simulated.\nMajor concerns\n\nsampling:\n\nwhere attributes are measured\n\ninterpolation:\n\nhow to model attributes elsewhere\n\ngrid types"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#spatial-fields-3",
    "href": "posts/L2-Data-Abstraction/index.html#spatial-fields-3",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SPATIAL FIELDS 3",
    "text": "SPATIAL FIELDS 3\n\n\n\nAttribute values associated w/ cells.\nCell contains value from continuous domain\n\n(eg temperature, pressure, wind velocity)\n\nMeasured or simulated.\nMajor concerns\n\nsampling:\n\nwhere attributes are measured\n\ninterpolation:\n\nhow to model attributes elsewhere\n\ngrid types\n\nMajor divisions - attributes per cell:\n\nscalar (1)\nvector (2)\ntensor (many)"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#geometry",
    "href": "posts/L2-Data-Abstraction/index.html#geometry",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "GEOMETRY",
    "text": "GEOMETRY"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#geometry-2",
    "href": "posts/L2-Data-Abstraction/index.html#geometry-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "GEOMETRY 2",
    "text": "GEOMETRY 2\n\n\n\nShape of items\nExplicit spatial positions / regions\n\npoints, lines, curves, surfaces, volumes\n\nBoundary between computer graphics and visualization\n\ngraphics: geometry taken as given\nvis: geometry is result of a design decision"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#collections",
    "href": "posts/L2-Data-Abstraction/index.html#collections",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "COLLECTIONS",
    "text": "COLLECTIONS"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#collections-2",
    "href": "posts/L2-Data-Abstraction/index.html#collections-2",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "COLLECTIONS 2",
    "text": "COLLECTIONS 2\n\n\nGrouping Items:\n\nsets\n\nunique items\nunordered\n\nlists\n\nordered\nduplicates possible\n\nclusters\n\ngroups of similar items"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-types",
    "href": "posts/L2-Data-Abstraction/index.html#data-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA TYPES",
    "text": "DATA TYPES"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#attribute-types",
    "href": "posts/L2-Data-Abstraction/index.html#attribute-types",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "ATTRIBUTE TYPES",
    "text": "ATTRIBUTE TYPES\n\n\n\nCategorical (nominal):\n\ncompare equality\nno implicit ordering\n\nOrdered:\n\nordinal\nless/greater than defined\n\nQuantitative:\n\nmeaningful magnitude\narithmetic possible"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#example",
    "href": "posts/L2-Data-Abstraction/index.html#example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "EXAMPLE",
    "text": "EXAMPLE\nCategorical\nOrdinal\nQuantitative"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#example-1",
    "href": "posts/L2-Data-Abstraction/index.html#example-1",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "EXAMPLE",
    "text": "EXAMPLE\nCategorical\nOrdinal\nQuantitative"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#additional-semantic-components",
    "href": "posts/L2-Data-Abstraction/index.html#additional-semantic-components",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "ADDITIONAL SEMANTIC COMPONENTS",
    "text": "ADDITIONAL SEMANTIC COMPONENTS"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#data-abstraction",
    "href": "posts/L2-Data-Abstraction/index.html#data-abstraction",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DATA ABSTRACTION",
    "text": "DATA ABSTRACTION\n\nGOAL: Translate from domain-specific language to generic (and consistent) visualization language.\n\nIdentify dataset type(s) and attribute types.\nIdentify cardinality.\n\nhow many items in the dataset?\nwhat is cardinality of each attribute?\n\nnumber of levels for categorical data?\nrange for quantitative data\n\n\nConsider whether to transform the data.\n\nguided by your understanding of the task."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#models-data-vs-conceptual",
    "href": "posts/L2-Data-Abstraction/index.html#models-data-vs-conceptual",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MODELS: DATA VS CONCEPTUAL",
    "text": "MODELS: DATA VS CONCEPTUAL\n\n\nData Model\n\nmathematical abstraction\nsets with operations, eg floats with * / - + variable data types in programming languages\n\nConceptual Model\n\nmental construction (semantics)\nsupports reasoning\ntypically based on understanding of tasks\n\nThe Data Abstraction process relies on conceptual model for transforming data if needed"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#models-example",
    "href": "posts/L2-Data-Abstraction/index.html#models-example",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "MODELS: EXAMPLE",
    "text": "MODELS: EXAMPLE\n\nData Model: floats\n\n32.52, 54.06, -14.35,\n\nConceptual Model:\n\ntemperature\n\nPossible data abstractions:\n\nQUANTITATIVE: continuous to 2 significant figures:\n\nTASK: Forecasting the weather\n\nORDINAL: Hot, Warm, Cold:\n\nTASK: Deciding if my bath water is ready\n\nCATEGORICAL: Above Freezing, Below Freezing:\n\nTASK: Deciding if I should leave the house today"
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#derived-attributes",
    "href": "posts/L2-Data-Abstraction/index.html#derived-attributes",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "DERIVED ATTRIBUTES",
    "text": "DERIVED ATTRIBUTES\nDerived attribute: Data computed from original (collected, observed) attributes."
  },
  {
    "objectID": "posts/L2-Data-Abstraction/index.html#summary",
    "href": "posts/L2-Data-Abstraction/index.html#summary",
    "title": "LECTURE 2 - DATA ABSTRACTION",
    "section": "SUMMARY",
    "text": "SUMMARY\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\n\n\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "posts/E1-GrantExpenditures/Banner.html",
    "href": "posts/E1-GrantExpenditures/Banner.html",
    "title": "Example 1: Grant Expenditures",
    "section": "",
    "text": "## Aggregating financial data for a grant\nThis is for my NIH grant and it uses the transaction detail report.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(lubridate)\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nSEPA &lt;- read.csv(\"SEPA.csv\")\n\nSEPAsmall &lt;- SEPA %&gt;%\n  select(Transaction.Date, Transaction.Type, PE, PE.Title,\n         Original.Budget, YTD, Encumbrance, Adjusted.Budget)%&gt;%\n  filter(PE != \"RV\")%&gt;%\n  mutate(Category = if_else(PE == 60, \"F&A\", \"Direct\"))%&gt;%\n  mutate(date = mdy(Transaction.Date),\n         month = month(date))%&gt;%\n  group_by(Category, month)%&gt;%\n  summarise(Total = sum(YTD))\n\n\n`summarise()` has grouped output by 'Category'. You can override using the\n`.groups` argument.\n\n\nCode\np &lt;- plot_ly(SEPAsmall, x = ~month, y = ~Total, color = ~Category, type = 'bar') %&gt;%\n  layout(barmode = 'stack')\n\np\n\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels"
  },
  {
    "objectID": "posts/Certificate/index.html",
    "href": "posts/Certificate/index.html",
    "title": "CERTIFICATE",
    "section": "",
    "text": "Learn how to think about, organize, analyze, and visualize data. Communicate data-driven insights to technical and lay audiences."
  },
  {
    "objectID": "posts/Certificate/index.html#overview",
    "href": "posts/Certificate/index.html#overview",
    "title": "CERTIFICATE",
    "section": "OVERVIEW",
    "text": "OVERVIEW\nWe live in an increasingly data-driven world. Basic data literacy and data science skills are becoming central to virtually every industry. Yet, limited opportunities exist to gain these skills without an advanced background in math and computer science. To address this workforce development need, we propose a competitively valued on-line graduate certificate in the Professional Applications in Data Science. The certificate is designed to offer rigorous training in the foundations of data science to anyone with a bachelor’s degree. Participants will learn how to think about, organize, analyze, and visualize data, and communicate data driven insights to diverse audiences. The curriculum emphasizes the use of data sets drawn from each student’s individual discipline, aligning the certificate’s workforce development impacts with the University of Idaho’s land grant mission.\n\nValue Proposition:\nThe graduate certificate in Professional Applications in Data Science will provide unique value to UI constituencies by:\n\nAligning data science training with fields of nascent demand that are part of our land grant mission, including Agriculture, Natural Resources, and Education.\nRequiring a digital data science portfolio with which students can demonstrate their proficiencies to potential employers.\nEmphasizing training in data communication - including verbal presentation and data visualization - two components of data science that are underrepresented in competing certificates.\nFilling a growing workforce development gap by offering a unique data science certificate that is appropriate for professionals with a bachelor’s degree who do not have a rigorous background in mathematics, statistics, or computer science.\n\n\n\nIntended Audience:\nThis certificate leverages the University of Idaho’s interdisciplinary culture to provide integrative training in the foundations of data science. It is intended for:\n\nWorking professionals with a bachelor’s degree whose career increasingly involves the generation, management, analysis, and visualization of large data sets. The certificate is appropriate for professionals in STEM fields, Health Care, Business, Government, Education, Journalism, Athletics, Natural Resources, and Agriculture.\nGraduate students in programs outside of the core technical disciplines of data science (statistics, math, engineering, or computer science). The certificate will complement disciplinary research methods courses with training in data management, visualization, and communication.\nUndergraduates at the UI who wish to incorporate data science training into their degree and graduate with a Bachelor’s degree and a graduate certificate.\n\n\n\nStudent Learning Outcomes:\nUpon completion of the certificate, students will be able to:\n\nUse open-source software to reproducibly manage, analyze, and visualize large, complex, and noisy data sets.\nPractice high quality and ethical data stewardship.\nUnderstand and execute data exploration.\nEffectively communicate data driven insights to experts and non-experts.\nDemonstrate their skills with an online portfolio of analyses and visualizations relevant to their field of specialization."
  },
  {
    "objectID": "posts/Certificate/index.html#curriculum",
    "href": "posts/Certificate/index.html#curriculum",
    "title": "CERTIFICATE",
    "section": "CURRICULUM",
    "text": "CURRICULUM\n\nPrerequisites:\nA Bachelor’s degree OR the student has senior standing and is enrolled in a bachelor’s degree program at the University of Idaho.\n\n\nCertificate Requirements (12 Credits Total)\n\n\n\n\n\n\n\n\n\n\n\n\nCourse\nName\nCredits\nPrerequisites\nSchedule\n\n\n\n\nINTR 509\nIntroduction to Applied Data Science\n3\nBS degree or permission\nSpring and asynchronous online\n\n\nBCB 551\nCommunicating with Data\n2\nINTR 509 or BS degree or permission\nFall and asynchronous online\n\n\nBCB 520\nData Visualization\n3\nSTAT 251 or INTR 509 or permission\nSpring and asynchronous online\n\n\nBCB 522\nData Science Portfolio\n1\nINTR 509 and BCB 520 (Data Viz)\nAsynchronous online\n\n\nElective\nVaries\n3\nVaries\nVaries\n\n\n\n\n\nnote: Courses designated with “BCB 5XX” are new courses that will be offered in the 2023-24 academic year\n\n\nCourse Descriptions\n\nINTR 509 Introduction to Applied Data Science (3 credits)\nIn person (spring) and asynchronous online.\nStudents are provided a foundation for “thinking with data” through the introduction of computational, statistical, and data literacy skills. This includes the selection, collection, cleaning, management, descriptive analysis, and exploratory analysis of a dataset unique to their professional interests using modern computing languages. This course is taught by Dr. Michael Overton.\n\n\nBCB 521 Communicating with Data (2 credits)\nIn person (fall) and asynchronous online.\nStudents are taught writing and presentation skills to improve their communication of data-driven insights to specialist and lay audiences. The course emphasizes reproducible research practices, including literate programming (R Markdown) and version control (GitHub). Course content includes the conceptual foundations of communicating with data along with written and verbal communication assignments using data sets individualized to each student’s interest.\nText: Nolan and Stoudt. 2021. Communicating with data: The art of writing for data science. Oxford University Press.\nPrerequisites: INTR 509 OR A BS degree OR permission.\n\n\nBCB 520 Data Visualization (3 credits)\nIn person (spring) and asynchronous online\nThis course covers the conceptual foundations of data visualization and design. Students will learn how visualization design choices related to marks and channels, color, and spatial arrangement interact with the human perceptual system. The course considers tabular, network, and spatial data, and students will implement visualizations in R.\nText: Munzner. 2014. Visualization Analysis & Design. CRC Press.\nPrerequisites: INTR 509 OR A BS degree OR Stat 251 OR Permission.\n\n\nBCB 522 Online Portfolio (1 credit)\nAsynchronous online\nThis course provides feedback, review, and approval of the student’s online data science portfolio. This portfolio is intended to represent the body of work accumulated by the student over the course of the certificate. It should contain examples of novel data products (such as FAIR data sets), analyses, and visualizations. All elements of the portfolio will be hosted online (likely in a GitHub repository or professional website), be open source, and demonstrate best practices of literate programming and reproducible research.\n\n\nElectives:\nThe certificate allows each student to customize their training by choosing a 3-credit graduate elective.\nFor students seeking foundational training who have not already taken Stat 431 or its equivalent, we recommend Stat 431 or a 3-credit graduate level disciplinary research methods course.\nFor students seeking to add the certificate to an existing degree at UI, or students who already have some advanced technical training, additional electives are possible. Note that many of these optional electives have substantial disciplinary pre-requisites. Not all electives are available in an online format.\n\n\nChoose one of the following:\n\n\n\n\n\nCourse\nName\nCredits\nPrerequisites\n\n\n\n\nAVS 531\nPractical Methods in Analyzing Animal Science Experiments\n3\n400-level statistics course\n\n\nBE 521\nImage Processing and Computer Vision\n3\n(BE 242 and MATH 275) or permission\n\n\nBE 541\nInstrumentation and Measurements\n3\nENGR 240; Coreqs: STAT 301\n\n\nBIOL 526\nSystems Biology\n3\n(BIOL 115, BIOL 115L and MATH 170) or permission of instructor\n\n\nBIOL 545\nPhylogenetics\n3\nPLSC 205 or BIOL 213 and BIOL 310\n\n\nBIOL 549\nComputer Skills for Biologists\n3\nBIOL 310 and STAT 251 or STAT 301; or Permission\n\n\nBIOL 563\nMathematical Genetics\n3\nMATH 160 or MATH 170 and STAT 251 or STAT 301\n\n\nCE 526\nAquatic Habitat Modeling\n3\nA minimum grade of ‘C’ or better is required for all pre/corequisites; Prereqs: CE 322 and CE 325 or BE 355; or Permission\n\n\nCE 579\nSimulation of Transportation Systems\n3\nPermission\n\n\nCS 511\nParallel Programming\n3\nCS 395\n\n\nCS 574\nDeep Learning\n3\n(CS 121 or MATH 330) and STAT 301\n\n\nCS 570\nArtificial Intelligence\n3\nCS 210\n\n\nCS 572\nEvolutionary Computation\n3\nCS 211\n\n\nCS 575\nMachine Learning\n3\nCS 210\n\n\nCS 577\nPython for Machine Learning\n3\n(CS 121 or MATH 330) and STAT 301\n\n\nCS 578\nNeural Network Design\n3\nPermission\n\n\nCS 579\nData Science\n3\nMATH 330 or Permission\n\n\nCS 589\nSemantic Web and Open Data\n3\nCS 360 or CS 479 or CS 579\n\n\nCTE 519\nDatabase Applications and Information Management\n3\nNA\n\n\nCYB 520\nDigital Forensics\n3\nCYB 310\n\n\nED 571\nIntroduction to Quantitative Research\n3\nGraduate standing\n\n\nED 584\nUnivariate Quantitative Research in Education\n3\nED 571\n\n\nED 587\nMultivariate Quantitative Analysis in Education\n3\nED 584 or Permission\n\n\nED 589\nTheoretical Applications and Designs of Qualitative Research\n3\nED 574 or Permission\n\n\nED 590\nData Analysis and Interpretation of Qualitative Research\n3\nED 574 and ED 589\n\n\nED 591\nIndigenous and Decolonizing Research Methods\n3\nNA\n\n\nED 592\nDecolonizing, Indigenous, and Action-Based Research Methods\n3\nNA\n\n\nED 595\nSurvey Design for Social Science Research\n3\nRecommended Preparation: Foundations of Research course at graduate level.\n\n\nEDAD 570\nMethods of Educational Research\n3\nNA\n\n\nENT 504\nApplied Bioinformatics\n3\nPermission\n\n\nENVS 511\nData Wizardry in Environmental Sciences\n3\nSTAT 251\n\n\nENVS 551\nResearch Methods in the Environmental Social Sciences\n3\nOne course or experience in basic statistics or Instructor Permission\n\n\nFOR 514\nForest Biometrics\n3\nSTAT 431 or equivalent\n\n\nFOR 535\nRemote Sensing of Fire\n3\nFOR 375 or permission\n\n\nGEOG 507\nSpatial Statistics and Modeling\n3\nSTAT 431 or permission\n\n\nGEOG 583\nRemote Sensing/GIS Integration\n3\nCoreqs: GEOG 385 or equivalent.\n\n\nMath 538\nStochastic Models\n3\nMATH 451 or Permission\n\n\nMIS 555\nData Management for Big Data\n3\nNA\n\n\nNRS 578\nLidar and optical remote sensing analysis using open-source software\n3\nSTAT251 & WLF370 or STAT427 and NRS/FOR 472 or equivalent/instructor permission\n\n\nPOLS 558\nResearch Methods for Local Government and Community Administration\n3\nSTAT 251\n\n\nREM 507\nLandscape and Habitat Dynamics\n3\nPermission; Recommended Preparation: courses in ecology, statistics, and GIS.\n\n\nStat 431\nStatistical Analysis\n3\nSTAT 251 or STAT 301\n\n\nSTAT 514\nNonparametric Statistics\n3\nSTAT 431\n\n\nSTAT 516\nApplied Regression Modeling\n3\nSTAT 431\n\n\nStat 517\nStatistical Learning and Predictive Modeling\n3\nSTAT 431\n\n\nStat 519\nMultivariate Analysis\n3\nSTAT 431 or equivalent.\n\n\nSTAT 535\nIntroduction to Bayesian Statistics\n3\nSTAT 431\n\n\nSTAT 555\nStatistical Ecology\n3\nMATH 451 or Permission\n\n\nStat 565\nComputer Intensive Methods\n3\n STAT 451, STAT 452, MATH 330, and computer programming experience or Permission\n\n\nWLF 552\nEcological Modeling\n3\nMATH 175 and FOR 221 or Permission.\n\n\nWLF 555\nStatistical Ecology\n3\nMATH 451 or permission\n\n\nWR 552\nWater Economics and Policy\n3\nAGEC 301 or AGEC 302, or ECON 351 or ECON 352, or by permission"
  },
  {
    "objectID": "posts/Certificate/index.html#general-university-requirements",
    "href": "posts/Certificate/index.html#general-university-requirements",
    "title": "CERTIFICATE",
    "section": "GENERAL UNIVERSITY REQUIREMENTS",
    "text": "GENERAL UNIVERSITY REQUIREMENTS\nIn addition to the requirements specified in this document, the certificate would be subject to all UI Policies regarding Graduate Certificates."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#last-lecture",
    "href": "posts/L3-TaskAbstraction/index.html#last-lecture",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "LAST LECTURE",
    "text": "LAST LECTURE\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#task-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TASK ABSTRACTION",
    "text": "TASK ABSTRACTION\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#from-domain-to-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "FROM DOMAIN TO ABSTRACTION",
    "text": "FROM DOMAIN TO ABSTRACTION"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "href": "posts/L3-TaskAbstraction/index.html#key-components-of-task-abstraction",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "KEY COMPONENTS OF TASK ABSTRACTION",
    "text": "KEY COMPONENTS OF TASK ABSTRACTION\n{action, target} pairs\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "href": "posts/L3-TaskAbstraction/index.html#actions-and-targets",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS AND TARGETS",
    "text": "ACTIONS AND TARGETS"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---analyze",
    "href": "posts/L3-TaskAbstraction/index.html#actions---analyze",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Analyze",
    "text": "ACTIONS - Analyze\n\n\n\nConsume: Information has already been generated and stored as data.\n\nDiscover: new knowledge, test hypothesis, generate new hypothesis, verify\nPresent: communicate something specific and already understood\nEnjoy: casual encounters with visualization\n\nProduce: generate new material or information\n\nAnnotate: addition of graphical or text to existing visualization elements\nRecord: saves or captures visualization elements as persistent artifacts (screenshots, lists, parameter sets, annotations)\nDerive: produce new data based on existing data (aka transform)"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---search",
    "href": "posts/L3-TaskAbstraction/index.html#actions---search",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Search",
    "text": "ACTIONS - Search\n\n\n\nLookup: Location and target both known\n\nExample: Look up humans in the Tree of Life, knowing they are mammals.\n\nLocate: Location unknown and target known\n\nExample: Look up rabbits in the Tree of Life, not knowing they are lagomorphs.\n\nBrowse: Location known and target unknown\n\nExample: Find any clades within Mammalia that have only one species.\n\nExplore: Location unknown and target unknown\n\nExample: Searching for anomalies in time series data."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#actions---query",
    "href": "posts/L3-TaskAbstraction/index.html#actions---query",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "ACTIONS - Query",
    "text": "ACTIONS - Query\n\n\n\nQuery: How much of the data matters to the task?\n\nIdentify: One (specific Item, individual, cell, etc)\nCompare: Some (multiple targets)\nSummarize: All (very common, aka Overview)"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---all-data",
    "href": "posts/L3-TaskAbstraction/index.html#targets---all-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - All Data",
    "text": "TARGETS - All Data"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---attributes",
    "href": "posts/L3-TaskAbstraction/index.html#targets---attributes",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Attributes",
    "text": "TARGETS - Attributes"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#targets---other-data",
    "href": "posts/L3-TaskAbstraction/index.html#targets---other-data",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "TARGETS - Other Data",
    "text": "TARGETS - Other Data"
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#summary",
    "href": "posts/L3-TaskAbstraction/index.html#summary",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "SUMMARY",
    "text": "SUMMARY\nComputer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively."
  },
  {
    "objectID": "posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "href": "posts/L3-TaskAbstraction/index.html#mandatory-fun",
    "title": "LECTURE 3 - TASK ABSTRACTION",
    "section": "MANDATORY FUN",
    "text": "MANDATORY FUN\nWe will do these until everyone has done at least one example.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCANVAS…HOME"
  },
  {
    "objectID": "posts/T1-Lit-Prog/index.html",
    "href": "posts/T1-Lit-Prog/index.html",
    "title": "TUTORIAL 1 - Literate Programming",
    "section": "",
    "text": "Learning new tools is hard. Plowing though the tomes of the Data Science Mythos is hard. Perhaps this tutorial will guide you through the mind shattering truths of… LITERATE PROGRAMMING."
  },
  {
    "objectID": "posts/T1-Lit-Prog/index.html#intro-to-quarto",
    "href": "posts/T1-Lit-Prog/index.html#intro-to-quarto",
    "title": "TUTORIAL 1 - Literate Programming",
    "section": "",
    "text": "Learning new tools is hard. Plowing though the tomes of the Data Science Mythos is hard. Perhaps this tutorial will guide you through the mind shattering truths of… LITERATE PROGRAMMING."
  },
  {
    "objectID": "posts/A5-TabularData/index.html",
    "href": "posts/A5-TabularData/index.html",
    "title": "ASSIGNMENT 5",
    "section": "",
    "text": "In this assignment, we are going to practice creating visualizations for tabular data. Unlike previous assignments, however, this time we will all be using the same data sets. I’m doing this because I want everyone to engage in the same logic process and have the same design objectives in mind."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#overview",
    "href": "posts/A5-TabularData/index.html#overview",
    "title": "ASSIGNMENT 5",
    "section": "",
    "text": "In this assignment, we are going to practice creating visualizations for tabular data. Unlike previous assignments, however, this time we will all be using the same data sets. I’m doing this because I want everyone to engage in the same logic process and have the same design objectives in mind."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#learning-objectives",
    "href": "posts/A5-TabularData/index.html#learning-objectives",
    "title": "ASSIGNMENT 5",
    "section": "LEARNING OBJECTIVES",
    "text": "LEARNING OBJECTIVES\n\nDemonstrate that you can manipulate tabular data to facilitate different visualization tasks. The minimum skills are FILTERING, SELECTING, and SUMMARIZING, all while GROUPING these operations as dictated by your data.\nDemonstrate that you can use tabular data to explore, analyze, and choose the most appropriate visualization idioms given a specific motivating question.\nDemonstrate that you can Find, Access, and Integrate additional data in order to fully address the motivating question.\n\nThe scenario below will allow you to complete the assignment. It deals with data that are of the appropriate complexity and extent (number of observations and variables) to challenge you. If you want to use different data (yours or from another source) I am happy to work with you to make that happen!"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#scenario",
    "href": "posts/A5-TabularData/index.html#scenario",
    "title": "ASSIGNMENT 5",
    "section": "SCENARIO",
    "text": "SCENARIO\nImagine you are a high priced data science consultant. One of your good friends, Cassandra Canuck, is an Assistant General Manager for the Vancouver Canucks, a team in the National Hockey League with a long, long…. long history of futility.\nThis season feels different.\nThe Canucks are currently among the league leaders and appear to be on their way to their first playoff appearance in many years. A few weeks ago, the Vancouver Canucks decided to trade an underperforming player with a high upside and their first round draft pick to the Calgary Flames for Elias Lindholm, a very solid forward that might prove to be the missing piece of their Stanley Cup puzzle. Exciting!\nExcept that now the Canucks are struggling. They’ve lost 4 straight games and have seemingly lost their identity as a team. The fans are questioning whether the trade was worth it. Woe is me!\nFor the purposes of this exercise, let’s set the 2024 NHL draft order using the Tankathon Simulator. The NHL uses a lottery system in which the teams lowest in the standings have the highest odds of getting the first overall pick. This year the Canucks are at the top of the league, and positioned to have the 31st overall pick. According to the simulator, Calgary will pick at number 2 (which is very valuable!), and the Canuck’s pick at 31.\nHere is a screenshot:\n\n\nHere is the question:\nWas the trade worth it? This trade has a high likelihood of becoming what we call a rental. Elias Lindholm is on an expiring contract, meaning Vancouver is guaranteed to hold his contract only through the end of the season. They might be able to extend him, but that depends on the salary cap.\nMeanwhile, Calgary can draft a player at position 31, who may or may not turn out to be of equal or greater value than Lindholm.\nWas the trade worth it? Did Vancouver or Calgary “win” the trade?\nCan we make some visualizations that help us answer this question?\nHere is an article on modeling draft pick value!\nOriginal analysis!\nEric Tulsky’s original paper*"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#directions",
    "href": "posts/A5-TabularData/index.html#directions",
    "title": "ASSIGNMENT 5",
    "section": "DIRECTIONS",
    "text": "DIRECTIONS\nCreate a new post in your portfolio for this assignment. Call it something cool, like NHL draft analysis, or Hockey Analytics, or John Wick….\nCopy the data files from the repository, and maybe also the .qmd file.\nUse the .qmd file as the backbone of your assignment, changing the code and the markdown text as you go."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#the-data",
    "href": "posts/A5-TabularData/index.html#the-data",
    "title": "ASSIGNMENT 5",
    "section": "THE DATA",
    "text": "THE DATA\nHow can we evaluate whether trading a first round pick for a rental player is a good idea? One approach is to look at the historical performance of players from various draft positions.\nI’ve created a data set that will allow us to explore player performance as a function of draft position. If you are curious as to how I obtained and re-arranged these data, you can check out that tutorial here. For this assignment, though, I want to focus on the visualizations.\n\n\nCode\nNHLDraft&lt;-read.csv(\"NHLDraft.csv\")\nNHLDictionary&lt;-read_excel(\"NHLDictionary.xlsx\")\n\nknitr::kable(NHLDictionary)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\ndraftyear\nOrdinal\nCalendar year in which the player was drafted into the NHL.\n\n\nname\nItem\nFull name of the player.\n\n\nround\nOrdinal\nRound in which the player was drafted (1 to 7).\n\n\noverall\nOrdinal\nOverall draft position of the player (1 to 224)\n\n\npickinRound\nOrdinal\nPosition in which the player was drafted in their round (1 to 32).\n\n\nheight\nQuantitative\nPlayer height in inches.\n\n\nweight\nQuantitative\nPlayer weight in pounds.\n\n\nposition\nCategorical\nPlayer position (Forward, Defense, Goaltender)\n\n\nplayerId\nItem\nUnique ID (key) assigned to each player.\n\n\npostdraft\nOrdinal\nNumber of seasons since being drafted (0 to 20).\n\n\nNHLgames\nQuantitative\nNumber of games played in the NHL in that particular season (regular season is 82 games, playoffs are up to 28 more).\n\n\n\n\n\nIn this case, we have a dataframe with all the drafted players from 2000-2018, their position, their draft year and position, and then rows for each season since being drafted (postdraft). The key variable here is NHLgames, which tells us how many games they played in the NHL each season since being drafted. Whether drafted players even make the NHL, and how many games they play, might be a good proxy to understand the value of a draft pick we just traded away."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#simple-scatterplot",
    "href": "posts/A5-TabularData/index.html#simple-scatterplot",
    "title": "ASSIGNMENT 5",
    "section": "SIMPLE SCATTERPLOT",
    "text": "SIMPLE SCATTERPLOT\nOne thing to realize about professional hockey is that it is pretty rare for a player to play in the NHL right after being drafted. Players get drafted when they are 18 years old, and they usually play in the juniors, minor leagues, or the NCAA for a bit to further develop.\nLet’s use a scatterplot to visualize this phenomenon with the most recent draft classes.\n\n\nCode\ndraft2022&lt;-NHLDraft%&gt;%\n  filter(draftyear==2022 & postdraft==0)\n\nggplot(draft2022, aes(x=round, y=NHLgames))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nAs you can see, the players drafted in June of 2022 didn’t play much last season. There are few things wrong with this visualization, however:\n\nOverplotting. All those points on the y=0 line represent about 32 players each. Can you think of a way that adding extra channels might help?\nLabelling. Can we create a solid figure caption and better axis labels for this figure? In your caption, please specify the task(s) the visualizaiton is intended to facilitate, as well as the marks, channels, and key-value pairs used.\nKey-Value pairs: Looks like we are using “round” as a continuous variable. Can we change this to an ordered factor?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#expanded-scatterplot",
    "href": "posts/A5-TabularData/index.html#expanded-scatterplot",
    "title": "ASSIGNMENT 5",
    "section": "EXPANDED SCATTERPLOT",
    "text": "EXPANDED SCATTERPLOT\nThe data from the most recent drafts aren’t really helpful for our question. Let’s go back in time and use a draft year that has had some time to develop and reach their potential. How about 2018?\n\n\nCode\ndraft2018&lt;-NHLDraft%&gt;%\n  filter(draftyear==2018 & postdraft&lt;6) \n\n# wondering why I've filtered postdraft to be less than 6?  Try removing that filter to see what happens.\n\nggplot(draft2018, aes(x=round, y=NHLgames))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nHmmm… in addition to the problem of overplotting, we’ve got an additional issue here. We actually have two keys and one attribute. The attribute is NHLgames, and the keys are round and postdraft, but we are only using round.\nPostdraft indicates the number of seasons after being drafted. We have several choices here. We can make a visualization that uses both keys, or we can somehow summarize the data for one of the keys.\nFor example, let’s say we just wanted to know the TOTAL number of NHL games played since being drafted.\n\n\nCode\ndrafttot2018&lt;- draft2018%&gt;%\n  group_by(playerId, round, overall, position, name)%&gt;%\n  summarise(totgames=sum(NHLgames))\n\n\n`summarise()` has grouped output by 'playerId', 'round', 'overall', 'position'.\nYou can override using the `.groups` argument.\n\n\nCode\nggplot(drafttot2018, aes(x=round, y=totgames))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nLook closely at the two graphs above. How are they different?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#stop-and-reflect",
    "href": "posts/A5-TabularData/index.html#stop-and-reflect",
    "title": "ASSIGNMENT 5",
    "section": "STOP AND REFLECT",
    "text": "STOP AND REFLECT\nI’ve been a bit sneaky up to this point. You’ve probably been focusing primarily on my (crappy) visualizations. That’s fine, but let’s think about the manipulations to the TABULAR DATA I’ve had to perform.\nI’m using the Tidyverse to do these manipulations. I set up the original data frame to conform to the tidy data principles (every column is a variable, every row is an observation), which is pretty much the base form of how we’ve discussed Tabular Data in class.\nI’ve snuck in some functions that have allowed me to FILTER, GROUP, and SUMMARIZE the data, often creating new dataframes as I do so. Hey, look! A handy cheatsheet for data transformation using the tidyverse!\nThese functions come from the dplyr package that gets installed as part of the tidyverse. The basic categories of actions are:\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names.\nfilter() picks cases based on their values.\nsummarise() reduces multiple values down to a single summary.\narrange() changes the ordering of the rows.\n\nAll of these work with group_by() so you can perform whichever operation on the groups that might be present in your data set.\nLet’s get back to improving our understanding of the relative value of NHL draft picks. The figure above considers a single draft class (2018), and shows the total number of NHL games all the players have accumulated, separating each draft round on an ordinal x axis.\nFine, I guess, but we still have to deal with overplotting, and think about whether a scatterplot really helps us accomplish our task. For this figure do the following:\n\nOverplotting. All those points on the y=0 line represent about 32 players each. Can you you think of a way that adding extra channels might help?\nLabelling. Can we create a solid figure caption and better axis labels for this figure? In your caption, please specify the task(s) the visualizaiton is intended to facilitate, as well as the marks, channels, and key-value pairs used.\nKey-Value pairs: Looks like we are using “round” as a continuous variable. Can we change this to an ordered factor?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#scatterplot-with-overall-draft-position",
    "href": "posts/A5-TabularData/index.html#scatterplot-with-overall-draft-position",
    "title": "ASSIGNMENT 5",
    "section": "SCATTERPLOT WITH OVERALL DRAFT POSITION",
    "text": "SCATTERPLOT WITH OVERALL DRAFT POSITION\nThis approach might yield a better match with the scatterplot idiom. What if we ignore draft round, and use the player’s overall draft position instead? It also might help us focus on our motivating question! What is the potential value of pick 31, and how does Elias Lindholm compare to that value?\n\n\nCode\nggplot(drafttot2018, aes(x=overall, y=totgames))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nFor this figure, address the following:\n\nWe are trying to address the notion of trading pick 31. How might you facilitate the task of evaluating picks in that range?\nCreate a caption and better axis labels for this figure.\nWhat if we wanted to use more than just the 2018 draft class?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#scatterplot-summary",
    "href": "posts/A5-TabularData/index.html#scatterplot-summary",
    "title": "ASSIGNMENT 5",
    "section": "SCATTERPLOT SUMMARY",
    "text": "SCATTERPLOT SUMMARY\nWe seem to be running into an issue in terms of overplotting. Scatterplots are great, but they work best for two quantitative attributes, and we have a situation with one or two keys and one quantitative attribute. The thing is, scatterplots can be very useful when part of our workflow involves modeling the data in some way. We’ll cover this kind of thing in future assignments, but just a bit of foreshadowing here:\n\n\nCode\nggplot(drafttot2018, aes(x=round, y=totgames))+\n  geom_point()+\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAdding the smoothed line doesn’t eliminate the overplotting problem, but it does indicate that it exists. We’ll cover other potential solutions (such as box plots and violin plots) to this issue later in the course, when we get to the notions of faceting and data reduction.\nWhy not include all the data? A scatter plot with that many players (4775) isn’t going to be great. But we could plot some sort of polynomial model to get a sense of the relationship between draft position and NHL games. We’ll filter to the first 8 years of their career.\n\n\nCode\ndrafttot&lt;- NHLDraft%&gt;%\n  filter(postdraft&lt;8)%&gt;%\n  group_by(playerId, round, overall, position, name)%&gt;%\n  summarise(totgames=sum(NHLgames))\n\n\n`summarise()` has grouped output by 'playerId', 'round', 'overall', 'position'.\nYou can override using the `.groups` argument.\n\n\nCode\nggplot(drafttot, aes(x=overall, y=totgames))+\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nOr we could visualize the average number of games played as a function of time since being drafted.\n\n\nCode\nggplot(NHLDraft, aes(x=postdraft, y=NHLgames))+\n  geom_smooth(aes(color=as.factor(round)))\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHINT\n\n\n\nIn the GitHub repository there is a file called NHLdraftstats.csv. What’s in there? Can we use that information?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#divergence",
    "href": "posts/A5-TabularData/index.html#divergence",
    "title": "ASSIGNMENT 5",
    "section": "DIVERGENCE",
    "text": "DIVERGENCE\nEnough esoteric wandering. The original version of this assignment focused on the relative value of draft picks in the NHL. This version has a more specific question. What might picks in the range of pick 31 conceivably yield? How often do picks in that range yield players of Elias Lindholm’s value?\nI guess we’d better figure out what Elias Lindholm brings to the table.\nCan you find him in our existing data? Can you think of a way to highlight him in the context of number of games played? What other kinds of data might we need to fairly evaluate Lindholm and pick 31?\nYou will be surprised how these seemingly simple questions force you to explore the nuances of working with and visualizing tabular data."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#simple-bar-chart",
    "href": "posts/A5-TabularData/index.html#simple-bar-chart",
    "title": "ASSIGNMENT 5",
    "section": "SIMPLE BAR CHART",
    "text": "SIMPLE BAR CHART\nOne of the best ways to deal with overplotting is to use our keys to SEPARATE and ORDER our data. Let’s do that now. I’ll stick with the summarized data for the 2018 draft year for now.\n\n\nCode\nggplot(drafttot2018, aes(x = name, y=totgames))+\n  geom_col()\n\n\n\n\n\n\n\n\n\nEpic. We now have a bar (column, really) chart with the key being player name, and the attribute being the total number of games played. We’ve SEPARATED the data using the spatial x-axis position channel, and aligned to that axis as well. But this visualization clearly sucks. You need to make it better by:\n\nAdding a visual channel indicating draft round.\nFixing the order of the x axis.\nMaking a caption and better axis labels.\nFixing the values of the x axis labels so they aren’t such a mess."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#stacked-bar",
    "href": "posts/A5-TabularData/index.html#stacked-bar",
    "title": "ASSIGNMENT 5",
    "section": "STACKED BAR?",
    "text": "STACKED BAR?\nStacked bar charts use two keys and one value. Can we leverage this idiom? Perhaps if we used both round and postdraft as our keys and NHLgames as our value…\nThe idea here is that we might be able to get a sense of the temporal pattern of NHL games after a player is drafted. Do first round picks join the NHL earlier? Do they stay in the NHL longer? That kind of thing.\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(-round)))+\n  geom_col(position = \"stack\")\n\n\n\n\n\n\n\n\n\nCode\nNHLDraft &lt;-NHLDraft %&gt;%\n  mutate(descround = desc(as.factor(round)))\n\nrounds&lt;-c(\"darkred\",\"red\",\"pink\", \"violet\", \"lightblue\",\"blue\", \"darkblue\")\n\nggplot(NHLDraft, aes(x = postdraft, y=NHLgames, fill=as.factor(-round), \n                     alpha = as.factor(-round)))+\n  geom_col(position = \"stack\")+\n  theme(legend.position = \"none\")\n\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nThis seems like it has some potential, but it definitely needs some work (by you):\n\nYou know the drill by now. Caption! Labels!\nImprove the color palette.\nDo we really only want data from the 2018 draft class?\nConsider the order of rounds within the stack (glyph). Which round is most important? Change the order within the glyphs to reflect this."
  },
  {
    "objectID": "posts/A5-TabularData/index.html#pie-charts-normalized-bar-charts",
    "href": "posts/A5-TabularData/index.html#pie-charts-normalized-bar-charts",
    "title": "ASSIGNMENT 5",
    "section": "PIE CHARTS / NORMALIZED BAR CHARTS",
    "text": "PIE CHARTS / NORMALIZED BAR CHARTS\nWe all know that Pie Charts are rarely a good choice, but let’s look at how to make one here. I’ll eliminate all the players drafted in 2018 who never played an NHL game, leaving us 80 players drafted in that year who made “THE SHOW”. Let’s look at how those 80 players were drafted:\n\n\nCode\nplayedNHL2018 &lt;- drafttot2018%&gt;%\n  filter(totgames&gt;0)\n\nggplot(playedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")\n\n\n\n\n\n\n\n\n\nObviously this isn’t great, but can you state why? Write a little critique of this visualizaiton that:\n\nConsiders a player who played hundreds of games over their first five years vs a player who played one game in five years.\nEvaluates the relative value of a second round pick and a third round pick.\n\nNow let’s change this to account for the various years post draft:\n\n\nCode\nseasonplayedNHL2018 &lt;- draft2018%&gt;%\n  filter(NHLgames&gt;0)\n\n\nggplot(seasonplayedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")+\n  facet_wrap(~postdraft)\n\n\n\n\n\n\n\n\n\nSeems like there is something to work with here, but let’s compare this to a normalized bar chart:\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(round)))+\n  geom_col(position = \"fill\")\n\n\nWarning: Removed 218 rows containing missing values (`geom_col()`).\n\n\n\n\n\n\n\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=overall))+\n  geom_col(position = \"fill\")\n\n\nWarning: Removed 218 rows containing missing values (`geom_col()`).\n\n\n\n\n\n\n\n\n\nCan you work with this to make it a useful visualization for your friend, Cassandra Canuck?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#heatmap",
    "href": "posts/A5-TabularData/index.html#heatmap",
    "title": "ASSIGNMENT 5",
    "section": "HEATMAP?",
    "text": "HEATMAP?\nCould this be useful?\n\n\nCode\nround1&lt;-NHLDraft%&gt;%\n  filter(round==1)\n\nggplot(NHLDraft, aes(y = reorder(name, overall), x = postdraft, fill = NHLgames)) +\n  geom_tile(width = .5, height = 5) +\n  theme(plot.margin = unit(c(0,0,0,0), \"cm\"),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())+\n  scale_fill_gradient(low = \"blue\", high = \"red\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(NHLDraft, aes(y = overall, x = postdraft, z=NHLgames)) + \n  geom_tile(aes(fill=NHLgames))"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#other-stuff-to-consider",
    "href": "posts/A5-TabularData/index.html#other-stuff-to-consider",
    "title": "ASSIGNMENT 5",
    "section": "OTHER STUFF TO CONSIDER",
    "text": "OTHER STUFF TO CONSIDER\n\nDo these visualizations change as a function of player position?\nIs the number of NHL games played really the best metric to use?"
  },
  {
    "objectID": "posts/A5-TabularData/index.html#conclusion",
    "href": "posts/A5-TabularData/index.html#conclusion",
    "title": "ASSIGNMENT 5",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nBased on your visualizations, what would you advise regarding this trade proposal? Why?"
  },
  {
    "objectID": "posts/A2-YourData/index.html",
    "href": "posts/A2-YourData/index.html",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "",
    "text": "A key feature of this course is that students should be using their own data whenever possible. This is critical to forging a learning experience that is customized to each student’s aspirations and the eccentricities of their chosen research domain. This assignment begins the process of helping you identify the data sets with which you want to work, and aligns with the notion of understanding the concepts of Data Semantics and Data Abstraction."
  },
  {
    "objectID": "posts/A2-YourData/index.html#summary",
    "href": "posts/A2-YourData/index.html#summary",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "",
    "text": "A key feature of this course is that students should be using their own data whenever possible. This is critical to forging a learning experience that is customized to each student’s aspirations and the eccentricities of their chosen research domain. This assignment begins the process of helping you identify the data sets with which you want to work, and aligns with the notion of understanding the concepts of Data Semantics and Data Abstraction."
  },
  {
    "objectID": "posts/A2-YourData/index.html#assignment",
    "href": "posts/A2-YourData/index.html#assignment",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nThe basic structure of this assignment is for you to identify, import, describe, and host a data set. I’ll break down the specifics for each of these actions below.\n\nIdentify a Data Set\nThe main criteria is that the data set has to matter to you in some way. Often, this will mean that it is your data set. It was collected by you and has a central role in your current or past graduate research. Awesome! Another scenario is that the data you want to use comes from your current job. Maybe it isn’t part of a research project, but you are motivated to learn how to better visualize the data or you are very interested in learning more about it. Also Awesome!\nSome of you might not have your own data. Perhaps you have just started your graduate training. Maybe your job doesn’t yet have data that you need to work with. No Problem!\nIt is perfectly fine to find publicly available data sets online. As long as the data set is interesting to you! You just need to make sure that the data:\n\nAre publicly available.\nAre not restricted by some kind of license or copyright.\nDo not contain private information.\nAre not covered by HIPPA, FERPA, CMMC, or other federal regulations related to data.\n\nIf you need help finding a data set, just let me know.\nSome fun potential categories for data sources include:\n\nSports Analytics from your favorite sport or team.\nPublicly available genomics data bases.\nKeggle.\nThe movie data base.\nClassic data sets from your field.\n\n\n\nImport the Data Set\nThis one is probably straightforward if your data set comes from your own research and lives on your local hard drive already.\n\n\nDescribe the Data Set\nThis is the bulk of the assignment. I want you to use the framework described in Dr. Munzner’s textbook to understand your data set and describe it to someone who is unfamiliar with your work. The basis of this approach is descibed in this lecture. In addition, this figure from the textbook summarizes the kinds of data types, data set types, and attribute types you might have in your data:\n\n\n\nBONUS OBJECTIVE: Host your Data Set\nUltimately, we are moving toward each of you hosting your assignments within an online repository that can serve as your data science portfolio. For this course, we are going to assume this is GitHub. At the very least, I want everyone to create (or dust off and log in to) your own GitHub account. We’ll try to use this assignment to set up a project repository, and perhaps even a simple web site using GitHub pages."
  },
  {
    "objectID": "posts/A2-YourData/index.html#resources",
    "href": "posts/A2-YourData/index.html#resources",
    "title": "ASSIGNMENT 2 - Your Data.",
    "section": "RESOURCES",
    "text": "RESOURCES\nA YouTube Video from Posit on Building your Data Science Portfolio\nTidyTuesday\nA fun Spotify example from TidyTuesday by Kaylin Pavlik.\nBarrie attempts this assignment in Tutorial 4."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "This class will help students establish a core understanding of data visualization. We will consider how data type (including tabular, network, and spatial data) interacts with visualization task to guide design choices. Diverse types of visual encodings and how they relate to human perception will be presented, along with practical exercises using the R and Python programming languages. Upon completion of the course, students will understand WHY particular visualization approaches are effective for a given data set and HOW to implement those visualizations using the language of their choice. The course is designed to be “discipline agnostic” - each student is encouraged to use data sets that they deem important / interesting. The goal is to have students learn how to develop visualizations that are relevant to their own disciplinary interests.\n\n\nI am maintaining the course here and on its CANVAS PAGE (for enrolled students).\nSYLLABUS\nBarrie’s GitHub\n\n\nKB GitHub and KB Portfolio\nHS GitHub and HS Porfolio\nYG GitHub and YG Porfolio\nRS GitHub and RS Porfolio\nGT GitHub and GT Porfolio\nLR GitHub and LR Porfolio\n\n\n\n\nThe Functional Art\nGGSIDE! A companion to ggplot for making side plots! COOL!\nAwesome Quarto: A potentially interesting repository of Quarto documents, talks, tools, examples, etc.\nThe MockUp Blog - TABLES! This blog post explores the R packages gt and gtextras which will help us up our table game!\nRiffomonas Project: Pat Schloss is a Professor at the University of Michigan. The Riffomonas Project is his Youtube channel, which has HUNDREDS of easy to follow and amazingly useful instructional videos on R, ggplot, version control, and literate programming.\nDr. Tamara Munzner’s Website: It isn’t fancy, but Dr. Munzner’s website has tons of resources from her textbook and the many data visualization courses she has offered. This includes recorded lectures that align directly with the chapters of the text, much like what we are using.\nCheat Sheets: So many visual guides for many R packages, including the tidyverse, ggplot, dplyr, etc.\nLearning Vis Tools: Teaching Data Visualization Tutorials An interesting paper for discussion as we forge the structure for this class."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "I am maintaining the course here and on its CANVAS PAGE (for enrolled students).\nSYLLABUS\nBarrie’s GitHub\n\n\nKB GitHub and KB Portfolio\nHS GitHub and HS Porfolio\nYG GitHub and YG Porfolio\nRS GitHub and RS Porfolio\nGT GitHub and GT Porfolio\nLR GitHub and LR Porfolio"
  },
  {
    "objectID": "index.html#learning-resources",
    "href": "index.html#learning-resources",
    "title": "BCB 520:: Foundations of Data Visualization",
    "section": "",
    "text": "The Functional Art\nGGSIDE! A companion to ggplot for making side plots! COOL!\nAwesome Quarto: A potentially interesting repository of Quarto documents, talks, tools, examples, etc.\nThe MockUp Blog - TABLES! This blog post explores the R packages gt and gtextras which will help us up our table game!\nRiffomonas Project: Pat Schloss is a Professor at the University of Michigan. The Riffomonas Project is his Youtube channel, which has HUNDREDS of easy to follow and amazingly useful instructional videos on R, ggplot, version control, and literate programming.\nDr. Tamara Munzner’s Website: It isn’t fancy, but Dr. Munzner’s website has tons of resources from her textbook and the many data visualization courses she has offered. This includes recorded lectures that align directly with the chapters of the text, much like what we are using.\nCheat Sheets: So many visual guides for many R packages, including the tidyverse, ggplot, dplyr, etc.\nLearning Vis Tools: Teaching Data Visualization Tutorials An interesting paper for discussion as we forge the structure for this class."
  }
]